document_id,text
page1-elem0,Building custom IBM Watson NLP Images.
page1-elem0,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem0,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem0,This post describes how to package custom models in container images for deployments.
page1-elem0,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem0,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem0,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem0,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726] is available.
page1-elem0,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page1-elem0,Downloading trained Models
page1-elem0,My post Training IBM Watson NLP Models [http://heidloff.net/article/training-ibm-watson-nlp-models/] explains how to train Watson NLP based models with notebooks in Watson Studio.
page1-elem0,After the training models can be saved as file asset in the Watson Studio project and they can be downloaded.
page1-elem0,project.save_data('ensemble_model' data=ensemble_model.as_file_like_object() overwrite=True)
page1-elem0,The models are typically stored locally a directory ‘models’.
page1-elem0,The directory can contain the zipped file with or without ‘.zip’ extension.
page1-elem0,The zip file can also be extracted.
page1-elem0,The name of the file (or directory name when extracted) is the model id which you’ll need later to refer to it.
page1-elem0,There are three ways to build deploy Watson NLP and models.
page1-elem0,1.
page1-elem0,Standalone containers: One pod with one container including the NLP runtime and the models
page1-elem0,2.
page1-elem0,Init containers for models: One pod with one NLP runtime container and one init container per model
page1-elem0,3.
page1-elem0,Cloud object storage for models and kServe (not covered in this post)
page1-elem0,Building Standalone Images with custom Models
page1-elem0,The easiest way is to put everything in one image.
page1-elem0,$ docker login cp.icr.io --username cp --password <entitlement_key>
page1-elem0,The following Dockerfile extends the runtime image with a local copy of the model(s).
page1-elem0,ARG WATSON_RUNTIME_BASE="cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18"
page1-elem0,FROM ${WATSON_RUNTIME_BASE} as base
page1-elem0,ENV LOCAL_MODELS_DIR=/app/models
page1-elem0,ENV ACCEPT_LICENSE=true
page1-elem0,COPY models /app/models
page1-elem0,The following commands build the image and run the container.
page1-elem0,$ docker build .
page1-elem0,-t watson-nlp-custom-container:v1
page1-elem0,$ docker run -d -e ACCEPT_LICENSE=true -p 8085:8085 watson-nlp-custom-container:v1
page1-elem0,Building Standalone Images with predefined Models
page1-elem0,Similarly to custom models predefined models can be put in a standalone image.
page1-elem0,Predefined Watson NLP models are available in the IBM image registry as init container images.
page1-elem0,When these containers are run normally they will invoke an unpack_model.sh script.
page1-elem0,The following Dockerfile shows how to download the images with models and how to put the models into the directory where the runtime container expects them.
page1-elem0,ARG WATSON_RUNTIME_BASE="cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18"
page1-elem0,ARG SENTIMENT_MODEL="cp.icr.io/cp/ai/watson-nlp_sentiment_aggregated-cnn-workflow_lang_en_stock:1.0.6"
page1-elem0,ARG EMOTION_MODEL="cp.icr.io/cp/ai/watson-nlp_classification_ensemble-workflow_lang_en_tone-stock:1.0.6"
page1-elem0,FROM ${SENTIMENT_MODEL} as model1
page1-elem0,RUN ./unpack_model.sh
page1-elem0,FROM ${EMOTION_MODEL} as model2
page1-elem0,RUN ./unpack_model.sh
page1-elem0,FROM ${WATSON_RUNTIME_BASE} as release
page1-elem0,RUN true && \
page1-elem0,mkdir -p /app/models
page1-elem0,ENV LOCAL_MODELS_DIR=/app/models
page1-elem0,COPY --from=model1 app/models /app/models
page1-elem0,COPY --from=model2 app/models /app/models
page1-elem0,Building Model Images for Init Containers
page1-elem0,Custom models can also be put in init container images.
page1-elem0,A Python tool is provided to do this.
page1-elem0,$ python3 -m venv client-env
page1-elem0,$ source client-env/bin/activate
page1-elem0,$ pip install watson-embed-model-packager
page1-elem0,$ python3 -m watson_embed_model_packager setup --library-version watson_nlp:3.2.0 --local-model-dir models --output-csv ./customer-complaints.csv
page1-elem0,$ python3 -m watson_embed_model_packager build --config customer-complaints.csv
page1-elem0,$ docker tag watson-nlp_ensemble_model:v1 <REGISTRY>/<NAMESPACE>/watson-nlp_ensemble_model:v1
page1-elem0,$ docker push <REGISTRY>/<NAMESPACE>/watson-nlp_ensemble_model:v1
page1-elem0,To find out more about Watson NLP and Watson for Embed in general check out these resources:
page1-elem0,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem0,* IBM Watson NLP Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page1-elem0,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem0,* IBM Watson NLP Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem0,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem0,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem0,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem0,The post Building custom IBM Watson NLP Images [http://heidloff.net/article/building-custom-ibm-watson-nlp-images-models/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem1,Understanding IBM Watson Containers.
page1-elem1,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem1,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem1,This post explains how to find the latest versions of containers and how to get the model files and gRPC proto files.
page1-elem1,To try it a trial [https://www.ibm.com/products/ibm-watson-natural-language-processing] is available.
page1-elem1,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page1-elem1,This post has three parts:
page1-elem1,* Finding the latest Version of Images
page1-elem1,* Accessing the Model Files
page1-elem1,* Accessing the gRCP Proto Files
page1-elem1,Finding the latest Version of Images
page1-elem1,In addition to the runtime container ‘cp.icr.io/cp/ai/watson-nlp-runtime’ IBM provides out of the box models which are stored in images that are run as init containers.
page1-elem1,* NLP [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page1-elem1,* Speech To Text [https://www.ibm.com/docs/en/watson-libraries?topic=home-models-catalog]
page1-elem1,* Text To Speech [https://www.ibm.com/docs/en/watson-libraries?topic=wtsleh-models-catalog]
page1-elem1,To find out the latest version even before the documentation is updated you can use Skopeo [https://github.com/containers/skopeo].
page1-elem1,The output shows the available tags and environment variables for the model and proto directories.
page1-elem1,$ docker login cp.icr.io --username cp --password <your-entitlement-key>
page1-elem1,$ skopeo login cp.icr.io
page1-elem1,$ skopeo inspect docker://cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page1-elem1,"Name": "cp.icr.io/cp/ai/watson-nlp-runtime"
page1-elem1,"Digest": "sha256:0cbcbd5bde0e4691e4cb1bf7fbe306a4b2082cc553c32f0be2bd60dfac75a2a5"
page1-elem1,"RepoTags": [
page1-elem1,"1.0.18"
page1-elem1,"1.0.20"
page1-elem1,"1.0"
page1-elem1,"1"
page1-elem1,]
page1-elem1,...
page1-elem1,]
page1-elem1,"Env": [
page1-elem1,"JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk"
page1-elem1,"SERVICE_PROTO_GEN_MODULE_DIR=generated"
page1-elem1,"LOCAL_MODELS_DIR=/app/models"
page1-elem1,...
page1-elem1,Accessing the Model Files
page1-elem1,Custom images can be built which only include the models you need.
page1-elem1,To include out of the box models in your custom images you need to download the model files first.
page1-elem1,Each model is stored in one sub-directory or as zip file.
page1-elem1,The name of the directory or zip file is the model id.
page1-elem1,You can get the model files by invoking these commands:
page1-elem1,$ docker login cp.icr.io --username cp --password <your-entitlement-key>
page1-elem1,$ mkdir models
page1-elem1,$ docker run -it --rm -e ACCEPT_LICENSE=true -v `pwd`/models:/app/models cp.icr.io/cp/ai/watson-nlp_syntax_izumo_lang_en_stock:1.0.7
page1-elem1,$ ls -la models
page1-elem1,Build a custom image with the syntax model:
page1-elem1,$ cat <<EOF >>Dockerfile
page1-elem1,FROM cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page1-elem1,COPY models /app/models
page1-elem1,EOF
page1-elem1,$ docker build .
page1-elem1,-t watson-nlp-with-syntax-model:latest
page1-elem1,$ docker run --rm -it \
page1-elem1,-e ACCEPT_LICENSE=true \
page1-elem1,-p 8085:8085 \
page1-elem1,-p 8080:8080 \
page1-elem1,watson-nlp-with-syntax-model
page1-elem1,Invoke Watson NLP via REST:
page1-elem1,$ open http://localhost:8080/swagger/
page1-elem1,$ curl -X POST "http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict" \
page1-elem1,-H "accept: application/json" \
page1-elem1,-H "grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock" \
page1-elem1,-H "content-type: application/json" \
page1-elem1,-d " { \"rawDocument\": { \"text\": \"It is so easy to embed Watson NLP in applications.
page1-elem1,Very cool.\" }}"
page1-elem1,"text": "It is so easy to embed Watson NLP in applications.
page1-elem1,Very cool."
page1-elem1,"producerId": { "name": "Izumo Text Processing" "version": "0.0.1" }
page1-elem1,...
page1-elem1,"sentences": [
page1-elem1,"span": {
page1-elem1,"begin": 0 "end": 50 "text": "It is so easy to embed Watson NLP in applications."
page1-elem1,}
page1-elem1,{ "span": { "begin": 51 "end": 61 "text": "Very cool."
page1-elem1,} }
page1-elem1,Accessing the gRCP Proto Files
page1-elem1,To invoke the gRCP APIs the proto files are needed which you can get from GitHub [https://github.com/IBM/ibm-watson-embed-clients/tree/main/watson_nlp/protos].
page1-elem1,To make sure you always use the right version you can also ‘download’ them from the runtime image.
page1-elem1,$ mkdir protos
page1-elem1,$ docker create --name watson-runtime-protos cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page1-elem1,$ docker cp watson-runtime-protos:/app/protos/.
page1-elem1,protos
page1-elem1,$ docker rm watson-runtime-protos
page1-elem1,Start the container:
page1-elem1,$ docker run --rm -it \
page1-elem1,-e ACCEPT_LICENSE=true \
page1-elem1,-p 8085:8085 \
page1-elem1,-p 8080:8080 \
page1-elem1,watson-nlp-with-syntax-model
page1-elem1,Invoke Watson NLP via gRCP:
page1-elem1,$ cd protos
page1-elem1,$ grpcurl -plaintext -proto common-service.proto -d '{
page1-elem1,"raw_document": {
page1-elem1,"text": "It is so easy to embed Watson NLP in applications.
page1-elem1,Very cool"}
page1-elem1,"parsers": ["token"]
page1-elem1,}' -H 'mm-model-id: syntax_izumo_lang_en_stock' localhost:8085 watson.runtime.nlp.v1.NlpService.SyntaxPredict
page1-elem1,"text": "It is so easy to embed Watson NLP in applications.
page1-elem1,Very cool"
page1-elem1,"producerId": {
page1-elem1,"name": "Izumo Text Processing"
page1-elem1,"version": "0.0.1"
page1-elem1,}
page1-elem1,...
page1-elem1,"sentences": [
page1-elem1,"span": {
page1-elem1,"end": 50
page1-elem1,"text": "It is so easy to embed Watson NLP in applications."
page1-elem1,}
page1-elem1,"span": {
page1-elem1,"begin": 51
page1-elem1,"end": 60
page1-elem1,"text": "Very cool"
page1-elem1,To find out more about Watson NLP Watson Speech To Text Watson Text To Speech and Watson for Embed in general check out the resources in my post Guide to IBM Watson Libraries [http://heidloff.net/article/the-ultimate-guide-to-ibm-watson-libraries/].
page1-elem1,The post Understanding IBM Watson Containers [http://heidloff.net/article/understanding-ibm-watson-containers/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem2,Running IBM Watson Speech To Text in Minikube.
page1-elem2,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem2,Via REST and WebSockets APIs AI can easily be embedded in applications.
page1-elem2,This post describes how to run Watson Speech To Text locally in Minikube.
page1-elem2,To set some context check out the landing page IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries].
page1-elem2,The Watson Speech To Text library is available as containers providing REST and WebSockets interfaces.
page1-elem2,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Cloud SaaS service for STS and IBM Cloud Pak for Data.
page1-elem2,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754] is available.
page1-elem2,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726].
page1-elem2,How to run STS locally via Minikube
page1-elem2,My post Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/] explained how to run Watson STT locally in Docker.
page1-elem2,The instructions below describe how to deploy Watson Speech To Text locally to Minikube via kubectl and yaml files.
page1-elem2,First you need to install Minikube for example via brew on MacOS.
page1-elem2,Next Minikube needs to be started with more memory and disk size than the Minikube defaults.
page1-elem2,I’ve used the settings below which is more than required but I wanted to leave space for other applications.
page1-elem2,Note that you also need to give your container runtime more resources.
page1-elem2,For example if you use Docker Desktop navigate to Preferences-Resources to do this.
page1-elem2,$ brew install minikube
page1-elem2,$ minikube start --cpus 12 --memory 16000 --disk-size 50g
page1-elem2,The namespace and secret need to be created.
page1-elem2,$ kubectl create namespace watson-demo
page1-elem2,$ kubectl config set-context --current --namespace=watson-demo
page1-elem2,$ kubectl create secret docker-registry \
page1-elem2,--docker-server=cp.icr.io \
page1-elem2,--docker-username=cp \
page1-elem2,--docker-password=<your IBM Entitlement Key> \
page1-elem2,-n watson-demo \
page1-elem2,ibm-entitlement-key
page1-elem2,Clone a repo with the Kubernetes yaml files to deploy Watson Speech To Text.
page1-elem2,$ git clone https://github.com/nheidloff/watson-embed-demos.git
page1-elem2,$ kubectl apply -f watson-embed-demos/minikube-speech-to-text/kubernetes/
page1-elem2,$ kubectl get pods --watch
page1-elem2,To use other models modify deployment.yaml [https://github.com/nheidloff/watson-embed-demos/blob/4660d1db1471e1d3079f2932bfc3107845bf6e45/minikube-speech-to-text/kubernetes/deployment.yaml#L70-L90].
page1-elem2,- name: watson-stt-en-us-telephony
page1-elem2,image: cp.icr.io/cp/ai/watson-stt-en-us-telephony:1.0.0
page1-elem2,args:
page1-elem2,- sh
page1-elem2,- -c
page1-elem2,- cp model/* /models/pool2
page1-elem2,env:
page1-elem2,- name: ACCEPT_LICENSE
page1-elem2,value: "true"
page1-elem2,resources:
page1-elem2,limits:
page1-elem2,cpu: 1
page1-elem2,ephemeral-storage: 1Gi
page1-elem2,memory: 1Gi
page1-elem2,requests:
page1-elem2,cpu: 100m
page1-elem2,ephemeral-storage: 1Gi
page1-elem2,memory: 256Mi
page1-elem2,volumeMounts:
page1-elem2,- name: models
page1-elem2,mountPath: /models/pool2
page1-elem2,When you open the Kubernetes Dashboard (via ‘minikube dashboard’) you’ll see the deployed resources.
page1-elem2,The pod contains the runtime container and four init containers (two specific models a generic model and a utility container).
page1-elem2,To invoke Watson Speech To Text port forwarding can be used.
page1-elem2,$ kubectl port-forward svc/ibm-watson-tts-embed 1080
page1-elem2,Invoke the REST API with a sample audio file.
page1-elem2,$ curl "http://localhost:1080/speech-to-text/api/v1/recognize" \
page1-elem2,--header "Content-Type: audio/wav" \
page1-elem2,--data-binary @watson-embed-demos/demo.wav
page1-elem2,"result_index": 0
page1-elem2,"results": [
page1-elem2,"final": true
page1-elem2,"alternatives": [
page1-elem2,"transcript": "ibm watson speech to text can easily be embedded in applications"
page1-elem2,"confidence": 0.85
page1-elem2,To find out more about Watson Speech To Speech and Watson for Embed in general check out these resources:
page1-elem2,* Watson Speech To Text Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-speech-text-library-embed-home]
page1-elem2,* Watson Speech To Text Model Catalog [https://www.ibm.com/docs/en/watson-libraries?topic=wtsleh-models-catalog]
page1-elem2,* Watson Speech To Text SaaS Model Catalog [https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models]
page1-elem2,* Watson Speech To Text SaaS API docs [https://cloud.ibm.com/apidocs/speech-to-text]
page1-elem2,* Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754]
page1-elem2,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem2,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem2,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem2,* Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/]
page1-elem2,* Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/]
page1-elem2,The post Running IBM Watson Speech To Text in Minikube [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-minikube/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem3,Guide to IBM Watson Libraries.
page1-elem3,IBM provides Watson NLP (Natural Language Understand) Watson Speech To Text and Watson Text To Speech as containers which can be embedded in cloud-native applications.
page1-elem3,This post lists links to relevant information in the context of Embeddable AI from IBM and the three libraries.
page1-elem3,Overview
page1-elem3,* IBM’s Embeddable AI [https://www.ibm.com/partnerworld/program/embeddableai]
page1-elem3,* IBM’s announcement regarding its embeddable AI software portfolio [https://newsroom.ibm.com/2022-10-25-IBM-Helps-Ecosystem-Partners-Accelerate-AI-Adoption-by-Making-it-Easier-to-Embed-and-Scale-AI-Across-Their-Business]
page1-elem3,* Rob Thomas on Accelerating AI Adoption with Ecosystem Partners [https://youtu.be/V8oGXnqVZEs?t=743]
page1-elem3,* IBM Digital Self-Serve Co-Create Experience for Embeddable AI [https://dsce.ibm.com/]
page1-elem3,* TechZone: Embeddable AI [https://techzone.ibm.com/collection/embedded-ai]
page1-elem3,* IBM Developer: Watson Libraries [https://developer.ibm.com/articles/watson-libraries-embeddable-ai-that-works-for-you/]
page1-elem3,Watson NLP
page1-elem3,Overview and Documentation
page1-elem3,* Landing page [https://www.ibm.com/products/ibm-watson-natural-language-processing]
page1-elem3,* Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem3,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem3,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem3,* Model Catalog [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page1-elem3,Development
page1-elem3,* Running and Deploying IBM Watson NLP Containers [http://heidloff.net/article/running-and-deploying-ibm-watson-nlp-containers/]
page1-elem3,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem3,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem3,* Understanding IBM Watson Containers [http://heidloff.net/article/understanding-ibm-watson-containers/]
page1-elem3,* Deploying Watson NLP to IBM Code Engine [http://heidloff.net/article/deploying-watson-nlp-to-ibm-code-engine/]
page1-elem3,* Watson Embedded AI Runtime Client Libraries [https://github.com/IBM/ibm-watson-embed-clients]
page1-elem3,* Embed Model Builder (init Containers) [https://github.com/IBM/ibm-watson-embed-model-builder]
page1-elem3,* Watson NLP Python Client [https://github.com/ibm-build-lab/Watson-NLP/blob/main/MLOps/Dash-App-gRPC-Client/readme.md]
page1-elem3,Operations
page1-elem3,* Building custom IBM Watson NLP Images [http://heidloff.net/article/building-custom-ibm-watson-nlp-images-models/]
page1-elem3,* Automation for embedded IBM Watson Deployments [http://heidloff.net/article/automation-for-ibm-watson-deployments/]
page1-elem3,* Setting up OpenShift and Applications in one Hour [http://heidloff.net/article/setting-up-openshift-and-applications-in-one-hour/]
page1-elem3,* Repo: Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem3,* Deploying TechZone Toolkit Modules on existing Clusters [http://heidloff.net/article/deploying-techzone-toolkit-modules-on-existing-clusters/]
page1-elem3,* Serving Watson NLP on Kubernetes with KServe ModelMesh [http://heidloff.net/article/serving-watson-nlp-on-kubernetes-with-kserve-modelmesh/]
page1-elem3,* Repo: Samples [https://github.com/ibm-build-lab/Watson-NLP/tree/main/MLOps]
page1-elem3,Training
page1-elem3,* Training IBM Watson NLP Models [http://heidloff.net/article/training-ibm-watson-nlp-models/]
page1-elem3,* Text Classification [https://techzone.ibm.com/collection/watson-nlp-text-classification]
page1-elem3,* Repo: Samples [https://github.com/ibm-build-lab/Watson-NLP/tree/main/ML]
page1-elem3,* Sentiment and Emotion Analysis [https://techzone.ibm.com/collection/watson-core-nlp]
page1-elem3,* Topic Modeling [https://techzone.ibm.com/collection/watson-nlp-topic-modeling]
page1-elem3,* Entities and Keywords Extraction [https://techzone.ibm.com/collection/watson-nlp-entities-and-keywords-extraction]
page1-elem3,Speech To Text
page1-elem3,* IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries]
page1-elem3,* Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754]
page1-elem3,* Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem3,* Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/]
page1-elem3,* Running IBM Watson Text To Speech in Minikube [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-minikube/]
page1-elem3,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-speech-text-library-embed-home]
page1-elem3,* Model Catalog [https://www.ibm.com/docs/en/watson-libraries?topic=home-models-catalog]
page1-elem3,* (SaaS) API Documentation [https://cloud.ibm.com/apidocs/speech-to-text]
page1-elem3,* SaaS Documentation [https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-gettingStarted]
page1-elem3,* Convert speech to text and extract meaningful insights from data [https://developer.ibm.com/tutorials/extract-meaningful-insights-from-data/]
page1-elem3,* Watson Speech To Text Analysis Notebook [https://github.com/ibm-build-lab/Watson-Speech/blob/main/Speech%20To%20%20Text/Speech%20To%20Text%20Analysis.ipynb]
page1-elem3,* STT Spring Application [https://github.com/ibm-build-lab/Watson-Speech/tree/main/STTApplication#readme]
page1-elem3,Text To Speech
page1-elem3,* IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries]
page1-elem3,* Trial [https://www.ibm.com/account/reg/signup?formid=urx-51758]
page1-elem3,* Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem3,* Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/]
page1-elem3,* Running IBM Watson Text To Speech in Minikube [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-minikube/]
page1-elem3,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-text-speech-library-embed-home"]
page1-elem3,* Model Catalog [https://www.ibm.com/docs/en/watson-libraries?topic=wtsleh-models-catalog]
page1-elem3,* (SaaS) API Documentation [https://cloud.ibm.com/apidocs/text-to-speech]
page1-elem3,* SaaS Documentation [https://cloud.ibm.com/docs/text-to-speech?topic=text-to-speech-gettingStarted]
page1-elem3,* Using TTS in a Notebook [https://github.com/ibm-build-lab/Watson-Speech/blob/main/Text%20To%20Speech/Text-to-Speech-Tutorial.md]
page1-elem3,* Watson Developer Cloud (Client SDKs) [https://github.com/watson-developer-cloud]
page1-elem3,The post Guide to IBM Watson Libraries [http://heidloff.net/article/the-ultimate-guide-to-ibm-watson-libraries/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem4,Deploying Watson NLP to IBM Code Engine.
page1-elem4,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem4,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem4,This post describes how to deploy and run Watson NLP on the serverless offering IBM Code Engine.
page1-elem4,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem4,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem4,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem4,To try it a trial [https://www.ibm.com/products/ibm-watson-natural-language-processing] is available.
page1-elem4,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page1-elem4,Step by Step Instructions
page1-elem4,First a custom image needs to be built which includes the NLP runtime and a list of models.
page1-elem4,$ docker login cp.icr.io --username cp --password <your-entitlement-key>
page1-elem4,$ mkdir models
page1-elem4,$ docker run -it --rm -e ACCEPT_LICENSE=true -v `pwd`/models:/app/models cp.icr.io/cp/ai/watson-nlp_syntax_izumo_lang_en_stock:1.0.7
page1-elem4,$ ls -la models
page1-elem4,$ cat <<EOF >>Dockerfile
page1-elem4,FROM cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page1-elem4,COPY models /app/models
page1-elem4,EOF
page1-elem4,$ docker build .
page1-elem4,-t my-watson-nlp-runtime:latest
page1-elem4,Next the custom image is pushed to a registry in this case the IBM Container Registry.
page1-elem4,$ ibmcloud plugin install cr
page1-elem4,$ ibmcloud login --sso
page1-elem4,$ ibmcloud cr region-set global
page1-elem4,$ ibmcloud cr namespace-add watson-nlp-demo
page1-elem4,$ ibmcloud cr login
page1-elem4,$ docker tag my-watson-nlp-runtime:latest icr.io/watson-nlp-demo/my-watson-nlp-runtime:latest
page1-elem4,$ docker push icr.io/watson-nlp-demo/my-watson-nlp-runtime:latest
page1-elem4,After this the Code Engine project is created.
page1-elem4,$ ibmcloud plugin install code-engine
page1-elem4,$ ibmcloud target -r us-south -g default
page1-elem4,$ ibmcloud ce project create --name watson-nlp-demo
page1-elem4,$ ibmcloud ce project select --name watson-nlp-demo
page1-elem4,To access the container registry from Code Engine a secret is created.
page1-elem4,This can be done manually or programmatically.
page1-elem4,* ibmcloud CLI documentation [https://cloud.ibm.com/docs/codeengine?topic=codeengine-cli#cli-secret-create]
page1-elem4,* Manual instructions [https://github.com/ibm-build-lab/Watson-NLP/blob/main/MLOps/Deploy-to-Code-Engine/README.md#step-14-create-a-code-engine-managed-secret-from-the-ibm-cloud-web-console]
page1-elem4,Finally the serverless application can be created.
page1-elem4,$ ibmcloud ce application create \
page1-elem4,--name watson-nlp-runtime \
page1-elem4,--port 8080 \
page1-elem4,--min-scale 1 --max-scale 2 \
page1-elem4,--cpu 2 --memory 4G \
page1-elem4,--image private.icr.io/watson-nlp-demo/my-watson-nlp-runtime:latest \
page1-elem4,--registry-secret ce-auto-icr-private-global \
page1-elem4,--env ACCEPT_LICENSE=true
page1-elem4,$ ibmcloud ce app list
page1-elem4,$ ibmcloud ce app logs --application watson-nlp-runtime
page1-elem4,$ ibmcloud ce app events --application watson-nlp-runtime
page1-elem4,$ curl -X POST "https://watson-nlp-runtime.vl0podgeqyi.us-south.codeengine.appdomain.cloud/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict" \
page1-elem4,-H "accept: application/json" \
page1-elem4,-H "grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock" \
page1-elem4,-H "content-type: application/json" \
page1-elem4,-d " { \"rawDocument\": { \"text\": \"It is so easy to embed Watson NLP in applications.
page1-elem4,Very cool.\" }}"
page1-elem4,To find out more about Watson NLP and Watson for Embed in general check out these resources:
page1-elem4,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem4,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem4,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem4,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem4,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem4,The post Deploying Watson NLP to IBM Code Engine [http://heidloff.net/article/deploying-watson-nlp-to-ibm-code-engine/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem5,Training IBM Watson NLP Models.
page1-elem5,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem5,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem5,This post describes how to train a simple model for text classfication.
page1-elem5,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem5,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem5,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem5,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726] is available.
page1-elem5,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page1-elem5,Training Watson NLP Models
page1-elem5,Watson NLP comes with many predefined models [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog] that can either be re-used without any modifications or that can be customized and extended.
page1-elem5,One way to deploy Watson NLP is to deploy the Watson runtime container plus one init container per model.
page1-elem5,The init containers store their models in the same volume so that the runtime container can access them.
page1-elem5,The runtime container provides REST and gRPC interfaces which can be invoked by applications to run predictions.
page1-elem5,To train custom models Watson Studio needs to be used at this point.
page1-elem5,Let’s take a look at a simple example.
page1-elem5,IBM provides a sample notebook [https://github.com/ibm-build-lab/Watson-NLP/blob/b0ba0652b11cee336a401b66f5d46629f4f71e02/ML/Text-Classification/Consumer%20complaints%20Classification.ipynb] to classify consumer complaints about financial products and services.
page1-elem5,This could be used for example to route a complaint to the appropriate staff member.
page1-elem5,The data that is used in this notebook is taken from the Consumer Complaint Database [https://www.consumerfinance.gov/complaint/data-use/] that is published by the Consumer Financial Protection Bureau a U.S. government agency.
page1-elem5,Step 1: Import Libraries
page1-elem5,First the Watson NLP library needs to be imported which is part of Watson Studio.
page1-elem5,!pip install ibm-watson
page1-elem5,import watson_nlp
page1-elem5,...
page1-elem5,from watson_core.data_model.streams.resolver import DataStreamResolver
page1-elem5,from watson_core.toolkit import fileio
page1-elem5,from watson_nlp.blocks.classification.svm import SVM
page1-elem5,from watson_nlp.workflows.classification import Ensemble
page1-elem5,from watson_core.toolkit.quality_evaluation import QualityEvaluator EvalTypes
page1-elem5,Step 2: Load Data
page1-elem5,For convenience reasons the data can be downloaded from Box.
page1-elem5,This is the original structure of the data.
page1-elem5,Step 3: Clean up Data and prepare Training
page1-elem5,The dataset is divided in training and test data.
page1-elem5,# 80% training data
page1-elem5,train_orig_df = train_test_df.groupby('Product').sample(frac=0.8 random_state=6)
page1-elem5,# 20% test data
page1-elem5,test_orig_df = train_test_df.drop(train_orig_df.index)
page1-elem5,Unnecessary columns are dropped and columns are renamed.
page1-elem5,def prepare_data(df):
page1-elem5,# only the text column and the target label *Product* are needed
page1-elem5,df_out = df[['Consumer complaint narrative' 'Product']].reset_index(drop=True)
page1-elem5,# rename to the identifiers expected by Watson NLP
page1-elem5,df_out = df_out.rename(columns={"Consumer complaint narrative": "text" 'Product': 'labels'})
page1-elem5,# the label column should be an array (although we have only one label per complaint)
page1-elem5,df_out['labels'] = df_out['labels'].map(lambda label: [label])
page1-elem5,return df_out
page1-elem5,train_df = prepare_data(train_orig_df)
page1-elem5,train_file = './train_data.json'
page1-elem5,train_df.to_json(train_file orient='records')
page1-elem5,test_df = prepare_data(test_orig_df)
page1-elem5,test_file = './test_data.json'
page1-elem5,test_df.to_json(test_file orient='records')
page1-elem5,test_df.explode('labels')
page1-elem5,This is the resulting structure which can be used for the training.
page1-elem5,Step 4: Train Model
page1-elem5,In this example ensemble methods [https://en.wikipedia.org/wiki/Ensemble_learning] are applied which “use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone”.
page1-elem5,The following models are downloaded:
page1-elem5,syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))
page1-elem5,use_model = watson_nlp.load(watson_nlp.download('embedding_use_en_stock'))
page1-elem5,stopwords = watson_nlp.download_and_load('text_stopwords_classification_ensemble_en_stock')
page1-elem5,The ensemble model depends on the syntax model and the GloVe and USE embeddings.
page1-elem5,They are passed with the file containing the training data.
page1-elem5,‘Ensemble.train’ runs the actual training which takes roughly one hour if you run the training in Watson Studio using the TechZone demo environment [https://techzone.ibm.com/collection/watson-nlp-text-classification#tab-1].
page1-elem5,from watson_nlp.workflows.classification import Ensemble
page1-elem5,ensemble_model = Ensemble.train(train_file 'syntax_izumo_en_stock' 'embedding_glove_en_stock' 'embedding_use_en_stock' stopwords=stopwords cnn_epochs=5)
page1-elem5,Step 5: Evaluate Model
page1-elem5,In the last step the trained model is evaluated by using the test dataset which was not part of the training.
page1-elem5,To understand the results you need to know what precision recall and f1-score mean.
page1-elem5,Read the article How to measure an AI models performance [https://abeyon.com/ai-performance-measurement-f1score] or watch the video What is Precision Recall and F1-Score [https://www.youtube.com/watch?v=wYevg3gLhnI]?.
page1-elem5,Summary from the article:
page1-elem5,* Precision can be thought of as a measure of exactness
page1-elem5,* Recall can be thought of as a measure of completeness
page1-elem5,* F1-score is a combination of Precision and Recall.
page1-elem5,A good F1 score means that you have low false positives and low false negatives
page1-elem5,Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial.
page1-elem5,Here are the results:
page1-elem5,To find out more about Watson NLP and Watson for Embed in general check out these resources:
page1-elem5,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem5,* IBM Watson NLP Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page1-elem5,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem5,* IBM Watson NLP Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem5,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem5,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem5,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem5,The post Training IBM Watson NLP Models [http://heidloff.net/article/training-ibm-watson-nlp-models/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem6,Serving Watson NLP on Kubernetes with KServe ModelMesh.
page1-elem6,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem6,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem6,This post describes how to deploy and run Watson NLP and Watson NLP models on Kubernetes via the highly scalable model inference platform KServe ModelMesh.
page1-elem6,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem6,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem6,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem6,What is KServe ModelMesh?
page1-elem6,KServe is a Kubernetes-based platform for ML model inference (predictions).
page1-elem6,It supports several standard ML model formats including TensorFlow PyTorch ONNX scikit-learn and more.
page1-elem6,Additionally it is highly scalable and dynamic.
page1-elem6,KServe ModelMesh is used for sophisticated AI scenarios where multiple models are used at the same time.
page1-elem6,For example you might have a scenario where you need various NLP models (classification emotions concepts etc.) various Speech models (different qualities voices etc.) and all this for for different languages.
page1-elem6,In this case utting all models in one container is not an option.
page1-elem6,Let’s look at the definition from the KServe [https://kserve.github.io/website/0.9/] landing page.
page1-elem6,> ModelMesh is designed for high-scale high-density and frequently-changing model use cases.
page1-elem6,ModelMesh intelligently loads and unloads AI models to and from memory to strike an intelligent trade-off between responsiveness to users and computational footprint.
page1-elem6,Why KServe?
page1-elem6,* KServe is a standard Model Inference Platform on Kubernetes built for highly scalable use cases.
page1-elem6,* Provides performant standardized inference protocol across ML frameworks.
page1-elem6,* Support modern serverless inference workload with Autoscaling including Scale to Zero on GPU.
page1-elem6,* Provides high scalability density packing and intelligent routing using ModelMesh
page1-elem6,* Simple and Pluggable production serving for production ML serving including prediction pre/post processing monitoring and explainability.
page1-elem6,* Advanced deployments with canary rollout experiments ensembles and transformers.
page1-elem6,KServe runs on Kubernetes.
page1-elem6,It requires etcd S3 storage and optionally Knative and Istio.
page1-elem6,The video Exploring ML Model Serving with KServe [https://www.youtube.com/watch?v=FX6naJLaq2Y] provides a good introduction and overview.
page1-elem6,Deploying Watson NLP Models to KServe ModelMesh
page1-elem6,There is a tutorial [https://github.com/ibm-build-lab/Watson-NLP/blob/main/MLOps/Deploy-to-KServe-ModelMesh-Serving/README.md] that provides detailed instructions how to deploy NLP models to KServe.
page1-elem6,For IBM partners there is also a test environment available.
page1-elem6,Below are the key steps of the tutorial.
page1-elem6,First you need to store predefined or custom Watson NLP models on some S3 complianted cloud object storage.
page1-elem6,The test environment uses Minio which can be installed in your own clusters.
page1-elem6,Via the Minio CLI models can be uploaded to buckets.
page1-elem6,If you use IBM’s Cloud Object Storage make sure to use the HMAC credentials.
page1-elem6,Next you define an instance of the custom resource definition InferenceService per model.
page1-elem6,In this definition you refer to your model in S3.
page1-elem6,kubectl create -f - <<EOF
page1-elem6,apiVersion: serving.kserve.io/v1beta1
page1-elem6,kind: InferenceService
page1-elem6,metadata:
page1-elem6,name: $NAME
page1-elem6,annotations:
page1-elem6,serving.kserve.io/deploymentMode: ModelMesh
page1-elem6,spec:
page1-elem6,predictor:
page1-elem6,model:
page1-elem6,modelFormat:
page1-elem6,name: watson-nlp
page1-elem6,storage:
page1-elem6,path: $PATH_TO_MODEL
page1-elem6,key: $BUCKET
page1-elem6,parameters:
page1-elem6,bucket: $BUCKET
page1-elem6,EOF
page1-elem6,To get the endpoints invoke this command.
page1-elem6,$ kubectl get inferenceservice
page1-elem6,ensemble-classification-wf-en-emotion-stock-predictor   grpc://modelmesh-serving.ibmid-6620037hpc-669mq7e2:8033
page1-elem6,sentiment-document-cnn-workflow-en-stock-predictor      grpc://modelmesh-serving.ibmid-6620037hpc-669mq7e2:8033
page1-elem6,syntax-izumo-en-stock-predictor                         grpc://modelmesh-serving.ibmid-6620037hpc-669mq7e2:8033
page1-elem6,To invoke Watson NLP from local code or via commands forward the port.
page1-elem6,kubectl port-forward service/modelmesh-serving 8085:8033
page1-elem6,Next you need to get the proto files.
page1-elem6,You can download them from a repo and copy them from the runtime image.
page1-elem6,$ git clone https://github.com/IBM/ibm-watson-embed-clients
page1-elem6,$ cd watson_nlp/protos
page1-elem6,or
page1-elem6,$ kubectl exec deployment/modelmesh-serving-watson-nlp-runtime -c watson-nlp-runtime -- jar cM -C /app/protos .
page1-elem6,| jar x
page1-elem6,The watson-automation [https://github.com/ibm/watson-automation#grpc] repo shows a little example how to invoke Watson NLP functionality via gRPC.
page1-elem6,Installing KServe ModelMesh Serving
page1-elem6,See the KServe ModelMesh Serving installation instructions [https://github.com/kserve/modelmesh-serving/blob/release-0.8/docs/install/install-script.md] for detailed instructions on how to install KServe with ModelMesh onto your cluster.
page1-elem6,You need to install etcd S3 KServe and optionally Istio.
page1-elem6,Unfortunately there is no operator yet but a script is provided.
page1-elem6,To deploy Watson NLP on KServe a ServingRuntime [https://www.ibm.com/docs/en/watson-libraries?topic=containers-run-kubernetes-kserve-modelmesh-serving] instance needs to be defined and applied.
page1-elem6,A serving runtime is a template for a pod that can serve one or more particular model formats.
page1-elem6,Apply the following sample to create a simple serving runtime for Watson NLP models:
page1-elem6,apiVersion: serving.kserve.io/v1alpha1
page1-elem6,kind: ServingRuntime
page1-elem6,metadata:
page1-elem6,name: watson-nlp-runtime
page1-elem6,spec:
page1-elem6,containers:
page1-elem6,- env:
page1-elem6,- name: ACCEPT_LICENSE
page1-elem6,value: "true"
page1-elem6,- name: LOG_LEVEL
page1-elem6,value: info
page1-elem6,- name: CAPACITY
page1-elem6,value: "1000000000"
page1-elem6,- name: DEFAULT_MODEL_SIZE
page1-elem6,value: "500000000"
page1-elem6,image: cp.icr.io/cp/ai/watson-nlp-runtime:1.0.20
page1-elem6,imagePullPolicy: IfNotPresent
page1-elem6,name: watson-nlp-runtime
page1-elem6,resources:
page1-elem6,limits:
page1-elem6,cpu: 2
page1-elem6,memory: 16Gi
page1-elem6,requests:
page1-elem6,cpu: 1
page1-elem6,memory: 16Gi
page1-elem6,grpcDataEndpoint: port:8085
page1-elem6,grpcEndpoint: port:8085
page1-elem6,multiModel: true
page1-elem6,storageHelper:
page1-elem6,disabled: false
page1-elem6,supportedModelFormats:
page1-elem6,- autoSelect: true
page1-elem6,name: watson-nlp
page1-elem6,To find out more about Watson NLP and Watson for Embed in general check out these resources:
page1-elem6,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem6,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem6,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem6,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem6,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem6,The post Serving Watson NLP on Kubernetes with KServe ModelMesh [http://heidloff.net/article/serving-watson-nlp-on-kubernetes-with-kserve-modelmesh/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem7,Running and Deploying IBM Watson NLP Containers.
page1-elem7,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem7,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem7,This post describes different options how to run and deploy Watson NLP.
page1-elem7,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem7,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem7,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem7,There are multiple options how to run and deploy Watson NLP:
page1-elem7,* Locally via container engines like Docker or Podman [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem7,* Deployments to Kubernetes (or OpenShift and Minikube) via Helm chart [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem7,* Deployments to Kubernetes/OpenShift via TechZone Deployer (Terraform and ArgoCD) [http://heidloff.net/article/setting-up-openshift-and-applications-in-one-hour/]
page1-elem7,* Deployments to Kubernetes via kubectl and yaml files (focus of this post)
page1-elem7,* Deployments to Kubernetes and KServe ModelMesh Serving [https://www.ibm.com/docs/en/watson-libraries?topic=containers-run-kubernetes-kserve-modelmesh-serving]
page1-elem7,To run Watson NLP two components are needed:
page1-elem7,* Watson NLP runtime: Executes the core functionality and provides REST and gRPC interfaces.
page1-elem7,* Models: Predefined or custom models are stored in a directory/volume that the runtime can access.
page1-elem7,The models can be copied there manually init containers can be used or they can be downloaded from cloud object storage.
page1-elem7,There are different ways to package these two components up in containers.
page1-elem7,Read the post Building custom IBM Watson NLP Images [http://heidloff.net/article/building-custom-ibm-watson-nlp-images-models/] for details.
page1-elem7,Deployments to Kubernetes via kubectl and yaml files
page1-elem7,Via kubectl or oc Kubernetes resources [https://github.com/nheidloff/watson-embed-demos/blob/main/nlp/kubernetes/deployment.yaml] can be deployed.
page1-elem7,The Watson NLP pod contains the NLP runtime container and potentially multiple init containers.
page1-elem7,Each init container contains either predefined [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog] or custom models.
page1-elem7,initContainers:
page1-elem7,- name: ensemble-model
page1-elem7,image: cp.icr.io/cp/ai/watson-nlp_syntax_izumo_lang_en_stock:1.0.7
page1-elem7,volumeMounts:
page1-elem7,- name: model-directory
page1-elem7,mountPath: "/app/models"
page1-elem7,env:
page1-elem7,- name: ACCEPT_LICENSE
page1-elem7,value: 'true'
page1-elem7,containers:
page1-elem7,- name: watson-nlp-runtime
page1-elem7,image: cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page1-elem7,env:
page1-elem7,- name: ACCEPT_LICENSE
page1-elem7,value: 'true'
page1-elem7,- name: LOCAL_MODELS_DIR
page1-elem7,value: "/app/models"
page1-elem7,To deploy the Kubernetes resources the following commands need to be executed.
page1-elem7,$ kubectl create namespace watson-demo
page1-elem7,$ kubectl config set-context --current --namespace=watson-demo
page1-elem7,$ kubectl create secret docker-registry \
page1-elem7,--docker-server=cp.icr.io \
page1-elem7,--docker-username=cp \
page1-elem7,--docker-password=<your IBM Entitlement Key> \
page1-elem7,-n watson-demo \
page1-elem7,ibm-entitlement-key
page1-elem7,$ git clone https://github.com/nheidloff/watson-embed-demos.git
page1-elem7,$ kubectl apply -f watson-embed-demos/nlp/kubernetes/
page1-elem7,$ kubectl get pods --watch
page1-elem7,$ kubectl get svc
page1-elem7,$ kubectl port-forward svc/watson-nlp-runtime-service 8080
page1-elem7,In the second terminal the REST API can be invoked.
page1-elem7,$ curl -X POST "http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict" \
page1-elem7,-H "accept: application/json" \
page1-elem7,-H "grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock" \
page1-elem7,-H "content-type: application/json" \
page1-elem7,-d " { \"rawDocument\": { \"text\": \"It is so easy to embed Watson NLP in applications.
page1-elem7,Very cool.\" }}"
page1-elem7,To see and run other REST APIs the Swagger (OpenAPI) user interface can be opened: http://localhost:8080/swagger.
page1-elem7,To find out more about Watson NLP and Watson for Embed in general check out these resources:
page1-elem7,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem7,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem7,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem7,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem7,* Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/]
page1-elem7,The post Running and Deploying IBM Watson NLP Containers [http://heidloff.net/article/running-and-deploying-ibm-watson-nlp-containers/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem8,Running IBM Watson Text To Speech in Minikube.
page1-elem8,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem8,Via REST and WebSockets APIs AI can easily be embedded in applications.
page1-elem8,This post describes how to run Watson Text To Speech locally in Minikube.
page1-elem8,To set some context check out the landing page IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries].
page1-elem8,The Watson Text to Speech library is available as containers providing REST and WebSockets interfaces.
page1-elem8,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Cloud SaaS service for TTS and IBM Cloud Pak for Data.
page1-elem8,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754] is available.
page1-elem8,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726].
page1-elem8,How to run TTS locally via Minikube
page1-elem8,My post Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/] explained how to run Watson TTS locally in Docker.
page1-elem8,The instructions below describe how to deploy Watson Text To Speech locally to Minikube via kubectl and yaml files.
page1-elem8,First you need to install Minikube for example via brew on MacOS.
page1-elem8,Next Minikube needs to be started with more memory and disk size than the Minikube defaults.
page1-elem8,I’ve used the settings below which is more than required but I wanted to leave space for other applications.
page1-elem8,Note that you also need to give your container runtime more resources.
page1-elem8,For example if you use Docker Desktop navigate to Preferences-Resources to do this.
page1-elem8,$ brew install minikube
page1-elem8,$ minikube start --cpus 12 --memory 16000 --disk-size 50g
page1-elem8,The namespace and secret need to be created.
page1-elem8,$ kubectl create namespace watson-demo
page1-elem8,$ kubectl config set-context --current --namespace=watson-demo
page1-elem8,$ kubectl create secret docker-registry \
page1-elem8,--docker-server=cp.icr.io \
page1-elem8,--docker-username=cp \
page1-elem8,--docker-password=<your IBM Entitlement Key> \
page1-elem8,-n watson-demo \
page1-elem8,ibm-entitlement-key
page1-elem8,Clone a repo with the Kubernetes yaml files to deploy Watson Text To Speech.
page1-elem8,$ git clone https://github.com/nheidloff/watson-embed-demos.git
page1-elem8,$ kubectl apply -f watson-embed-demos/minikube-text-to-speech/kubernetes/
page1-elem8,$ kubectl get pods --watch
page1-elem8,To use other speech models modify deployment.yaml [https://github.com/nheidloff/watson-embed-demos/blob/04c52d563039b10a86fdb25b8effe8ddf2d1e948/minikube-text-to-speech/kubernetes/deployment.yaml#L48-L68].
page1-elem8,- name: watson-tts-en-us-allisonv3voice
page1-elem8,image: cp.icr.io/cp/ai/watson-tts-en-us-allisonv3voice:1.0.0
page1-elem8,args:
page1-elem8,- sh
page1-elem8,- -c
page1-elem8,- cp model/* /models/pool2
page1-elem8,env:
page1-elem8,- name: ACCEPT_LICENSE
page1-elem8,value: "true"
page1-elem8,resources:
page1-elem8,limits:
page1-elem8,cpu: 1
page1-elem8,ephemeral-storage: 1Gi
page1-elem8,memory: 1Gi
page1-elem8,requests:
page1-elem8,cpu: 100m
page1-elem8,ephemeral-storage: 1Gi
page1-elem8,memory: 256Mi
page1-elem8,volumeMounts:
page1-elem8,- name: models
page1-elem8,mountPath: /models/pool2
page1-elem8,When you open the Kubernetes Dashboard (via ‘minikube dashboard’) you’ll see the deployed resources.
page1-elem8,The pod contains the runtime container and four init containers (two specific voice models a generic model and a utility container).
page1-elem8,To invoke Watson Text To Speech port forwarding can be used.
page1-elem8,$ kubectl port-forward svc/ibm-watson-tts-embed 1080
page1-elem8,The result of the curl command will be written to output.wav.
page1-elem8,$ curl "http://localhost:1080/text-to-speech/api/v1/synthesize" \
page1-elem8,--header "Content-Type: application/json" \
page1-elem8,--data '{"text":"Hello world"}' \
page1-elem8,--header "Accept: audio/wav" \
page1-elem8,--output output.wav
page1-elem8,To find out more about Watson Text To Speech and Watson for Embed in general check out these resources:
page1-elem8,* Watson Text To Speech Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-text-speech-library-embed-home]
page1-elem8,* Watson Text To Speech Model Catalog [https://www.ibm.com/docs/en/watson-libraries?topic=home-models-catalog]
page1-elem8,* Watson Text To Speech SaaS API docs [https://cloud.ibm.com/apidocs/text-to-speech]
page1-elem8,* Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754]
page1-elem8,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page1-elem8,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem8,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page1-elem8,* Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/]
page1-elem8,* Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/]
page1-elem8,The post Running IBM Watson Text To Speech in Minikube [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-minikube/] appeared first on Niklas Heidloff [http://heidloff.net].
page1-elem9,Deploying custom Watson NLP Models with Terraform.
page1-elem9,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page1-elem9,Via REST and gRCP APIs AI can easily be embedded in applications.
page1-elem9,This post describes how custom Watson NLP models can be deployed with TechZone Deployer an opinionated deployment and operations toolkit based on Terraform and ArgoCD.
page1-elem9,Watson NLP
page1-elem9,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page1-elem9,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page1-elem9,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page1-elem9,TechZone Deployer
page1-elem9,With TechZone Deployer (also known as TechZone Accelerator Toolkit TechZone Automation Software Everywhere Cloud Native Toolkit) IBM software open source projects and custom applications can easily be deployed to various clouds.
page1-elem9,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page1-elem9,The toolkit leverages Terraform and GitOps and is based on best practices from IBM projects with partners and clients.
page1-elem9,With the toolkit both infrastructure like Kubernetes clusters as well as Kubernetes resources within clusters can be deployed.
page1-elem9,Infrastructure resources are deployed via Terraform resources within clusters via Argo CD.
page1-elem9,Automatic Deployments of the Watson NLP Runtime and Models
page1-elem9,Based on TechZone Deployer my team has created an asset to deploy 1.
page1-elem9,OpenShift clusters 2.
page1-elem9,Watson NLP and 3.
page1-elem9,custom applications [https://github.com/IBM/watson-automation] in these clusters in one hour [http://heidloff.net/article/setting-up-openshift-and-applications-in-one-hour/].
page1-elem9,Watch the short video Automation for IBM Watson Deployments [https://www.youtube.com/watch?v=8lbVRAvJgy4] for an introduction.
page1-elem9,The usage of TechZone Deployer is very easy:
page1-elem9,* Install CLI
page1-elem9,* Define which modules to deploy from a module catalog [https://modules.cloudnativetoolkit.dev/]
page1-elem9,* Configure modules in variables.yaml and credentials.properties files
page1-elem9,* Use CLI to create Terraform modules
page1-elem9,* Launch local tools container and apply Terraform modules
page1-elem9,This sample Watson NLP configuration uses one predefined model hosted in the IBM Cloud Pak registry.
page1-elem9,- name: terraform_gitops_watson_nlp_runtime_image
page1-elem9,value: watson-nlp-runtime:1.0.18
page1-elem9,- name: terraform_gitops_watson_nlp_runtime_registry
page1-elem9,value: watson
page1-elem9,- name: terraform_gitops_watson_nlp_accept_license
page1-elem9,value: false
page1-elem9,- name: terraform_gitops_watson_nlp_imagePullSecrets
page1-elem9,value:
page1-elem9,- ibm-entitlement-key
page1-elem9,- name: terraform_gitops_watson_nlp_models
page1-elem9,value:
page1-elem9,- registry: watson
page1-elem9,image: watson-nlp_syntax_izumo_lang_en_stock:1.0.7
page1-elem9,- name: terraform_gitops_watson_nlp_registries
page1-elem9,value:
page1-elem9,- name: watson
page1-elem9,url: cp.icr.io/cp/ai
page1-elem9,- name: terraform_gitops_watson_nlp_registryUserNames
page1-elem9,value:
page1-elem9,- registry: watson
page1-elem9,userName: cp
page1-elem9,Deployments of multiple Models
page1-elem9,It’s also possible to deploy in addition to the Watson NLP runtime multiple models both predefined models as well as custom models.
page1-elem9,At a minimum you need the Watson NLP runtime image.
page1-elem9,The NLP runtime container runs in the Watson NLP pod at runtime.
page1-elem9,Additionally you can have 1 to N ‘model images’ which run as Kubernetes init containers.
page1-elem9,They are triggered when pods are created.
page1-elem9,Their purpose is to put the model artifacts on disk so that the Watson NLP runtime container can access them.
page1-elem9,Once they have done this these containers terminate.
page1-elem9,Images reside in registries which are typically protected.
page1-elem9,Pull secrets need to be provided to access them.
page1-elem9,Sealed Secrets for Kubernetes [https://github.com/bitnami-labs/sealed-secrets] are used to protect the secrets.
page1-elem9,There can be multiple registries (N >= 1) and multiple secrets (M >= 0).
page1-elem9,Registries can use secrets but don’t have to (N > M).
page1-elem9,There needs to be one registry to access the NLP runtime image which is stored in a protected registry.
page1-elem9,The configuration is done in two files:
page1-elem9,* variables.yaml
page1-elem9,* credentials.yaml
page1-elem9,Pull secrets have to contain the following information:
page1-elem9,* Secret name: Defined in the “imagePullSecrets” array in variables.yaml.
page1-elem9,* Registry URL: Defined in the “registries” array in variables.yaml.
page1-elem9,* Registry user name: Defined in the “registryUserNames” array in variables.yaml.
page1-elem9,the “registry” name needs to map to the same name under registries.
page1-elem9,* Registry password: Defined in “TF_VAR_terraform-gitops-watson-nlp_registry_credentials” in credentials.properties.
page1-elem9,This variable can include a comma delimited list of registry passwords/tokens.
page1-elem9,For multiple secrets the order needs to be the same one as in variables.yaml for “registryUserNames”.
page1-elem9,The screenshot shows the deployed containers.
page1-elem9,To find out more about Watson NLP and TechZone Deployer check out these resources:
page1-elem9,* IBM Watson NLP Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page1-elem9,* IBM Watson NLP Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726]
page1-elem9,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page1-elem9,* TechZone Accelerator Toolkit Documentation [https://operate.cloudnativetoolkit.dev/]
page1-elem9,* TechZone Accelerator Toolkit Modules [https://operate.cloudnativetoolkit.dev/]
page1-elem9,The post Deploying custom Watson NLP Models with Terraform [http://heidloff.net/article/deploying-custom-watson-nlp-models-with-terraform/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem0,Developing TechZone Toolkit GitOps Modules with Helm.
page2-elem0,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page2-elem0,This article provides an overview how to develop your own GitOps modules to deploy resources to Kubernetes via Helm.
page2-elem0,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page2-elem0,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page2-elem0,With the toolkit both infrastructure like Kubernetes clusters as well as Kubernetes resources within clusters can be deployed.
page2-elem0,Infrastructure resources are deployed via Terraform resources within clusters via Argo CD.
page2-elem0,This article explains how Helm [https://helm.sh/] can be used in the TechZone Toolkit to deploy resources to Kubernetes or OpenShift clusters with Argo CD.
page2-elem0,Helm is a popular package manager for Kubernetes.
page2-elem0,In the context of the toolkit Helm is the preferred option to deploy Kubernetes resources.
page2-elem0,Alternatively you could deploy Kubernetes resources directly via yaml files.
page2-elem0,However the advantage of Helm is that it can easily be configured for different environments via its built-in templating mechanism.
page2-elem0,Another advantage of Helm in this context is that Terraform variables can easily be mapped to Helm values which is described below.
page2-elem0,To understand the following content I suggest to read these documents that give some background:
page2-elem0,* Blog: Deploying Kubernetes Resources via GitOps [http://heidloff.net/article/deploying-kubernetes-resources-via-gitops/]
page2-elem0,* Blog: Understanding TechZone Toolkit GitOps Modules [http://heidloff.net/article/understanding-techzone-toolkit-gitops-modules/]
page2-elem0,* Red Hat blog: Continuous Delivery with Helm and Argo CD [https://cloud.redhat.com/blog/continuous-delivery-with-helm-and-argo-cd]
page2-elem0,* Argo CD documentation: Helm [https://argo-cd.readthedocs.io/en/stable/user-guide/helm/]
page2-elem0,* Toolkit documentation: Develop an own GitOps module [https://github.com/cloud-native-toolkit/site-operator-guide/blob/bed61cb15079cc998ec49a15454de4d671c6ad42/docs/learn/iascable/lab4/index.md]
page2-elem0,Let’s look at a concrete example.
page2-elem0,We developed a TechZone Toolkit GitOps module to deploy Watson NLP [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp].
page2-elem0,The repo of the module includes the Helm chart.
page2-elem0,The chart expects values [https://github.com/IBM/watson-automation/blob/0ba8ec48a8c3695b717db9ca55217706e44ecb0f/helm-nlp/values.yaml] in the following format for example to define which NLP models and versions you want to deploy.
page2-elem0,componentName: watson-nlp
page2-elem0,acceptLicense: false
page2-elem0,serviceType: ClusterIP
page2-elem0,imagePullSecrets:
page2-elem0,- ibm-entitlement-key
page2-elem0,registries:
page2-elem0,- name: watson
page2-elem0,url: cp.icr.io/cp/ai
page2-elem0,runtime:
page2-elem0,registry: watson
page2-elem0,image: watson-nlp-runtime:1.0.15
page2-elem0,models:
page2-elem0,- registry: watson
page2-elem0,image: watson-nlp_syntax_izumo_lang_en_stock:1.0.5
page2-elem0,To allow Argo CD to deploy Watson NLP via the Helm the Helm chart itself as well as the specific values.yaml file need to be put in the GitOps repo.
page2-elem0,GitOps modules are just Terraform modules but follow additional conventions defined by the toolkit to access the automatically provisioned GitOps repo.
page2-elem0,Input variables of Terraform (GitOps) modules can be defined in variables.tf [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/4c22e5bba2023602bdd8e4a1a1634b4d024ee937/variables.tf].
page2-elem0,To enable the toolkit to put the chart as well as the values into the Git repo the following steps are necessary.
page2-elem0,First the Terraform input variables are converted to Helm values into JSON format in main.tf [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/4c22e5bba2023602bdd8e4a1a1634b4d024ee937/main.tf#L7-L50].
page2-elem0,values_content = {
page2-elem0,"componentName" = "embedded"
page2-elem0,"acceptLicense" = var.accept_license
page2-elem0,"serviceType" = "ClusterIP"
page2-elem0,"registries" = var.registries
page2-elem0,"imagePullSecrets" = var.imagePullSecrets
page2-elem0,"runtime" = {
page2-elem0,"registry": var.runtime_registry
page2-elem0,"image": var.runtime_image
page2-elem0,"models" = var.models
page2-elem0,layer = "services"
page2-elem0,...
page2-elem0,resource null_resource create_yaml {
page2-elem0,provisioner "local-exec" {
page2-elem0,command = "${path.module}/scripts/create-yaml.sh '${local.name}' '${local.yaml_dir}'"
page2-elem0,environment = {
page2-elem0,VALUES_CONTENT = yamlencode(local.values_content)
page2-elem0,After this the chart and the values are copied into the ‘services’ payload directory via a script [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/4c22e5bba2023602bdd8e4a1a1634b4d024ee937/scripts/create-yaml.sh].
page2-elem0,#!/usr/bin/env bash
page2-elem0,SCRIPT_DIR=$(cd $(dirname "$0"); pwd -P)
page2-elem0,MODULE_DIR=$(cd "${SCRIPT_DIR}/.."
page2-elem0,; pwd -P)
page2-elem0,CHART_DIR=$(cd "${MODULE_DIR}/chart/watson-nlp"; pwd -P)
page2-elem0,DEST_DIR="$2"
page2-elem0,## Add logic here to put the yaml resource content in DEST_DIR
page2-elem0,mkdir -p "${DEST_DIR}"
page2-elem0,cp -R "${CHART_DIR}/"* "${DEST_DIR}"
page2-elem0,if [[ -n "${VALUES_CONTENT}" ]]; then
page2-elem0,echo "${VALUES_CONTENT}" > "${DEST_DIR}/values.yaml"
page2-elem0,fi
page2-elem0,find "${DEST_DIR}" -name "*"
page2-elem0,echo "Files in output path"
page2-elem0,ls -l "${DEST_DIR}"
page2-elem0,After the toolkit module has been deployed you’ll find everything in your own GitOps repo as shown in the screenshot above.
page2-elem0,The values.yaml file does not contain the default chart values but the input variables of your Terraform module.
page2-elem0,To change deployments you can simply change the configuration in the GitOps repo for example to update to a later version of Watson NLP.
page2-elem0,Argo CD will be triggered automatically to synchronize the desired state with the actual state.
page2-elem0,Combination of Terraform and Argo CD
page2-elem0,As you’ve seen the TechZone Toolkit uses a combination of Terraform and Argo CD.
page2-elem0,Initially Terraform takes the lead to deploy infrastructure components like VPCs Kubernetes clusters Argo CD within clusters and GitOps repos.
page2-elem0,After this Argo CD is triggered to deploy the initial version of resources as defined in the GitOps repos.
page2-elem0,However after the initial deployments Argo CD takes over the lead.
page2-elem0,To deploy newer versions of Kubernetes resources changes can simply be push to Git.
page2-elem0,When the same Terraform modules are invoked later for example to deploy more infrastructure components the toolkit does not modify the already deployed Git repo!
page2-elem0,To find out more about these capabilities check out the following resources:
page2-elem0,* TechZone Accelerator Toolkit Documentation [https://operate.cloudnativetoolkit.dev/]
page2-elem0,* TechZone Accelerator Toolkit Modules [https://operate.cloudnativetoolkit.dev/]
page2-elem0,* TechZone Accelerator Toolkit CLI (iascable) [https://github.com/cloud-native-toolkit/iascable]
page2-elem0,* Sample GitOps Module: UBI [https://github.com/cloud-native-toolkit/terraform-gitops-ubi]
page2-elem0,* Sample GitOps Module: Watson NLP [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp]
page2-elem0,* Sample BOMs to deploy Watson NLP [https://github.com/IBM/watson-automation]
page2-elem0,The post Developing TechZone Toolkit GitOps Modules with Helm [http://heidloff.net/article/developing-techzone-toolkit-gitops-modules-with-helm/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem1,Running IBM Watson NLP in Minikube.
page2-elem1,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page2-elem1,Via REST and gRCP APIs AI can easily be embedded in applications.
page2-elem1,This post describes how to run Watson NLP locally in Minikube.
page2-elem1,To set some context check out the landing page IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page2-elem1,The Watson NLP containers can be run on different container platforms they provide REST and gRCP interfaces they can be extended with custom models and they can easily be embedded in solutions.
page2-elem1,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51726] is available.
page2-elem1,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page2-elem1,How to run NLP locally in Minikube
page2-elem1,My post Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/] explained how to run Watson NLP locally in Docker.
page2-elem1,The instructions below describe how to deploy Watson NLP locally to Minikube via the Watson NLP Helm chart [https://github.com/IBM/watson-automation/blob/90e61e05a5d0eacd268c97fc3c8b67e285c99241/documentation/NLPHelmChart.md].
page2-elem1,First you need to install Minikube for example via brew on MacOS.
page2-elem1,Next Minikube needs to be started with more memory and disk size than the Minikube defaults.
page2-elem1,I’ve used the settings below which is more than required but I wanted to leave space for other applications.
page2-elem1,Note that you also need to give your container runtime more resources.
page2-elem1,For example if you use Docker Desktop go to Preferences-Resources and define your settings.
page2-elem1,$ brew install minikube
page2-elem1,$ minikube start --cpus 12 --memory 16000 --disk-size 50g
page2-elem1,For some reason in my setup the watson-nlp-runtime image couldn’t be pulled by the Deployment resource/operator.
page2-elem1,I guess it’s related to the big size of the image.
page2-elem1,I’ve found this workaround:
page2-elem1,$ eval $(minikube docker-env)
page2-elem1,$ docker login cp.icr.io --username cp --password <entitlement_key>
page2-elem1,$ docker pull cp.icr.io/cp/ai/watson-nlp-runtime:1.0.18
page2-elem1,Next the namespace and secret need to be created.
page2-elem1,$ kubectl create namespace watson-demo
page2-elem1,$ kubectl config set-context --current --namespace=watson-demo
page2-elem1,$ kubectl create secret docker-registry \
page2-elem1,--docker-server=cp.icr.io \
page2-elem1,--docker-username=cp \
page2-elem1,--docker-password=<your IBM Entitlement Key> \
page2-elem1,-n watson-demo \
page2-elem1,ibm-entitlement-key
page2-elem1,After this a repo with the Helm chart and another repo with a sample values.yaml [https://github.com/IBM/watson-automation/blob/94f28f12a58608f7b7fe355d36f101ddf7cd8cb8/helm-nlp/values.yaml] file are cloned and the license needs to be accepted.
page2-elem1,$ git clone https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp
page2-elem1,$ git clone https://github.com/IBM/watson-automation.git
page2-elem1,$ code watson-automation/helm-nlp/values.yaml #change acceptLicense to true
page2-elem1,$ cp watson-automation/helm-nlp/values.yaml terraform-gitops-watson-nlp/chart/watson-nlp/values.yaml
page2-elem1,componentName: watson-nlp
page2-elem1,acceptLicense: true
page2-elem1,serviceType: ClusterIP
page2-elem1,imagePullSecrets:
page2-elem1,- ibm-entitlement-key
page2-elem1,registries:
page2-elem1,- name: watson
page2-elem1,url: cp.icr.io/cp/ai
page2-elem1,runtime:
page2-elem1,registry: watson
page2-elem1,image: watson-nlp-runtime:1.0.18
page2-elem1,models:
page2-elem1,- registry: watson
page2-elem1,image: watson-nlp_syntax_izumo_lang_en_stock:1.0.7
page2-elem1,Finally the chart can be installed.
page2-elem1,$ cd terraform-gitops-watson-nlp/chart/watson-nlp
page2-elem1,$ helm install -f values.yaml watson-embedded .
page2-elem1,$ kubectl get pods -n watson-demo --watch
page2-elem1,$ kubectl get deployment/watson-embedded-watson-nlp -n watson-demo
page2-elem1,$ kubectl get svc/watson-embedded-watson-nlp -n watson-demo
page2-elem1,When you open the Kubernetes Dashboard (via ‘minikube dashboard’) you’ll see the deployed resources.
page2-elem1,The Watson NLP pod contains the watson-nlp-runtime container and a simple syntax model container.
page2-elem1,To invoke Watson NLP via REST you need to find out the IP address and port.
page2-elem1,Alternatively you could use port forwarding.
page2-elem1,$ minikube service watson-embedded-watson-nlp -n watson-demo --url
page2-elem1,$ curl -X POST "http://<ip-and-port>/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict" \
page2-elem1,-H "accept: application/json" \
page2-elem1,-H "grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock" \
page2-elem1,-H "content-type: application/json" \
page2-elem1,-d " { \"rawDocument\": { \"text\": \"It is so easy to embed Watson NLP in applications.
page2-elem1,Very cool.\" }}"
page2-elem1,The NLP containers also provides a gRCP interface [https://github.com/IBM/watson-automation#grpc].
page2-elem1,To find out more about Watson NLP check out these resources:
page2-elem1,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page2-elem1,* Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page2-elem1,* Trial [https://www.ibm.com/products/ibm-watson-natural-language-processing]
page2-elem1,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page2-elem1,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page2-elem1,* Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/]
page2-elem1,* Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/]
page2-elem1,* Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/]
page2-elem1,The post Running IBM Watson NLP in Minikube [http://heidloff.net/article/running-ibm-watson-nlp-in-minikube/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem2,Understanding TechZone Toolkit GitOps Modules.
page2-elem2,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page2-elem2,This article explains how to deploy resources in Kubernetes clusters via GitOps.
page2-elem2,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page2-elem2,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page2-elem2,With the toolkit both infrastructure like Kubernetes clusters as well as Kubernetes resources within clusters can be deployed.
page2-elem2,Infrastructure resources are deployed via Terraform resources within clusters via Argo CD.
page2-elem2,To deploy resources in Kubernetes clusters DevOps modules are used which can be found in the TechZone Module Catalog [https://modules.cloudnativetoolkit.dev/].
page2-elem2,The TechZone Toolkit uses Argo CD for GitOps which is deployed automatically.
page2-elem2,Argo CD requires a Git repo to store the desired state which it continuously synchronizes with the actual state in the cluster.
page2-elem2,Read my blog Deploying Kubernetes Resources via GitOps [http://heidloff.net/article/deploying-kubernetes-resources-via-gitops/] for an introduction to GitOps.
page2-elem2,Let’s look how the toolkit works for a concrete sample [https://github.com/ibm/watson-automation] where Watson NLP is deployed to OpenShift via GitOps.
page2-elem2,First you define the modules argocd-bootstrap and gitops-repo in the BOM (bill of material).
page2-elem2,apiVersion: cloudnativetoolkit.dev/v1alpha1
page2-elem2,kind: BillOfMaterial
page2-elem2,metadata:
page2-elem2,name: cluster-with-watson-nlp
page2-elem2,spec:
page2-elem2,modules:
page2-elem2,- name: ibm-ocp-vpc
page2-elem2,- name: argocd-bootstrap
page2-elem2,- name: gitops-repo
page2-elem2,- name: terraform-gitops-ubi
page2-elem2,- name: terraform-gitops-watson-nlp
page2-elem2,To configure the GitOps module change the configuration in variables.yaml [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/output/cluster-with-watson-nlp/variables-template.yaml#L31-L42].
page2-elem2,# gitops
page2-elem2,- name: gitops_repo_repo
page2-elem2,description: The name of the gitops repository that will be created
page2-elem2,value: xxx
page2-elem2,- name: gitops_repo_host
page2-elem2,value: github.com
page2-elem2,- name: gitops_repo_org
page2-elem2,value: xxx
page2-elem2,- name: gitops_repo_username
page2-elem2,value: xxx
page2-elem2,After applying the Terraform modules a GitOps repo will be created with a specific structure [https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template] that the toolkit expects.
page2-elem2,There are two major types of resources in these repos:
page2-elem2,1.
page2-elem2,ArgoCD configuration
page2-elem2,2.
page2-elem2,Application ‘payloads’
page2-elem2,ArgoCD configuration
page2-elem2,In Argo CD collections of kubernetes resources that are deployed together are called “applications”.
page2-elem2,Applications in ArgoCD are configured using a custom resource definition (CRD) in the cluster which means ArgoCD applications can deploy other ArgoCD applications (called the ‘App of Apps pattern [https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/#app-of-apps-pattern]‘).
page2-elem2,With this pattern the Argo CD environment can be bootstrapped with an initial application.
page2-elem2,That initial bootstrap application can then be updated in the GitOps repository to configure other applications.
page2-elem2,Application ‘payloads’
page2-elem2,The ArgoCD configuration points to other paths within the GitOps repository that contain the actual “payload” yaml to provision the applications (the deployments config maps etc that make up the applications).
page2-elem2,In addition to separating the Argo CD configuration from the application ‘payloads’ the configuration has also been divided into three different layers of the cluster configuration:
page2-elem2,1.
page2-elem2,Infrastructure: Foundational elements within the cluster like namespaces service accounts role-based access control etc. These resources are often managed by the infrastructure team and are required by the other resources.
page2-elem2,2.
page2-elem2,Shared services: Shared services are application components that are used across multiple applications or across the cluster.
page2-elem2,Often these are operator-based services and managed independently from the applications.
page2-elem2,3.
page2-elem2,Applications: The application layer contains the applications deployed to the cluster using the infrastructure and shared service components.
page2-elem2,Let’s look at the Watson NLP GitOps module example.
page2-elem2,In the directory ‘argocd/2-services’ the source of the Argo CD application is defined which resides in the same repo in the ‘payload/2-services’ directory.
page2-elem2,Helm is used for the actual deployment of the Watson NLP resources.
page2-elem2,Helm is the preferred solution of the toolkit since it allows easy configurations for different environments based on its built-in templating mechanism.
page2-elem2,The Argo CD dashboard shows the registered applications and their synchronization states.
page2-elem2,Additionally the dashboard shows for each application which Kubernetes resources have been deployed.
page2-elem2,To change deployments you can simply change the configuration in the GitOps repo for example to update to a later version of Watson NLP.
page2-elem2,Argo CD will be triggered automatically to synchronize the desired state with the actual state.
page2-elem2,To find out more about these capabilities check out the following resources:
page2-elem2,* TechZone Accelerator Toolkit Documentation [https://operate.cloudnativetoolkit.dev/]
page2-elem2,* TechZone Accelerator Toolkit Modules [https://operate.cloudnativetoolkit.dev/]
page2-elem2,* TechZone Accelerator Toolkit CLI (iascable) [https://github.com/cloud-native-toolkit/iascable]
page2-elem2,* Sample GitOps Module: UBI [https://github.com/cloud-native-toolkit/terraform-gitops-ubi]
page2-elem2,* Sample GitOps Module: Watson NLP [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp]
page2-elem2,* Sample BOMs to deploy Watson NLP [https://github.com/IBM/watson-automation]
page2-elem2,The post Understanding TechZone Toolkit GitOps Modules [http://heidloff.net/article/understanding-techzone-toolkit-gitops-modules/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem3,The new Watson Assistant is awesome.
page2-elem3,IBM Watson Assistant is a SaaS offering from IBM to build conversational user experiences.
page2-elem3,The first version was already good but the new(ish) version is just awesome!
page2-elem3,Over the last years Watson Assistant has successfully been used by many IBM clients and partners.
page2-elem3,Based on the feedback from clients the IBM development and design team has created a brand new experience.
page2-elem3,This second version was released at the end of 2021.
page2-elem3,I was only able to try it recently and was positively surprised how much the offering has evolved.
page2-elem3,Below are some of my highlights.
page2-elem3,1.
page2-elem3,Actions
page2-elem3,The new Watson Assistant uses a more natural way to define conversations.
page2-elem3,The old ‘dialog’ experience was replaced with a more intuitive concept which is more goal oriented.
page2-elem3,For example Assistant users don’t use intents anymore but actions.
page2-elem3,Intents still exist internally but users want to define actions so that end users can reach their goals.
page2-elem3,Similarly Assistant users don’t have to define entities anymore.
page2-elem3,Again they still exist internally but as far as users are concerned they just define types (e.g. free text) in the actions experience and the entities are created automatically.
page2-elem3,2.
page2-elem3,Integration of Watson Discovery
page2-elem3,With the Assistant the most typical conversation flows are covered.
page2-elem3,However for advanced topics Assistant can call out to other services like Watson Discovery.
page2-elem3,This is useful if you have FAQs or other data stored in other systems that you want to expose to end users.
page2-elem3,The following screenshot shows how Assistant can return articles from my blog.
page2-elem3,It only takes five minutes to implement this.
page2-elem3,3.
page2-elem3,Much more
page2-elem3,There are so many more cool features and improvements.
page2-elem3,* Via REST APIs services can be queried or perform business logic
page2-elem3,* Integrations with phone
page2-elem3,* Integrations with chat clients and SMS
page2-elem3,* Custom integrations
page2-elem3,* Embeddable web chat widget
page2-elem3,* Hand over to agent
page2-elem3,* Free text responses
page2-elem3,* Integrated debugger in preview chat window
page2-elem3,* Clarifying question to identify the right action
page2-elem3,* Change conversation topics (switch between actions)
page2-elem3,* Built in analytics
page2-elem3,* User authentication and secure traffic
page2-elem3,* Private endpoints
page2-elem3,* Different plans for different needs including isolated deployments
page2-elem3,* High availability mechanisms
page2-elem3,* Multiple languages
page2-elem3,* Templates with lots of predefined reusable actions
page2-elem3,* Production and development environments
page2-elem3,* APIs for most common languages
page2-elem3,* …
page2-elem3,Getting started
page2-elem3,To get started try it out yourselves.
page2-elem3,There is a free lite plan [https://cloud.ibm.com/catalog/services/watson-assistant].
page2-elem3,The first time you log in a guided tour is offered.
page2-elem3,I’m usually not a fan of those but it worked very well for me to learn a lot in a short amout of time.
page2-elem3,The tour is also documented in a blog series [https://cloud.ibm.com/docs/assistant?topic=assistant-getting-started].
page2-elem3,The documentation [https://cloud.ibm.com/docs/watson-assistant?topic=watson-assistant-about] is well structured complete and easy to read.
page2-elem3,The post The new Watson Assistant is awesome [http://heidloff.net/article/the-new-watson-assistant-is-awesome/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem4,Developing TechZone Toolkit Terraform Modules.
page2-elem4,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page2-elem4,This article explains on a high level how to develop new modules with Terraform.
page2-elem4,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page2-elem4,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page2-elem4,With the toolkit both infrastructure like Kubernetes clusters as well as Kubernetes resources within clusters can be deployed.
page2-elem4,Infrastructure resources are deployed via Terraform resources within clusters via Argo CD.
page2-elem4,Custom Modules and custom Catalogs
page2-elem4,The toolkit is available as open source and it is extensible.
page2-elem4,Custom modules can be added to deploy more software or to add other target platforms.
page2-elem4,The TechZone Module Catalog [https://modules.cloudnativetoolkit.dev/] contains a list of curated modules which need to provide automatic testing capabilities.
page2-elem4,However the curated catalog doesn’t have to be used or can be used in addition to a custom catalog.
page2-elem4,This is necessary if you want to build modules for internal consumption only and it is necessary for modules while they are being developed.
page2-elem4,My colleague Thomas Südbröcker has documented how to create your own catalog [https://github.com/cloud-native-toolkit/site-operator-guide/blob/e0f2302f7d67c185edd63d71e2612ddf078bb34f/docs/learn/iascable/lab4/index.md#6-create-an-own-catalog].
page2-elem4,When running the ‘iascable’ CLI to generate Terraform modules based on BOMs (bill of materials) the locations of the catalogs can be passed in.
page2-elem4,Alternatively you can also define the catalogs directly in the BOMs.
page2-elem4,$ BASE_CATALOG=https://modules.cloudnativetoolkit.dev/index.yaml
page2-elem4,$ CUSTOM_CATALOG=https://raw.githubusercontent.com/Vishal-Ramani/UBI-helm-module-example/main/example/catalog/ubi-helm-catalog.yaml
page2-elem4,$ iascable build -i ibm-vpc-roks-argocd-ubi.yaml -c $BASE_CATALOG -c $CUSTOM_CATALOG
page2-elem4,Terraform Modules
page2-elem4,The TechZone Toolkit provides two types of modules:
page2-elem4,1.
page2-elem4,Terraform modules
page2-elem4,2.
page2-elem4,GitOps modules
page2-elem4,Terraform modules are used to create infrastructure like clusters VPCs external resources and more.
page2-elem4,The GitOps modules are used to deploy and operate different types of software within clusters.
page2-elem4,The Toolkit Terraform modules are just Terraform modules with some extended conventions how to build them.
page2-elem4,Modules contain these files [https://modules.cloudnativetoolkit.dev/#/how-to/terraform]:
page2-elem4,* main.tf: Logic of the module
page2-elem4,* variables.tf: Input variables
page2-elem4,* outputs.tf: Output variables which can be passed to child modules
page2-elem4,* version.tf: Minimum required Terraform version
page2-elem4,* module.yaml: Metadata descriptor
page2-elem4,* README.md: Documentation
page2-elem4,The best way to get started building modules is to look at the available modules in the catalog [https://modules.cloudnativetoolkit.dev/].
page2-elem4,The module catalog provides a filter ‘Module type’.
page2-elem4,Browse through the existing modules and pick one which sounds similar to what you want to achieve or simple enough to use it as template or starting point.
page2-elem4,To find out more about these capabilities check out the following resources:
page2-elem4,* TechZone Accelerator Toolkit Documentation [https://operate.cloudnativetoolkit.dev/]
page2-elem4,* TechZone Accelerator Toolkit Modules [https://operate.cloudnativetoolkit.dev/]
page2-elem4,* TechZone Accelerator Toolkit CLI (iascable) [https://github.com/cloud-native-toolkit/iascable]
page2-elem4,* Sample GitOps Module: UBI [https://github.com/cloud-native-toolkit/terraform-gitops-ubi]
page2-elem4,* Sample GitOps Module: Watson NLP [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp]
page2-elem4,* Sample BOMs to deploy Watson NLP [https://github.com/IBM/watson-automation]
page2-elem4,The post Developing TechZone Toolkit Terraform Modules [http://heidloff.net/article/developing-techzone-toolkit-terraform-modules/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem5,Running IBM Watson Text to Speech in Containers.
page2-elem5,IBM Watson NLP (Natural Language Understanding) and Watson Speech containers can be run locally on-premises or Kubernetes and OpenShift clusters.
page2-elem5,Via REST and WebSockets APIs AI can easily be embedded in applications.
page2-elem5,This post describes how to run Watson Text to Speech locally.
page2-elem5,To set some context here are the descriptions of IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries] and the Watson Text to Speech (TTS) library.
page2-elem5,> Build your applications with enterprise-grade speech technology: IBM Watson Speech Libraries for Embed are a set of containerized text-to-speech and speech-to-text libraries designed to offer our IBM partners greater flexibility to infuse the best of IBM Research technology into their solutions.
page2-elem5,Now available as embeddable AI partners gain greater capabilities to build voice transcription and voice synthesis applications more quickly and deploy them in any hybrid multi-cloud environment.
page2-elem5,> The Watson TTS library converts written text into natural-sounding voice in a variety of languages for real-time speech synthesis.
page2-elem5,Offered as a containerized library developers can build applications quickly with interoperable and production scalable components to run their speech tasks anywhere.
page2-elem5,The Watson Text to Speech library is available as containers providing REST and WebSockets interfaces.
page2-elem5,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Cloud SaaS service for TTS and IBM Cloud Pak for Data.
page2-elem5,To try it a trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754] is available.
page2-elem5,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726].
page2-elem5,How to run TTS locally via Docker
page2-elem5,To run STT as container the container image needs to be built first.
page2-elem5,Different models [https://www.ibm.com/docs/en/watson-libraries?topic=home-models-catalog] are provided for different languages and use cases.
page2-elem5,There is a sample [https://github.com/ibm-build-lab/Watson-Speech/tree/main/single-container-tts] that describes how to run TTS with two speech models locally [https://www.ibm.com/docs/en/watson-libraries?topic=rc-run-docker-run].
page2-elem5,In a first terminal execute these commands to build and run the container:
page2-elem5,$ docker login cp.icr.io --username cp --password <entitlement_key>
page2-elem5,$ git clone https://github.com/ibm-build-lab/Watson-Speech.git
page2-elem5,$ cd Watson-Speech/single-container-tts
page2-elem5,$ docker build .
page2-elem5,-t tts-standalone
page2-elem5,$ docker run --rm -it --env ACCEPT_LICENSE=true --publish 1080:1080 tts-standalone
page2-elem5,In second terminal invoke these commands to invoke a REST API:
page2-elem5,$ cd Watson-Speech/single-container-stt
page2-elem5,$ curl "http://localhost:1080/text-to-speech/api/v1/synthesize" \
page2-elem5,--header "Content-Type: application/json" \
page2-elem5,--data '{"text":"Hello world"}' \
page2-elem5,--header "Accept: audio/wav" \
page2-elem5,--output output.wav
page2-elem5,$ ls -la
page2-elem5,$ curl "http://localhost:1080/text-to-speech/api/v1/voices"
page2-elem5,Here is a screenshot of the container in action:
page2-elem5,To define which models you want to put in your image a multi stage Dockerfile [https://github.com/ibm-build-lab/Watson-Speech/blob/main/single-container-tts/Dockerfile] is used.
page2-elem5,# Model images
page2-elem5,FROM cp.icr.io/cp/ai/watson-tts-generic-models:1.0.0 AS catalog
page2-elem5,# Add additional models here
page2-elem5,FROM cp.icr.io/cp/ai/watson-tts-en-us-michaelv3voice:1.0.0 AS en-us-voice
page2-elem5,FROM cp.icr.io/cp/ai/watson-tts-fr-ca-louisev3voice:1.0.0 AS fr-ca-voice
page2-elem5,# Base image for the runtime
page2-elem5,FROM cp.icr.io/cp/ai/watson-tts-runtime:1.0.0 AS runtime
page2-elem5,# Environment variable used for directory where configurations are mounted
page2-elem5,ENV CONFIG_DIR=/opt/ibm/chuck.x86_64/var
page2-elem5,# Copy in the catalog and runtime configurations
page2-elem5,COPY --chown=watson:0 --from=catalog catalog.json ${CONFIG_DIR}/catalog.json
page2-elem5,COPY --chown=watson:0 ./config/* ${CONFIG_DIR}/
page2-elem5,# Intermediate image to populate the model cache
page2-elem5,FROM runtime as model_cache
page2-elem5,# Copy model archives from model images
page2-elem5,RUN sudo mkdir -p /models/pool2
page2-elem5,# For each additional models copy the line below with the model image
page2-elem5,COPY --chown=watson:0 --from=en-us-voice model/* /models/pool2/
page2-elem5,COPY --chown=watson:0 --from=fr-ca-voice model/* /models/pool2/
page2-elem5,# Run script to initialize the model cache from the model archives
page2-elem5,COPY ./prepareModels.sh .
page2-elem5,RUN ./prepareModels.sh
page2-elem5,# Final runtime image with models baked in
page2-elem5,FROM runtime as release
page2-elem5,COPY --from=model_cache ${CONFIG_DIR}/cache/ ${CONFIG_DIR}/cache/
page2-elem5,To find out more about Watson Text to Speech check out these resources:
page2-elem5,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-text-speech-library-embed-home]
page2-elem5,* Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=home-models-catalog]
page2-elem5,* SaaS API docs [https://cloud.ibm.com/apidocs/text-to-speech]
page2-elem5,* Trial [https://www.ibm.com/account/reg/us-en/signup?formid=urx-51754]
page2-elem5,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page2-elem5,The post Running IBM Watson Text to Speech in Containers [http://heidloff.net/article/running-ibm-watson-text-to-speech-in-containers/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem6,Running IBM Watson NLP locally in Containers.
page2-elem6,IBM announced the general availability of Watson NLP (Natural Language Understanding) and Watson Speech containers which can be run locally on-premises or Kubernetes and OpenShift clusters.
page2-elem6,This post describes how to run Watson NLP locally.
page2-elem6,To set some context here is the description of IBM Watson NLP Library for Embed [https://www.ibm.com/products/ibm-watson-natural-language-processing].
page2-elem6,> Enhance your applications with best-in-class Natural Language AI: Introducing IBM Watson NLP Library for Embed a containerized library designed to empower IBM partners with greater flexibility to infuse powerful natural language AI into their solutions.
page2-elem6,It combines the best of open source and IBM Research NLP algorithms to deliver superior AI capabilities developers can access and integrate into their apps in the environment of their choice.
page2-elem6,Offered to partners as embeddable AI a first of its kind software portfolio that offers best of breed AI from IBM.
page2-elem6,The Watson NLP library is available as containers providing REST and gRPC interfaces.
page2-elem6,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Watson Assistant and NLU (Natural Language Understanding) SaaS services and IBM Cloud Pak for Data.
page2-elem6,Watson NLP comes with a wide variety of text processing functions such as emotion analysis and topic modeling.
page2-elem6,Watson NLP is built on top of the best AI open source software.
page2-elem6,Additionally it provides stable and supported interfaces it handles a wide range of languages and its quality is enterprise proven.
page2-elem6,To try it a trial [https://www.ibm.com/products/ibm-watson-natural-language-processing] is available.
page2-elem6,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/signup?formid=urx-51726].
page2-elem6,How to run NLP locally via Docker
page2-elem6,To run NLP as container locally you first need to define which models [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog] you want to use which address different use cases.
page2-elem6,By only picking the ones you need the size of the containers is reduced.
page2-elem6,You can also train your own models which I want to blog about separately.
page2-elem6,To define your models save the following script to ‘runNLP.sh’ and modify the fourth line.
page2-elem6,The script pulls down the models and puts them in a volume that is accessed by the NLP runtime container.
page2-elem6,#!/usr/bin/env bash
page2-elem6,IMAGE_REGISTRY=${IMAGE_REGISTRY:-"cp.icr.io/cp/ai"}
page2-elem6,RUNTIME_IMAGE=${RUNTIME_IMAGE:-"watson-nlp-runtime:1.0.18"}
page2-elem6,export MODELS="${MODELS:-"watson-nlp_syntax_izumo_lang_en_stock:1.0.7watson-nlp_syntax_izumo_lang_fr_stock:1.0.7"}"
page2-elem6,IFS='' read -ra models_arr <<< "${MODELS}"
page2-elem6,# Create a shared volume and initialize with open permissions
page2-elem6,docker volume rm model_data 2>/dev/null || true
page2-elem6,docker volume create --label model_data
page2-elem6,docker run --rm -it -v model_data:/model_data alpine chmod 777 /model_data
page2-elem6,# Put models into the shared volume
page2-elem6,for model in "${models_arr[@]}"
page2-elem6,do
page2-elem6,docker run --rm -it -v model_data:/app/models -e ACCEPT_LICENSE=true $IMAGE_REGISTRY/$model
page2-elem6,done
page2-elem6,# Run the runtime with the models mounted
page2-elem6,docker run ${@} \
page2-elem6,--rm -it \
page2-elem6,-v model_data:/app/model_data \
page2-elem6,-e ACCEPT_LICENSE=true \
page2-elem6,-e LOCAL_MODELS_DIR=/app/model_data \
page2-elem6,-p 8085:8085 \
page2-elem6,-p 8080:8080 \
page2-elem6,$tls_args $IMAGE_REGISTRY/$RUNTIME_IMAGE
page2-elem6,To download the models and run the container invoke these commands in a first terminal:
page2-elem6,$ docker login cp.icr.io --username cp --password <entitlement_key>
page2-elem6,$ ./runNLP.sh
page2-elem6,In second terminal invoke this command to invoke a REST API:
page2-elem6,$ curl -X POST "http://localhost:8080/v1/watson.runtime.nlp.v1/NlpService/SyntaxPredict" \
page2-elem6,-H "accept: application/json" \
page2-elem6,-H "grpc-metadata-mm-model-id: syntax_izumo_lang_en_stock" \
page2-elem6,-H "content-type: application/json" \
page2-elem6,-d " { \"rawDocument\": { \"text\": \"It is so easy to embed Watson NLP in applications.
page2-elem6,Very cool.\" }}"
page2-elem6,Here is a screenshot of the container in action:
page2-elem6,You can invoke the Swagger (OpenAI) user interface by opening http://localhost:8080/swagger.
page2-elem6,The NLP containers also provides a gRCP interface [https://github.com/IBM/watson-automation#grpc].
page2-elem6,To find out more about Watson NLP check out these resources:
page2-elem6,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page2-elem6,* Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=models-catalog]
page2-elem6,* Trial [https://www.ibm.com/products/ibm-watson-natural-language-processing]
page2-elem6,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page2-elem6,* Automation for Watson NLP Deployments [https://github.com/IBM/watson-automation]
page2-elem6,The post Running IBM Watson NLP locally in Containers [http://heidloff.net/article/running-ibm-watson-nlp-locally-in-containers/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem7,Running IBM Watson Speech to Text in Containers.
page2-elem7,IBM announced the general availability of Watson NLP and Watson Speech containers which can be run locally on-premises or Kubernetes and OpenShift clusters.
page2-elem7,This post describes how to run Speech to Text (STT) locally.
page2-elem7,To set some context here are the descriptions of IBM Watson Speech Libraries for Embed [https://www.ibm.com/products/watson-speech-embed-libraries] and the Watson Speech to Text library.
page2-elem7,> Build your applications with enterprise-grade speech technology: IBM Watson Speech Libraries for Embed are a set of containerized text-to-speech and speech-to-text libraries designed to offer our IBM partners greater flexibility to infuse the best of IBM Research technology into their solutions.
page2-elem7,Now available as embeddable AI partners gain greater capabilities to build voice transcription and voice synthesis applications more quickly and deploy them in any hybrid multi-cloud environment.
page2-elem7,> Watson STT library uses natural language AI technology to understand the human voice and turn it into usable searchable text.
page2-elem7,As an embeddable AI library developers have greater access to the best of IBM Watson Speech technology and IBM Research algorithms to build voice transcription and voice synthesis applications faster: 1.
page2-elem7,Accuracy out-of-box with advanced training techniques.
page2-elem7,2.
page2-elem7,Customization tools to tailor the models for your specific domain.
page2-elem7,The Watson Speech to Text library is available as containers providing REST and WebSockets interfaces.
page2-elem7,While this offering is new the underlaying functionality has been used and optimized for a long time in IBM offerings like the IBM Cloud SaaS service STT and IBM Cloud Pak for Data.
page2-elem7,STT is a speech recognition service that offers functionalities like text recognition audio preprocessing noise removal background noise separation semantic sentence conversation and how many speakers are in conversions.
page2-elem7,To try it a trial [https://www.ibm.com/products/watson-speech-embed-libraries] is available.
page2-elem7,The container images are stored in an IBM container registry that is accessed via an IBM Entitlement Key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726].
page2-elem7,How to run STT locally via Docker
page2-elem7,To run STT as container the container image needs to be built first.
page2-elem7,Different speech models [https://www.ibm.com/docs/en/watson-libraries?topic=wtsleh-models-catalog] are provided for different languages and different voices.
page2-elem7,There is a sample [https://github.com/ibm-build-lab/Watson-Speech/tree/main/single-container-stt] that describes how to run STT with two speech models locally [https://www.ibm.com/docs/en/watson-libraries?topic=rc-run-docker-run-1].
page2-elem7,In a first terminal execute these commands to build and run the container:
page2-elem7,$ docker login cp.icr.io --username cp --password <entitlement_key>
page2-elem7,$ git clone https://github.com/ibm-build-lab/Watson-Speech.git
page2-elem7,$ cd Watson-Speech/single-container-stt
page2-elem7,$ docker build .
page2-elem7,-t speech-standalone
page2-elem7,$ docker run -e ACCEPT_LICENSE=true --rm --publish 1080:1080 speech-standalone
page2-elem7,In second terminal invoke these commands to invoke a REST API:
page2-elem7,$ cd Watson-Speech/single-container-stt
page2-elem7,$ curl "http://localhost:1080/speech-to-text/api/v1/recognize" \
page2-elem7,--header "Content-Type: audio/wav" \
page2-elem7,--data-binary @sample_dataset/en-quote-1.wav
page2-elem7,Here is a screenshot of the container in action:
page2-elem7,To define which models you want to put in your image a multi stage Dockerfile [https://github.com/ibm-build-lab/Watson-Speech/blob/main/single-container-stt/Dockerfile] is used.
page2-elem7,# Model images
page2-elem7,FROM cp.icr.io/cp/ai/watson-stt-generic-models:1.0.0 as catalog
page2-elem7,# Add additional models here
page2-elem7,FROM cp.icr.io/cp/ai/watson-stt-en-us-multimedia:1.0.0 as en-us-multimedia
page2-elem7,FROM cp.icr.io/cp/ai/watson-stt-fr-fr-multimedia:1.0.0 as fr-fr-multimedia
page2-elem7,# Base image for the runtime
page2-elem7,FROM cp.icr.io/cp/ai/watson-stt-runtime:1.0.0 AS runtime
page2-elem7,# Environment variable used for directory where configurations are mounted
page2-elem7,ENV CONFIG_DIR=/opt/ibm/chuck.x86_64/var
page2-elem7,# Copy in the catalog and runtime configurations
page2-elem7,COPY --chown=watson:0 --from=catalog catalog.json ${CONFIG_DIR}/catalog.json
page2-elem7,COPY --chown=watson:0 ./${LOCAL_DIR}/* ${CONFIG_DIR}/
page2-elem7,# Intermediate image to populate the model cache
page2-elem7,FROM runtime as model_cache
page2-elem7,# Copy model archives from model images
page2-elem7,RUN sudo mkdir -p /models/pool2
page2-elem7,# For each additional models copy the line below with the model image
page2-elem7,COPY --chown=watson:0 --from=en-us-multimedia model/* /models/pool2/
page2-elem7,COPY --chown=watson:0 --from=fr-fr-multimedia model/* /models/pool2/
page2-elem7,# Run script to initialize the model cache from the model archives
page2-elem7,COPY ./prepareModels.sh .
page2-elem7,RUN ./prepareModels.sh
page2-elem7,# Final runtime image with models baked in
page2-elem7,FROM runtime as release
page2-elem7,COPY --from=model_cache ${CONFIG_DIR}/cache/ ${CONFIG_DIR}/cache/
page2-elem7,To find out more about Watson Speech to Text check out these resources:
page2-elem7,* Documentation [https://www.ibm.com/docs/en/watson-libraries?topic=watson-text-speech-library-embed-home]
page2-elem7,* Model catalog [https://www.ibm.com/docs/en/watson-libraries?topic=wtsleh-models-catalog]
page2-elem7,* SaaS model catalog [https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models]
page2-elem7,* SaaS API docs [https://cloud.ibm.com/apidocs/speech-to-text]
page2-elem7,* Trial [https://www.ibm.com/products/watson-speech-embed-libraries]
page2-elem7,* Entitlement key [https://www.ibm.com/account/reg/us-en/subscribe?formid=urx-51726]
page2-elem7,The post Running IBM Watson Speech to Text in Containers [http://heidloff.net/article/running-ibm-watson-speech-to-text-in-containers/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem8,Setting up the TechZone Accelerator Toolkit.
page2-elem8,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page2-elem8,This article explains how to set up environments to use the Toolkit CLI.
page2-elem8,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page2-elem8,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page2-elem8,The Accelerator Toolkit comes with a CLI called iascable [https://github.com/cloud-native-toolkit/iascable] which converts BOMs (bill of materials/custom solution definitions) into Terraform assets.
page2-elem8,To run Terraform additional CLIs are needed in specific versions for example kubectl oc jq git helm etc.
page2-elem8,To simplify the setup of these tools a container image is provided which comes with everything you need.
page2-elem8,Here is an example flow of commands that show how to install the CLI how to run the container and how to run Terraform.
page2-elem8,Setup the CLI clone a sample repo and generate Terraform:
page2-elem8,$ curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh
page2-elem8,$ git clone https://github.com/IBM/watson-automation
page2-elem8,$ cd watson-deployments/roks-new-nlp
page2-elem8,$ iascable build -i bom.yaml
page2-elem8,$ cd output
page2-elem8,Launch the container:
page2-elem8,$ ./launch.sh
page2-elem8,Apply Terraform:
page2-elem8,$ cd cluster-with-watson-nlp
page2-elem8,$ ./apply.sh
page2-elem8,Watch this short video starting at 2:53 min [https://youtu.be/8lbVRAvJgy4?t=173] for a demo:
page2-elem8,The toolkit provides two alternatives to run the image:
page2-elem8,1.
page2-elem8,Docker
page2-elem8,2.
page2-elem8,Multipass
page2-elem8,There are some additional environments (Podman and Colima) that are used within the community but these are not supported and cannot be guaranteed to work.
page2-elem8,While Docker is easier to use Multipass is provided as alternative if you don’t want or cannot run Docker Desktop.
page2-elem8,Here is the definition from the Multipass [https://multipass.run/] home page.
page2-elem8,> Ubuntu VMs on demand for any workstation.
page2-elem8,Get an instant Ubuntu VM with a single command.
page2-elem8,Multipass can launch and run virtual machines and configure them with cloud-init like a public cloud.
page2-elem8,The following options are currently supported for recent versions of Linux MacOS and Windows:
page2-elem8,* Linux: Docker Engine
page2-elem8,* MacOS: 1.
page2-elem8,Docker Desktop 2.
page2-elem8,Multipass
page2-elem8,* Windows: Windows Subsystem for Linux running Ubuntu image with Docker Engine installed
page2-elem8,Follow the instructions in the Accelerator Toolkit documentation for details.
page2-elem8,* Supported runtime environments [https://operate.cloudnativetoolkit.dev/getting-started/setup/#supported-runtime-environments]
page2-elem8,* Installing the environment [https://operate.cloudnativetoolkit.dev/tutorials/1-setup/#installing-the-environment]
page2-elem8,The post Setting up the TechZone Accelerator Toolkit [http://heidloff.net/article/setting-up-the-techzone-accelerator-toolkit/] appeared first on Niklas Heidloff [http://heidloff.net].
page2-elem9,Deploying TechZone Toolkit Modules on existing Clusters.
page2-elem9,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page2-elem9,This article explains how to deploy resources on existing OpenShift clusters.
page2-elem9,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page2-elem9,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page2-elem9,With the toolkit both infrastructure like Kubernetes clusters as well as resources within Kubernetes clusters can be deployed.
page2-elem9,Infrastructure resources are deployed via Terraform resources within clusters via Argo CD.
page2-elem9,In some cases you might already have clusters and only want to set up resources within these clusters.
page2-elem9,Additionally when developing your own modules for the toolkit you often want to skip creations of clusters since it takes too much time.
page2-elem9,To automate the deployments of Watson containers [https://github.com/IBM/watson-automation] to embed AI in custom applications we’ve created a repo.
page2-elem9,The repo contains documentation how to set up an OpenShift cluster with Watson containers and also documentation how to deploy the Watson containers to existing clusters.
page2-elem9,The following sample [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/bom.yaml] shows how an OpenShift cluster is created in the IBM Cloud which comes with Argo CD a GitOps repo Watson NLP and a sample application based on ubi.
page2-elem9,apiVersion: cloudnativetoolkit.dev/v1alpha1
page2-elem9,kind: BillOfMaterial
page2-elem9,metadata:
page2-elem9,name: cluster-with-watson-nlp
page2-elem9,spec:
page2-elem9,modules:
page2-elem9,- name: ibm-ocp-vpc
page2-elem9,- name: argocd-bootstrap
page2-elem9,- name: gitops-repo
page2-elem9,- name: terraform-gitops-ubi
page2-elem9,- name: terraform-gitops-watson-nlp
page2-elem9,To deploy Watson containers to existing OpenShift cluster another module called ‘ocp-login [https://github.com/cloud-native-toolkit/terraform-ocp-login]‘ can be used.
page2-elem9,apiVersion: cloudnativetoolkit.dev/v1alpha1
page2-elem9,kind: BillOfMaterial
page2-elem9,metadata:
page2-elem9,name: cluster-with-watson-nlp
page2-elem9,spec:
page2-elem9,modules:
page2-elem9,- name: ocp-login
page2-elem9,- name: argocd-bootstrap
page2-elem9,- name: gitops-repo
page2-elem9,- name: terraform-gitops-ubi
page2-elem9,- name: terraform-gitops-watson-nlp
page2-elem9,There is documentation [https://github.com/IBM/watson-automation/blob/main/documentation/Usage.md#usage-of-existing-clusters] that describes how to use the ocp-login module.
page2-elem9,You need two pieces of information that are defined in credentials.properties.
page2-elem9,* OpenShift server URL for example ‘https://cXXX-e.yy-zz.containers.cloud.ibm.com:30364’
page2-elem9,* OpenShift login token for example ‘sha256~xxx’
page2-elem9,This is the complete credentials.properties [https://github.com/IBM/watson-automation/blob/main/roks-existing-nlp/output/credentials-template.properties] file which also includes credentials to access the GitOps repo and the Watson container registry:
page2-elem9,export TF_VAR_gitops_repo_token=___your-github-token____
page2-elem9,export TF_VAR_terraform_gitops_watson_nlp_registry_credentials=___your-registry-credentials___
page2-elem9,export TF_VAR_server_url=https://cXXX-e.yy-zz.containers.cloud.ibm.com:30364
page2-elem9,export TF_VAR_cluster_login_token=sha256~xxx
page2-elem9,To obtain ‘TF_VAR_server_url’ and ‘TF_VAR_cluster_login_token’ open the OpenShift console click on your user name in the upper right corner and choose ‘copy login command’.
page2-elem9,To find out more about these capabilities check out the following resources:
page2-elem9,* Watson Automation Repo [https://github.com/IBM/watson-automation]
page2-elem9,* TechZone Accelerator Toolkit [https://operate.cloudnativetoolkit.dev/]
page2-elem9,* Watson NLP [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page2-elem9,The post Deploying TechZone Toolkit Modules on existing Clusters [http://heidloff.net/article/deploying-techzone-toolkit-modules-on-existing-clusters/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem0,Interface Descriptions of TechZone Toolkit Modules.
page3-elem0,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page3-elem0,This article explains how input and output variables of modules are defined.
page3-elem0,Check out my earlier blog that introduces the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page3-elem0,The toolkit leverages Terrafrom and GitOps and is based on best practices from IBM projects with partners and clients.
page3-elem0,Solutions are defined via bill of materials (BOMs) which contain lists of modules [https://modules.cloudnativetoolkit.dev/].
page3-elem0,In the following example [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/bom.yaml] an OpenShift cluster is created in the IBM Cloud which comes with Argo CD a GitOps repo Watson NLP and a sample application based on ubi.
page3-elem0,apiVersion: cloudnativetoolkit.dev/v1alpha1
page3-elem0,kind: BillOfMaterial
page3-elem0,metadata:
page3-elem0,name: cluster-with-watson-nlp
page3-elem0,spec:
page3-elem0,modules:
page3-elem0,- name: ibm-ocp-vpc
page3-elem0,- name: argocd-bootstrap
page3-elem0,- name: gitops-repo
page3-elem0,- name: terraform-gitops-ubi
page3-elem0,- name: terraform-gitops-watson-nlp
page3-elem0,Modules have input and output variables.
page3-elem0,Read my blog Configuring the TechZone Accelerator Toolkit [http://heidloff.net/article/configuring-the-techzone-accelerator-toolkit/] how to use input variables to configure BOMs for different scenarios.
page3-elem0,The input variables are defined by convention in the variables.tf [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/main/variables.tf] files of modules via Terraform and HCL.
page3-elem0,variable "registries" {
page3-elem0,type    = list(map(string))
page3-elem0,default = [{
page3-elem0,name = "watson"
page3-elem0,url = "cp.icr.io/cp/ai"
page3-elem0,}]
page3-elem0,The readme [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp#3-example-usage] files of the modules also describe how to use modules and pass in variables if you would use Terraform directly and not the toolkit which is useful for module developers to test their modules.
page3-elem0,module "terraform_gitops_watson_nlp" {
page3-elem0,source = "github.com/cloud-native-toolkit/terraform-gitops-watson-nlp?ref=v0.0.80"
page3-elem0,accept_license = var.terraform_gitops_watson_nlp_accept_license
page3-elem0,...
page3-elem0,The output variables are defined by convention in output.tf [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/main/outputs.tf] files.
page3-elem0,output "namespace" {
page3-elem0,description = "The namespace where the module will be deployed"
page3-elem0,value       = local.namespace
page3-elem0,depends_on  = [resource.gitops_module.setup_gitops]
page3-elem0,Some variables are defined on a global level which is useful for common variables [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/output/cluster-with-watson-nlp/variables-template.yaml] like regions resource group names and common tags.
page3-elem0,variables:
page3-elem0,- name: region
page3-elem0,description: The IBM Cloud region where the instance should be provisioned
page3-elem0,value: xxx
page3-elem0,- name: resource_group_name
page3-elem0,description: The name of the IBM Cloud resource group where the resources should be provisioned
page3-elem0,value: xxx
page3-elem0,- name: common_tags
page3-elem0,description: The list of tags that should be applied to all resources (does not work)
page3-elem0,value: []
page3-elem0,To define the values of variables for certain modules naming conventions are used.
page3-elem0,For example to define the value of ‘runtime_image [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/main/variables.tf#L110-L113]‘ …
page3-elem0,variable "runtime_image" {
page3-elem0,description = "runtime_image"
page3-elem0,default     = "watson-nlp-runtime:1.0.15"
page3-elem0,… the name of the module [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/output/cluster-with-watson-nlp/variables-template.yaml#L45] is used followed by ‘_’ and the variable name.
page3-elem0,- name: terraform_gitops_watson_nlp_runtime_image
page3-elem0,value: watson-nlp-runtime:1.0.18
page3-elem0,To find out more about these capabilities check out the following resources:
page3-elem0,* Watson Automation Repo [https://github.com/IBM/watson-automation]
page3-elem0,* TechZone Accelerator Toolkit [https://operate.cloudnativetoolkit.dev/]
page3-elem0,* Watson NLP [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page3-elem0,The post Interface Descriptions of TechZone Toolkit Modules [http://heidloff.net/article/interface-descriptions-of-techzone-toolkit-modules/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem1,Handling of Versions of TechZone Toolkit Modules.
page3-elem1,With the TechZone Accelerator Toolkit IBM software open source projects and custom applications can easily be deployed to various clouds.
page3-elem1,This article explains how to ensure that the right versions of modules are deployed.
page3-elem1,In an earlier blog I introduced the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page3-elem1,The toolkit leverages Terrafrom and GitOps and is based on best practices based on IBM experiences in partner and clients projects.
page3-elem1,Solutions are defined via bill of materials (BOM) which contain lists of modules [https://modules.cloudnativetoolkit.dev/].
page3-elem1,In the following example [https://github.com/IBM/watson-automation/blob/main/roks-new-nlp/bom.yaml] an OpenShift cluster is created in the IBM Cloud which comes with Argo CD a GitOps repo Watson NLP and a sample application based on ubi.
page3-elem1,apiVersion: cloudnativetoolkit.dev/v1alpha1
page3-elem1,kind: BillOfMaterial
page3-elem1,metadata:
page3-elem1,name: cluster-with-watson-nlp
page3-elem1,spec:
page3-elem1,modules:
page3-elem1,- name: ibm-ocp-vpc
page3-elem1,- name: argocd-bootstrap
page3-elem1,- name: gitops-repo
page3-elem1,alias: gitops_repo
page3-elem1,- name: terraform-gitops-ubi
page3-elem1,alias: terraform_gitops_ubi
page3-elem1,- name: terraform-gitops-watson-nlp
page3-elem1,alias: terraform_gitops_watson_nlp
page3-elem1,If you don’t provide any version numbers in the BOMs the toolkit installs the latest versions.
page3-elem1,Modules can have dependencies which are defined in the module.yaml [https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp/blob/4c22e5bba2023602bdd8e4a1a1634b4d024ee937/module.yaml#L15] files.
page3-elem1,dependencies:
page3-elem1,- id: gitops
page3-elem1,refs:
page3-elem1,- source: github.com/cloud-native-toolkit/terraform-tools-gitops.git
page3-elem1,version: '>= 1.1.0'
page3-elem1,- id: namespace
page3-elem1,refs:
page3-elem1,- source: github.com/cloud-native-toolkit/terraform-gitops-namespace.git
page3-elem1,version: '>= 1.0.0'
page3-elem1,- id: setup_clis
page3-elem1,refs:
page3-elem1,- source: github.com/cloud-native-toolkit/terraform-util-clis.git
page3-elem1,version: '>= 1.0.0'
page3-elem1,As in other frameworks and programming languages like JavaScript Java Go etc. the best practise is to require certain versions of dependencies.
page3-elem1,Automatic updates of modules can easily break production applications.
page3-elem1,Before updating dependencies testing needs to be done.
page3-elem1,The only exception might be security fixes but even those need to be tested.
page3-elem1,Let’s take a look how this can be done with the TechZone Accelerator Toolkit.
page3-elem1,After you have run ‘iascable build …’ on BOM files which only include high level modules without version numbers like above you will find a second BOM file (shadow BOM) in the subdirectory ‘output/bom-name/bom.yaml’.
page3-elem1,These shadow BOM files contain not only the high level modules but a complete list of all modules including dependencies.
page3-elem1,They also include the latest version numbers.
page3-elem1,apiVersion: cloudnativetoolkit.dev/v1alpha1
page3-elem1,kind: BillOfMaterial
page3-elem1,metadata:
page3-elem1,name: cluster-with-watson-nlp
page3-elem1,spec:
page3-elem1,modules:
page3-elem1,- name: gitops-repo
page3-elem1,alias: gitops_repo
page3-elem1,version: v1.22.2
page3-elem1,- name: argocd-bootstrap
page3-elem1,alias: argocd-bootstrap
page3-elem1,version: v1.12.0
page3-elem1,- name: ibm-ocp-vpc
page3-elem1,alias: cluster
page3-elem1,version: v1.16.3
page3-elem1,- name: ibm-vpc
page3-elem1,alias: ibm-vpc
page3-elem1,version: v1.17.0
page3-elem1,- name: ibm-vpc-gateways
page3-elem1,alias: ibm-vpc-gateways
page3-elem1,version: v1.10.0
page3-elem1,- name: terraform-gitops-ubi
page3-elem1,alias: terraform_gitops_ubi
page3-elem1,version: v0.0.26
page3-elem1,- name: terraform-gitops-watson-nlp
page3-elem1,alias: terraform_gitops_watson_nlp
page3-elem1,version: v1.0.0
page3-elem1,- name: olm
page3-elem1,version: v1.3.5
page3-elem1,- name: sealed-secret-cert
page3-elem1,version: v1.0.1
page3-elem1,- name: ibm-resource-group
page3-elem1,alias: resource_group
page3-elem1,version: v3.3.5
page3-elem1,- name: ibm-object-storage
page3-elem1,alias: cos
page3-elem1,version: v4.1.0
page3-elem1,- name: ibm-vpc-subnets
page3-elem1,version: v1.14.0
page3-elem1,- name: gitops-namespace
page3-elem1,alias: namespace
page3-elem1,version: v1.14.0
page3-elem1,- name: util-clis
page3-elem1,version: v1.18.1
page3-elem1,To ‘pin’ the version numbers of modules for subsequent Terraform runs a best practise is to replace the original BOM file with the generated shadow BOM file.
page3-elem1,This approach is similar to JavaScript’s package-lock.json files and Golang’s go.sum files.
page3-elem1,To find out more about these capabilities check out the following resources:
page3-elem1,* Watson Automation Repo [https://github.com/IBM/watson-automation]
page3-elem1,* TechZone Accelerator Toolkit [https://operate.cloudnativetoolkit.dev/]
page3-elem1,* Watson NLP [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page3-elem1,The post Handling of Versions of TechZone Toolkit Modules [http://heidloff.net/article/versioning-of-techzone-accelerator-toolkit-modules/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem2,Automation for embedded IBM Watson Deployments.
page3-elem2,Last week IBM announced [https://newsroom.ibm.com/2022-10-25-IBM-Helps-Ecosystem-Partners-Accelerate-AI-Adoption-by-Making-it-Easier-to-Embed-and-Scale-AI-Across-Their-Business] the general availability of Watson NLP and Watson Speech.
page3-elem2,You can now run these services via containers everywhere.
page3-elem2,I’ve created a short video [https://youtu.be/8lbVRAvJgy4] that describes how complete solutions can be deployed and how automation via Terraform and Argo CD can be used.
page3-elem2,To find out more about these capabilities check out these resources:
page3-elem2,* Watson Automation Repo [https://github.com/IBM/watson-automation]
page3-elem2,* TechZone Accelerator Toolkit [https://operate.cloudnativetoolkit.dev/]
page3-elem2,* Watson NLP [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page3-elem2,The post Automation for embedded IBM Watson Deployments [http://heidloff.net/article/automation-for-ibm-watson-deployments/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem3,Configuring the TechZone Toolkit.
page3-elem3,With the TechZone Toolkit software like OpenShift can be set up in the cloud including custom applications by using automation via Terraform and Argo CD.
page3-elem3,This article describes how to configure the toolkit or more precisely the solution definitions.
page3-elem3,In an earlier blog I explained the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page3-elem3,The toolkit leverages Terrafrom and GitOps and is based on best practices based on IBM experiences in partner and clients projects.
page3-elem3,The toolkit module catalog [https://modules.cloudnativetoolkit.dev/] provides 200+ modules to install IBM Software and open source components which can be deployed on clouds like AWS Azure and IBM Cloud.
page3-elem3,Solutions are defined in yaml files.
page3-elem3,The bill of materials contain lists of modules in this example [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/roks-new-nlp/bom.yaml#L27] OpenShift in the IBM Cloud Argo CD including a GitOps repo the Watson NLP (natural language processing) container and a custom application.
page3-elem3,apiVersion: cloudnativetoolkit.dev/v1alpha1
page3-elem3,kind: BillOfMaterial
page3-elem3,metadata:
page3-elem3,name: cluster-with-watson-nlp
page3-elem3,spec:
page3-elem3,modules:
page3-elem3,- name: ibm-ocp-vpc
page3-elem3,version: v1.16.0
page3-elem3,- name: argocd-bootstrap
page3-elem3,version: v1.12.0
page3-elem3,- name: gitops-repo
page3-elem3,alias: gitops_repo
page3-elem3,version: v1.22.2
page3-elem3,- name: terraform-gitops-ubi
page3-elem3,alias: terraform_gitops_ubi
page3-elem3,version: v0.0.8
page3-elem3,- name: terraform-gitops-watson-nlp
page3-elem3,alias: terraform_gitops_watson_nlp
page3-elem3,version: v0.0.80
page3-elem3,Bill of materials can be and should be shared for different scenarios.
page3-elem3,To customize them two files are used.
page3-elem3,1.
page3-elem3,output/credentials.properties [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/roks-new-nlp/output/credentials-template.properties]: Contains credentials
page3-elem3,2.
page3-elem3,output/bom-name/variables.yaml [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/roks-new-nlp/output/cluster-with-watson-nlp/variables-template.yaml]: Contains all other variables
page3-elem3,The following sample shows how to define variables like regions resource group names size of the cluster etc.
page3-elem3,variables:
page3-elem3,# overall
page3-elem3,- name: region
page3-elem3,description: The IBM Cloud region where the instance should be provisioned
page3-elem3,value: xxx
page3-elem3,- name: resource_group_name
page3-elem3,description: The name of the IBM Cloud resource group where the resources should be provisioned
page3-elem3,value: xxx
page3-elem3,# ocp
page3-elem3,- name: worker_count
page3-elem3,description: The number of workers that should be provisioned per subnet
page3-elem3,value: 2
page3-elem3,- name: cluster_flavor
page3-elem3,description: The flavor of the worker nodes that will be provisioned
page3-elem3,value: bx2.4x16
page3-elem3,When ArgoCD is used a Git token is needed to access the GitOps repo which is stored in credentials.properties.
page3-elem3,The same mechanism applies for tokens to pull container images from protected registries as well as other credentials.
page3-elem3,export TF_VAR_gitops_repo_token=xxx
page3-elem3,export TF_VAR_ibmcloud_api_key=xxx
page3-elem3,export TF_VAR_terraform_gitops_watson_nlp_registry_credentials=xxxxxxxxx
page3-elem3,With the toolkit’s CLI the bill of material the variables and the credentials are converted to Terraform assets in the ‘output/bom-name/terraform’ folder.
page3-elem3,Be careful when managing this folder with Git to prevent your credentials to be exposed.
page3-elem3,Note that the generated files are usually not touched.
page3-elem3,When the variables and credentials are not predefined CLI users will get prompted to define them when invoking ‘iascable build’.
page3-elem3,To find out more about the toolkit check out the documentation [https://operate.cloudnativetoolkit.dev/] and the sample [https://github.com/IBM/watson-automation] which deploys OpenShift and Watson NLP.
page3-elem3,The post Configuring the TechZone Toolkit [http://heidloff.net/article/configuring-the-techzone-accelerator-toolkit/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem4,IBM announces Embeddable AI.
page3-elem4,Over the last months I have worked on a new initiate from IBM called Embeddable AI.
page3-elem4,You can now run some of the Watson services via containers everywhere.
page3-elem4,Watch the short video [https://www.ibm.com/partnerworld/program/embeddableai] on the Embeddable AI home page for an introduction or this interview with Rob Thomas.
page3-elem4,> Why embeddable AI?
page3-elem4,> For years IBM Research has invested in developing AI capabilities which are embedded in IBM software offerings.
page3-elem4,We are now making the same capabilities available to our partners providing them a simpler path to create AI-powered solutions.
page3-elem4,While it is easy to consume software as a service certain workloads need to be run on premises.
page3-elem4,A good example are AI applications where data must not leave certain countries.
page3-elem4,Containers are a great technology to help running AI services everywhere.
page3-elem4,There are many resources that explain the Embeddable AI [https://github.com/IBM/watson-automation/tree/8973caa7f1a4eac6831ffa087c8a5ad1a9195728#resources] offering.
page3-elem4,Here are some of the resources that help you getting started.
page3-elem4,* Watson NLP [https://www.ibm.com/docs/en/watson-libraries?topic=watson-natural-language-processing-library-embed-home]
page3-elem4,* Watson NLP Helm Chart [https://github.com/IBM/watson-automation/blob/8973caa7f1a4eac6831ffa087c8a5ad1a9195728/documentation/NLPHelmChart.md]
page3-elem4,* Watson Speech [https://www.ibm.com/products/watson-speech-embed-libraries]
page3-elem4,* Automation for Watson NLP deployments [https://github.com/IBM/watson-automation/]
page3-elem4,* IBM entitlement API key [https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=information-obtaining-your-entitlement-api-key]
page3-elem4,My team has implemented a Helm chart [https://github.com/IBM/watson-automation/blob/8973caa7f1a4eac6831ffa087c8a5ad1a9195728/documentation/NLPHelmChart.md] which makes it easy to deploy Watson NLP in Kubernetes environments like OpenShift.
page3-elem4,$ oc login --token=sha256~xxx --server=https://xxx
page3-elem4,$ oc new-project watson-demo
page3-elem4,$ oc create secret docker-registry \
page3-elem4,--docker-server=cp.icr.io \
page3-elem4,--docker-username=cp \
page3-elem4,--docker-password=<your IBM Entitlement Key> \
page3-elem4,ibm-entitlement-key
page3-elem4,$ git clone https://github.com/cloud-native-toolkit/terraform-gitops-watson-nlp
page3-elem4,$ git clone https://github.ibm.com/isv-assets/watson-automation
page3-elem4,$ acceptLicense: true # in values.yaml
page3-elem4,$ cp watson-automation/helm-nlp/values.yaml terraform-gitops-watson-nlp/chart/watson-nlp/values.yaml
page3-elem4,$ cd terraform-gitops-watson-nlp/chart/watson-nlp
page3-elem4,$ helm install -f values.yaml watson-embedded .
page3-elem4,As a result you’ll see the deployed Watson NLP pod in OpenShift.
page3-elem4,To find out more about the toolkit check out the documentation [https://operate.cloudnativetoolkit.dev/] and the sample [https://github.com/IBM/watson-automation] above.
page3-elem4,The post IBM announces Embeddable AI [http://heidloff.net/article/ibm-announces-embeddable-ai/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem5,Setting up OpenShift and Applications in one Hour.
page3-elem5,With the TechZone Accelerator Toolkit OpenShift can be set up in the cloud including custom applications by using automation via Terraform and Argo CD.
page3-elem5,In my previous blog I explained the toolkit: Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/].
page3-elem5,The toolkit leverages Terrafrom and GitOps and is based on best practices based on IBM’s deployment experiences from partner and client projects.
page3-elem5,Let’s take a look how the toolkit can be used.
page3-elem5,The module catalog [https://modules.cloudnativetoolkit.dev/] provides 200+ modules to install IBM Software and open source components which can be deployed on clouds like AWS Azure and IBM Cloud.
page3-elem5,Complete stack solutions are defined in yaml files.
page3-elem5,These bill of materials contain list of modules in this example [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/roks-new-nlp/bom.yaml#L27] OpenShift in the IBM Cloud Argo CD including a GitOps repo the Watson NLP (natural language processing) container and a custom application.
page3-elem5,apiVersion: cloudnativetoolkit.dev/v1alpha1
page3-elem5,kind: BillOfMaterial
page3-elem5,metadata:
page3-elem5,name: cluster-with-watson-nlp
page3-elem5,spec:
page3-elem5,modules:
page3-elem5,- name: ibm-ocp-vpc
page3-elem5,version: v1.16.0
page3-elem5,- name: argocd-bootstrap
page3-elem5,version: v1.12.0
page3-elem5,- name: gitops-repo
page3-elem5,alias: gitops_repo
page3-elem5,version: v1.22.2
page3-elem5,- name: terraform-gitops-ubi
page3-elem5,alias: terraform_gitops_ubi
page3-elem5,version: v0.0.8
page3-elem5,- name: terraform-gitops-watson-nlp
page3-elem5,alias: terraform_gitops_watson_nlp
page3-elem5,version: v0.0.80
page3-elem5,The modules can be configured via variables [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/roks-new-nlp/output/cluster-with-watson-nlp/variables-template.yaml] for example regions resource group names sizes of clusters etc.
page3-elem5,variables:
page3-elem5,# overall
page3-elem5,- name: region
page3-elem5,description: The IBM Cloud region where the instance should be provisioned
page3-elem5,value: xxx
page3-elem5,- name: resource_group_name
page3-elem5,description: The name of the IBM Cloud resource group where the resources should be provisioned
page3-elem5,value: xxx
page3-elem5,# ocp
page3-elem5,- name: worker_count
page3-elem5,description: The number of workers that should be provisioned per subnet
page3-elem5,value: 2
page3-elem5,- name: cluster_flavor
page3-elem5,description: The flavor of the worker nodes that will be provisioned
page3-elem5,value: bx2.4x16
page3-elem5,To run the sample you need to clone the repo and install the toolkit’s CLI [https://github.com/cloud-native-toolkit/iascable] called iascable.
page3-elem5,$ curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh
page3-elem5,$ git clone https://github.com/IBM/watson-automation
page3-elem5,With the CLI the bill of material is converted into Terraform assets.
page3-elem5,$ cd watson-deployments/roks-new-nlp
page3-elem5,$ iascable build -i bom.yaml
page3-elem5,$ cd output
page3-elem5,An image is provided which comes with all tools necessary to run Terraform.
page3-elem5,To start the container invoke this command in a terminal:
page3-elem5,$ ./launch.sh
page3-elem5,In the running container invoke these commands:
page3-elem5,$ cd cluster-with-watson-nlp
page3-elem5,$ ./apply.sh
page3-elem5,The ‘apply’ command takes roughly 45 minutes to set up the managed OpenShift cluster in the IBM Cloud and the additional modules.
page3-elem5,After this you can find the resources in the IBM Cloud console [https://github.com/IBM/watson-automation/blob/e92c9cef8acb1bd5c57177dad3d91c42ff9c8aee/documentation/screenshots/openshift-01.png].
page3-elem5,Argo CD has been set up to deploy the Watson NLP container and the custom application.
page3-elem5,To find out more about the toolkit check out the documentation [https://operate.cloudnativetoolkit.dev/] and the sample [https://github.com/IBM/watson-automation] above.
page3-elem5,The post Setting up OpenShift and Applications in one Hour [http://heidloff.net/article/setting-up-openshift-and-applications-in-one-hour/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem6,Introducing IBM’s Toolkit to handle Everything as Code.
page3-elem6,Terraform is a popular tool to set up infrastructure declaratively.
page3-elem6,Argo CD is a popular tool to manage Kubernetes resources via GitOps.
page3-elem6,To set up and manage complete solutions these tools can be used together.
page3-elem6,To run applications you need more than just your application code:
page3-elem6,* Bare metal hardware or VMs in the cloud or on-premises
page3-elem6,* Operating systems
page3-elem6,* SaaS or on-premises services like databases
page3-elem6,* Container orchestration platforms
page3-elem6,* Compiled and optimized applications
page3-elem6,Terraform meets Argo CD
page3-elem6,To be able to deploy and operate applications efficiently you need automation for the complete stack.
page3-elem6,1.
page3-elem6,Terraform for infrastructure like Kubernetes [http://heidloff.net/article/setting-up-ibm-software-with-terraform/]
page3-elem6,2.
page3-elem6,Argo CD for everything within Kubernetes clusters [http://heidloff.net/article/deploying-kubernetes-resources-via-gitops/]
page3-elem6,To automate deployments declarative approaches are often used known as ‘Infrastructure as Code [https://en.wikipedia.org/wiki/Infrastructure_as_code]‘.
page3-elem6,> Infrastructure as code is the process of managing and provisioning computer data centers through machine-readable definition files rather than physical hardware configuration or interactive configuration tools.
page3-elem6,However Infrastructure as Code does not handle the deployments of services and applications which is required for full solutions.
page3-elem6,What you really need is Everything as Code.
page3-elem6,This is why IBM has built a toolkit that combines Terraform and Argo CD.
page3-elem6,IBM has created a Terraform module which sets up Argo CD in a Kubernetes or OpenShift cluster.
page3-elem6,The Argo CD bootstrap module uses another module which creates a Git repo which is linked to ArgoCD.
page3-elem6,* terraform-tools-argocd-bootstrap [https://github.com/cloud-native-toolkit/terraform-tools-argocd-bootstrap]
page3-elem6,* terraform-tools-gitops [https://github.com/cloud-native-toolkit/terraform-tools-gitops]
page3-elem6,In GitOps repos all resources can be defined which Argo CD deploys.
page3-elem6,The following screenshot shows an Argo CD application which deploys IBM Watson NLP (natural language processing) via Helm.
page3-elem6,Introducing IBM’s Toolkit
page3-elem6,The current name of the toolkit is ‘IBM Technology Zone Accelerator Toolkit’ also known as ‘TechZone Accelerator Toolkit’ ‘Software Everywhere’ ‘TechZone Automation’ and ‘Cloud Native Toolkit’.
page3-elem6,With the toolkit IBM follows Red Hat’s successful model to open source projects (‘upstream projects [https://www.redhat.com/en/blog/what-open-source-upstream]‘) and to have supported versions of these open source projects.
page3-elem6,The open source version of the toolkit is available under operate.cloudnativetoolkit.dev [https://operate.cloudnativetoolkit.dev/].
page3-elem6,The toolkit is an opinionated framework which brings together the strengths of Terraform and Argo CD.
page3-elem6,I’ll blog more about it but to get started check out the labs [https://operate.cloudnativetoolkit.dev/learn/iascable/lab1/] in the toolkit documentation.
page3-elem6,The post Introducing IBM’s Toolkit to handle Everything as Code [http://heidloff.net/article/introducing-ibms-toolkit-to-handle-everything-as-code/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem7,Deploying Kubernetes Resources via GitOps.
page3-elem7,Infrastructure as Code is an important concept of DevOps.
page3-elem7,With GitOps tools like Argo CD [https://argo-cd.readthedocs.io/en/stable/] not only infrastructure can be handled as code but also custom applications and other Kubernetes resources.
page3-elem7,Here is the definition of GitOps [https://www.redhat.com/en/topics/devops/what-is-gitops] from Red Hat.
page3-elem7,> GitOps uses Git repositories as a single source of truth to deliver infrastructure as code.
page3-elem7,Submitted code checks the CI process while the CD process checks and applies requirements for things like security infrastructure as code or any other boundaries set for the application framework.
page3-elem7,All changes to code are tracked making updates easy while also providing version control should a rollback be needed.
page3-elem7,Software development is much more than writing source code.
page3-elem7,Modern software development also requires automation and CI/CD.
page3-elem7,One way to implement CI/CD is to use pipelines that build images and other resources and deploy them via scripts whenever the pipelines run.
page3-elem7,While pipelines are still required to build the artefacts the actual deployment of resources is done more and more via declarative and event based GitOps mechanisms.
page3-elem7,GitOps uses Git repos to define the desired state of a system.
page3-elem7,GitOps tools like Argo CD compare the current state with the desired state.
page3-elem7,If these states don’t match synchronisation is triggered automatically to apply update or delete resources.
page3-elem7,Here are some videos I’ve used to learn GitOps:
page3-elem7,* What is GitOps How GitOps works and Why it’s so useful [https://www.youtube.com/watch?v=f5EpcWp0THw]
page3-elem7,* ArgoCD Tutorial for Beginners | GitOps CD for Kubernetes [https://youtu.be/MeU5_k9ssrs]
page3-elem7,* What Is GitOps And Why Do We Want It?
page3-elem7,* GitOps Without Pipelines With ArgoCD Image Updater [https://youtu.be/avPUQin9kzU]
page3-elem7,* Argo CD – Applying GitOps Principles To Manage A Production Environment In Kubernetes [https://youtu.be/vpWQeoaiRM4]
page3-elem7,Over the next days I’d like to blog more about GitOps and Terraform and how my team has used these tools to set up Red Hat OpenShift in the IBM Cloud [https://github.com/IBM/watson-automation] including Watson NLP and a custom application.
page3-elem7,The post Deploying Kubernetes Resources via GitOps [http://heidloff.net/article/deploying-kubernetes-resources-via-gitops/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem8,Setting up IBM Software with Terraform.
page3-elem8,Automation is key when developing and deploying software.
page3-elem8,Community is key to get support to find samples and more.
page3-elem8,This is why I like Terraform which is a great tool to automate infrastructure on any cloud and there is a huge community.
page3-elem8,Infrastructure as Code
page3-elem8,While developers have used source control systems for a long time more and more DevOps engineers use the same mechanism to deploy and manage infrastructures.
page3-elem8,The main advantage of handling infrastructure as code is that with a declarative approach changes can be easier tracked and human errors can be reduced.
page3-elem8,Over the last years Terraform [https://www.terraform.io/] has gained popularity and became an established tool to set up infrastructure.
page3-elem8,Getting started with Terraform
page3-elem8,I’d like to blog soon more about Terraform.
page3-elem8,Here are some resources that I have used to learn it.
page3-elem8,* Terraform [https://www.terraform.io/]
page3-elem8,* Terraform explained in 15 mins [https://www.youtube.com/watch?v=l5k1ai_GBDE]
page3-elem8,* 8 Terraform Best Practices that will improve your TF workflow immediately [https://www.youtube.com/watch?v=gxPykhPxRW0]
page3-elem8,* Terraform vs. Pulumi vs. Crossplane – Infrastructure as Code (IaC) Tools Comparison [https://youtu.be/RaoKcJGchKM]
page3-elem8,* Complete Terraform Course [https://youtu.be/7xngnjfIlK4]
page3-elem8,Terraform for IBM Software
page3-elem8,IBM provides a lot of modules to deploy IBM Software.
page3-elem8,* IBM Cloud Provider [https://registry.terraform.io/providers/IBM-Cloud/ibm/latest/docs]
page3-elem8,* IBM Cloud modules [https://github.com/terraform-ibm-modules/documentation]
page3-elem8,* IBM Cloud Native Toolkit modules [https://github.com/orgs/cloud-native-toolkit/repositories]
page3-elem8,* Getting started with Terraform on IBM Cloud [https://cloud.ibm.com/docs/ibm-cloud-provider-for-terraform?topic=ibm-cloud-provider-for-terraform-getting-started]
page3-elem8,Here is a little sample to deploy a resource group in the IBM Cloud and a namespace in a Kubernetes cluster.
page3-elem8,Check out my blog for more information about Terraform GitOps and IBM Software over the next days.
page3-elem8,The post Setting up IBM Software with Terraform [http://heidloff.net/article/setting-up-ibm-software-with-terraform/] appeared first on Niklas Heidloff [http://heidloff.net].
page3-elem9,New Era of Software Development: Operations Automation.
page3-elem9,Check out this 40 mins video [https://youtu.be/D6njEyXPieg] to learn about the power of Kubernetes Operators to automate day 2 operation tasks.
page3-elem9,20 Years ago
page3-elem9,When I started my career 20+ years ago usually developers only wrote code some even without writing tests because that was supposed to be done by separate teams.
page3-elem9,10 Years ago
page3-elem9,10 years ago developers and technical leads realized that this was not sufficient.
page3-elem9,The concept of DevOps was born and over the last years most companies adopted it.
page3-elem9,Developers work now together with Ops engineers and are together responsible for the whole software development lifecycle including operations.
page3-elem9,For example DevOps teams write automation for deployments of software in different environments via CI/CD.
page3-elem9,Today
page3-elem9,Now a new era of software development has started.
page3-elem9,Rather than ‘only’ automating the deployments of software operation tasks are automated.
page3-elem9,This is usually referred to as day 2 tasks while day 1 tasks are focussed on deployments.
page3-elem9,Operating software in custom clusters no matter what clouds on premises or on edge devices can become quite expensive since issues are often hard to discover debug and analyse especially if people need to do this who are not SRE (service reliability engineers) for specific software.
page3-elem9,Kubernetes Operators
page3-elem9,The solution?
page3-elem9,Kubernetes Operators.
page3-elem9,An incredibly powerful technology to automate operations.
page3-elem9,The main concept behind it is GitOps.
page3-elem9,Custom logic synchronizes the ‘to be state’ with the ‘as is state’.
page3-elem9,This concept is used intensively within Kubernetes and can be used for custom applications as well.
page3-elem9,I like the Software as a Service approach a lot since it is the easiest way to consume services.
page3-elem9,However in some cases this doesn’t work for example since certain data needs to reside at certain locations or services need to be customized.
page3-elem9,I think using the Kubernetes Operators technology is the next best way to consume software.
page3-elem9,Plus Kubernetes Operators allow companies to reliably host software which manages itself.
page3-elem9,Operators Reference Architecture
page3-elem9,My team has created a reference architecture how to build Kubernetes Operators which goes above and beyond what the Operator SDK and Kubebuilder tutorials provide.
page3-elem9,Check it out!
page3-elem9,* Operators Sample Architecture – Source Code [https://github.com/ibm/operator-sample-go]
page3-elem9,* Operators Sample Architecture – Documentation [https://ibm.github.io/operator-sample-go-documentation/]
page3-elem9,We’ve created a 40 minutes video [https://youtu.be/D6njEyXPieg] that describes the project on a high level.
page3-elem9,If you are an IBM partner who is interested in building operators reach out to us!
page3-elem9,We can help.
page3-elem9,Authors:
page3-elem9,* Adam de Leeuw [https://www.linkedin.com/in/deleeuwa/]
page3-elem9,* Thomas Südbröcker [https://twitter.com/tsuedbroecker]
page3-elem9,* Alain Airom [https://twitter.com/AAairom]
page3-elem9,* Diwakar Tiwari [https://twitter.com/diwakarptiwari]
page3-elem9,* Vishal Ramani [https://www.linkedin.com/in/vishalramani/]
page3-elem9,* Niklas Heidloff [https://twitter.com/nheidloff]
page3-elem9,The post New Era of Software Development: Operations Automation [http://heidloff.net/article/new-era-software-development-automation-operations/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem0,IBM helps Partners to scale their Businesses.
page4-elem0,IBM Build Labs helps IBM partners to scale their businesses and to reach new markets.
page4-elem0,I am very proud that our team has produced two reference architectures to accomplish this:
page4-elem0,* Kubernetes Operators Reference Architecture [https://github.com/ibm/operator-sample-go]
page4-elem0,* Software as a Service (SaaS) Reference Architecture [https://github.com/IBM/multi-tenancy]
page4-elem0,This short video gives you more context.
page4-elem0,If you are interested in working with us reach us to me on Twitter or via the GitHub repos.
page4-elem0,The post IBM helps Partners to scale their Businesses [http://heidloff.net/article/ibm-helps-partners-to-scale-their-businesses/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem1,Home Improvements.
page4-elem1,Over the last two Corona years I have spent some time with home improvements on weekends.
page4-elem1,A lot of fun!
page4-elem1,Unfortunately I didn’t document all steps but the pictures below give an idea of the involved work.
page4-elem1,The post Home Improvements [http://heidloff.net/article/home-improvements/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem2,40 Blogs in 40 Days.
page4-elem2,Many IBM partners want to understand how to scale their offerings to reach new markets.
page4-elem2,Technically this often translates to two technologies: Kubernetes Operators and SaaS (Software as a Service).
page4-elem2,As I’m working with IBM partners my team has created several assets for our co-creation engagements.
page4-elem2,Below is the list of blogs about these topics.
page4-elem2,Over the last 40 (business) days I’ve written 40 blogs.
page4-elem2,As always thanks a lot for your feedback!
page4-elem2,Operators – Overview and Scenarios
page4-elem2,* Why you should build Kubernetes Operators [http://heidloff.net/article/why-you-should-build-kubernetes-operators/]
page4-elem2,* Day 2 Scenario: Automatically Archiving Data [http://heidloff.net/article/automatically-archiving-data-kubernetes-operators/]
page4-elem2,* Day 2 Scenario: Automatically Scaling of Applications [http://heidloff.net/article/scaling-applications-automatically-operators/]
page4-elem2,* The Kubernetes Operator Metamodel [http://heidloff.net/article/the-kubernetes-operator-metamodel/]
page4-elem2,Go based Operators
page4-elem2,* Creating and updating Resources [http://heidloff.net/article/updating-resources-kubernetes-operators/]
page4-elem2,* Deleting Resources [http://heidloff.net/article/deleting-resources-kubernetes-operators/]
page4-elem2,* Storing State of Resources with Conditions [http://heidloff.net/article/storing-state-status-kubernetes-resources-conditions-operators-go/]
page4-elem2,* Finding out the Kubernetes Versions and Capabilities [http://heidloff.net/article/finding-kubernetes-version-capabilities-operators/]
page4-elem2,* Configuring Webhooks [http://heidloff.net/article/configuring-webhooks-kubernetes-operators/]
page4-elem2,* Initialization and Validation Webhooks [http://heidloff.net/article/developing-initialization-validation-webhooks-kubernetes-operators/]
page4-elem2,* Converting Custom Resource Versions [http://heidloff.net/article/converting-custom-resource-versions-kubernetes-operators/]
page4-elem2,* Defining Dependencies [http://heidloff.net/article/defining-dependencies-kubernetes-operators/]
page4-elem2,* Exporting Metrics from Kubernetes Apps for Prometheus [http://heidloff.net/article/exporting-metrics-kubernetes-applications-prometheus/]
page4-elem2,* Accessing Kubernetes from Go Applications [http://heidloff.net/article/accessing-kubernetes-from-go-applications/]
page4-elem2,* How to build your own Database on Kubernetes [http://heidloff.net/article/how-to-build-your-own-database-on-kubernetes/]
page4-elem2,* Building Databases on Kubernetes with Quarkus [http://heidloff.net/quarkus/building-databases-kubernetes-quarkus/]
page4-elem2,* Manually deploying Operators to Kubernetes [http://heidloff.net/article/manually-deploying-operators-to-kubernetes/]
page4-elem2,* Deploying Operators with the Operator Lifecycle Manager [http://heidloff.net/article/deploying-operators-operator-lifecycle-manager-olm/]
page4-elem2,* Importing Go Modules in Operators [http://heidloff.net/article/importing-go-modules-kubernetes-operators/]
page4-elem2,* Accessing third Party Custom Resources in Go Operators [http://heidloff.net/article/accessing-third-party-custom-resources-go-operators/]
page4-elem2,* Using object-oriented Concepts in Golang based Operators [http://heidloff.net/article/object-oriented-concepts-golang/]
page4-elem2,Quarkus based Operators
page4-elem2,* Developing Quarkus and Web Applications locally [http://heidloff.net/article/developing-quarkus-and-web-application-locally/]
page4-elem2,* Developing and Debugging Kubernetes Operators in Java [http://heidloff.net/article/developing-debugging-kubernetes-operators-java/]
page4-elem2,* Accessing Kubernetes Resources from Java Operators [http://heidloff.net/article/accessing-kubernetes-resources-from-java-operators]
page4-elem2,* Leveraging third party Operators in Kubernetes Operators [http://heidloff.net/article/leveraging-third-party-operators-in-kubernetes-operators/]
page4-elem2,* Creating Database Schemas in Kubernetes Operators [http://heidloff.net/article/creating-database-schemas-kubernetes-operators/]
page4-elem2,* Resources to build Kubernetes Operators [http://heidloff.net/articles/resources-to-build-kubernetes-operators/]
page4-elem2,SaaS Reference Architecture and DevSecOps
page4-elem2,* New Open-Source Multi-Cloud Asset to build SaaS [http://heidloff.net/article/open-source-multi-cloud-assets-saas]
page4-elem2,* How to build SaaS for multiple Tenants [http://heidloff.net/article/how-to-build-saas-for-multiple-tenants/]
page4-elem2,* Accessing Postgres from Quarkus Containers via TLS [http://heidloff.net/article/accessing-postgres-from-quarkus-containers-via-tls/]
page4-elem2,* Typical Authentication Flow for Kubernetes Applications [http://heidloff.net/article/typical-authentication-flow-kubernetes-applications/]
page4-elem2,* Containerizing Web Applications [http://heidloff.net/article/containerizing-web-applications/]
page4-elem2,* Containerizing Quarkus Applications [http://heidloff.net/article/containerizing-quarkus-applications/]
page4-elem2,* DevSecOps for SaaS Reference Architecture on OpenShift [http://heidloff.net/article/devsecops-saas-reference-architecture-openshift/]
page4-elem2,* Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/]
page4-elem2,* Developing Serverless Toolchains [http://heidloff.net/article/developing-serverless-toolchains/]
page4-elem2,* Deploying Serverless SaaS with Serverless Toolchains [http://heidloff.net/article/deploying-serverless-saas-with-serverless-toolchains/]
page4-elem2,* Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]
page4-elem2,* Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/]
page4-elem2,* Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/]
page4-elem2,The post 40 Blogs in 40 Days [http://heidloff.net/article/40-blogs-in-40-days/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem3,Scaling Applications automatically with Operators.
page4-elem3,The real power or Kubernetes operators are not day 1 tasks like the initial deployments but the automation of day 2 operations.
page4-elem3,This article describes a sample operator that scales up an application automatically based on the number of API requests.
page4-elem3,The complete source code from this article is available in the ibm/operator-sample-go repo [https://github.com/IBM/operator-sample-go].
page4-elem3,The repo includes operator samples that demonstrate patterns and best practises.
page4-elem3,It also includes another day 2 sample scenario: Automatically Archiving Data with Kubernetes Operators [http://heidloff.net/article/automatically-archiving-data-kubernetes-operators/].
page4-elem3,The sample contains the following components:
page4-elem3,* Prometheus: Stores metrics from various sources and provides query capabilities
page4-elem3,* Sample microservice: Provides a /hello endpoint which exposes a counter to Prometheus
page4-elem3,* Application operator (core): Deploys the microservice
page4-elem3,* Application operator’s CronJob: Separate container which scales up the number of pod instances based on the amount of /hello invocations
page4-elem3,To set up and configure Prometheus check our my previous article Exporting Metrics from Kubernetes Apps for Prometheus [http://heidloff.net/article/exporting-metrics-kubernetes-applications-prometheus/].
page4-elem3,Below I focus on the implementation of the auto-scaler.
page4-elem3,The microservice has been implemented with Quarkus.
page4-elem3,It uses Eclipse MicroProfile to track the number of invocations (see code [https://github.com/IBM/operator-sample-go/blob/f130dc768df6d9178f6395690f508f0840e0b5ef/simple-microservice/src/main/java/net/heidloff/GreetingResource.java]).
page4-elem3,import org.eclipse.microprofile.config.inject.ConfigProperty;
page4-elem3,import org.eclipse.microprofile.metrics.annotation.Counted;
page4-elem3,@Path("/hello")
page4-elem3,public class GreetingResource {
page4-elem3,@ConfigProperty(name = "greeting.message")
page4-elem3,String message;
page4-elem3,@GET
page4-elem3,@Produces(MediaType.TEXT_PLAIN)
page4-elem3,@Counted(name = "countHelloEndpointInvoked" description = "How often /hello has been invoked")
page4-elem3,public String hello() {
page4-elem3,return String.format("Hello %s" message);
page4-elem3,To allow Prometheus to scrape these metrics a ServiceMonitor [https://github.com/IBM/operator-sample-go/blob/f130dc768df6d9178f6395690f508f0840e0b5ef/simple-microservice/kubernetes/service-monitor.yaml] is used.
page4-elem3,apiVersion: monitoring.coreos.com/v1
page4-elem3,kind: ServiceMonitor
page4-elem3,metadata:
page4-elem3,labels:
page4-elem3,app: myapplication
page4-elem3,name: myapplication-metrics-monitor
page4-elem3,namespace: application-beta
page4-elem3,spec:
page4-elem3,endpoints:
page4-elem3,- path: /q/metrics
page4-elem3,selector:
page4-elem3,matchLabels:
page4-elem3,app: myapplication
page4-elem3,With the Prometheus user interface queries to this data can be done.
page4-elem3,To develop the auto-scaler a separate image/container is used.
page4-elem3,This container is an extension to the application controller.
page4-elem3,The application controller sets up a CronJob for the auto-scaler container so that it is run on a scheduled basis.
page4-elem3,The CronJob [https://github.com/IBM/operator-sample-go/blob/f130dc768df6d9178f6395690f508f0840e0b5ef/operator-application-scaler/kubernetes/cronjob.yaml] that is created by the controller looks like this.
page4-elem3,Note that the application name and namespace are passed in as parameter.
page4-elem3,apiVersion: batch/v1
page4-elem3,kind: CronJob
page4-elem3,metadata:
page4-elem3,name: application-scaler
page4-elem3,namespace: operator-application-system
page4-elem3,spec:
page4-elem3,schedule: "0 * * * *"
page4-elem3,jobTemplate:
page4-elem3,spec:
page4-elem3,template:
page4-elem3,spec:
page4-elem3,containers:
page4-elem3,- name: application-scale
page4-elem3,image: docker.io/nheidloff/operator-application-scaler:v1.0.2
page4-elem3,imagePullPolicy: IfNotPresent
page4-elem3,env:
page4-elem3,- name: APPLICATION_RESOURCE_NAME
page4-elem3,value: "application"
page4-elem3,- name: APPLICATION_RESOURCE_NAMESPACE
page4-elem3,value: "application-beta"
page4-elem3,restartPolicy: OnFailure
page4-elem3,The implementation [https://github.com/IBM/operator-sample-go/blob/f130dc768df6d9178f6395690f508f0840e0b5ef/operator-application-scaler/scaler/scaler.go] of the actual auto-scaler is trivial.
page4-elem3,I’ve used the Prometheus Go client library [https://github.com/prometheus/client_golang/].
page4-elem3,Note that this library is still considered experimental.
page4-elem3,Alternatively you can use the Prometheus HTTP API [https://prometheus.io/docs/prometheus/latest/querying/api/].
page4-elem3,prometheusAddress := "http://prometheus-operated.monitoring:9090"
page4-elem3,queryAmountHelloEndpointInvocations := "application_net_heidloff_GreetingResource_countHelloEndpointInvoked_total"
page4-elem3,client err := api.NewClient(api.Config{
page4-elem3,Address: prometheusAddress
page4-elem3,})
page4-elem3,if err != nil {
page4-elem3,os.Exit(1)
page4-elem3,v1api := v1.NewAPI(client)
page4-elem3,ctx cancel := context.WithTimeout(context.Background() 10*time.Second)
page4-elem3,defer cancel()
page4-elem3,result warnings err := v1api.Query(ctx queryAmountHelloEndpointInvocations time.Now())
page4-elem3,if err != nil {
page4-elem3,os.Exit(1)
page4-elem3,resultVector conversionSuccessful := (result).
page4-elem3,(model.Vector)
page4-elem3,if conversionSuccessful == true {
page4-elem3,if resultVector.Len() > 0 {
page4-elem3,firstElement := resultVector[0]
page4-elem3,if firstElement.Value > 5 {
page4-elem3,// Note: '5' is only used for demo purposes
page4-elem3,scaleUp()
page4-elem3,To learn more about operator patterns and best practices check out the repo operator-sample-go [https://github.com/IBM/operator-sample-go].
page4-elem3,The instructions how to run the auto-scaler demo are in the documentation [https://github.com/IBM/operator-sample-go/blob/f130dc768df6d9178f6395690f508f0840e0b5ef/operator-application-scaler/README.md].
page4-elem3,The post Scaling Applications automatically with Operators [http://heidloff.net/article/scaling-applications-automatically-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem4,Exporting Metrics from Kubernetes Apps for Prometheus.
page4-elem4,Operators automate day 2 operations for Kubernetes based software.
page4-elem4,Operators need to know the state of their operands.
page4-elem4,One way to find out the state is to check metrics information stored in Prometheus.
page4-elem4,This article describes how to export metrics from applications running on Kubernetes to make them accessible by Prometheus.
page4-elem4,The complete source code from this article is available in the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go] repo.
page4-elem4,The repo includes operator samples that demonstrate patterns and best practises.
page4-elem4,Let’s look how Prometheus can be deployed on Kubernetes and how Go and Java based applications can export metrics so that Prometheus is able to read and store it.
page4-elem4,1.
page4-elem4,Setup of Prometheus
page4-elem4,An easy way to install Prometheus is to utilize the Prometheus operator [https://operatorhub.io/operator/prometheus].
page4-elem4,Before it can be installed the Operator Lifecycle Manager (OLM) needs to be deployed.
page4-elem4,When you develop operators with the Operator SDK [https://sdk.operatorframework.io/] it is possible to deploy OLM with just one command:
page4-elem4,$ operator-sdk olm install
page4-elem4,or
page4-elem4,$ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/install.sh | bash -s v0.20.0
page4-elem4,Next the Prometheus operator can be installed.
page4-elem4,$ kubectl create -f https://operatorhub.io/install/prometheus.yaml
page4-elem4,2.
page4-elem4,Configuration of Prometheus
page4-elem4,To set up the actual Prometheus instance on Kubernetes RBAC access rights need to be defined.
page4-elem4,Read the documentation [https://book.kubebuilder.io/reference/metrics.html] for details.
page4-elem4,In summary the following four files handle the minimal setup.
page4-elem4,* service-account.yaml [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/prometheus/prometheus/service-account.yaml]
page4-elem4,* cluster-role.yaml [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/prometheus/prometheus/cluster-role.yaml]
page4-elem4,* cluster-role-binding.yaml [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/prometheus/prometheus/cluster-role-binding.yaml]
page4-elem4,* prometheus.yaml [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/prometheus/prometheus/prometheus.yaml]
page4-elem4,3.
page4-elem4,Linkage between Prometheus and custom Applications
page4-elem4,Applications can export data in the format Prometheus expects.
page4-elem4,The applications don’t push this data but they provide endpoints that Prometheus pulls on a scheduled basis.
page4-elem4,To tell Prometheus these endpoints the custom resource ‘ServiceMonitor’ is used.
page4-elem4,Here is a simple sample [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/simple-microservice/kubernetes/service-monitor.yaml].
page4-elem4,The trick is the correct usage of labels and selectors.
page4-elem4,The selector in the service monitor finds the appropriate service.
page4-elem4,The selector in the service links to pods.
page4-elem4,apiVersion: monitoring.coreos.com/v1
page4-elem4,kind: ServiceMonitor
page4-elem4,metadata:
page4-elem4,labels:
page4-elem4,app: myapplication
page4-elem4,name: myapplication-metrics-monitor
page4-elem4,namespace: application-beta
page4-elem4,spec:
page4-elem4,endpoints:
page4-elem4,- path: /q/metrics
page4-elem4,selector:
page4-elem4,matchLabels:
page4-elem4,app: myapplication
page4-elem4,When building operators with the Operator SDK the SDK creates this service monitor [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/operator-application/config/prometheus/monitor.yaml] automatically.
page4-elem4,All you need to do is to uncomment one line [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/operator-application/config/default/kustomization.yaml#L24-L25].
page4-elem4,4.
page4-elem4,Writing Metrics
page4-elem4,There are several libraries and frameworks for different languages available.
page4-elem4,Here is a sample [https://github.com/IBM/operator-sample-go/blob/2a00d28cd40bf0c877589feb3fc636a7fa1e69f9/operator-application/controllers/application/controller.go#L23-L33] how to write metrics from a Golang application.
page4-elem4,import (
page4-elem4,"github.com/prometheus/client_golang/prometheus"
page4-elem4,var countReconcileLaunched = prometheus.NewCounter(
page4-elem4,prometheus.CounterOpts{
page4-elem4,Name: "reconcile_launched_total"
page4-elem4,Help: "reconcile_launched_total"
page4-elem4,}
page4-elem4,func (reconciler *ApplicationReconciler) Reconcile(ctx context.Context req ctrl.Request) (ctrl.Result error) {
page4-elem4,countReconcileLaunched.Inc()
page4-elem4,...
page4-elem4,To learn more about operator patterns and best practices check out the repo operator-sample-go [https://github.com/IBM/operator-sample-go].
page4-elem4,The repo shows how to export metrics from a Quarkus application and a Go based operator.
page4-elem4,The screenshot shows the two registered service monitors.
page4-elem4,Finally the data can be queried for example in the Prometheus user interface.
page4-elem4,The post Exporting Metrics from Kubernetes Apps for Prometheus [http://heidloff.net/article/exporting-metrics-kubernetes-applications-prometheus/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem5,Why you should build Kubernetes Operators.
page4-elem5,Operators are a powerful technology to automate operations of software in Kubernetes clusters.
page4-elem5,This article describes scenarios when operators should be used and which value they provide.
page4-elem5,There are multiple technologies to install and operate software in Kubernetes: kubectl oc Helm Kustomize CI/CD GitOps operators and more.
page4-elem5,As always the answer to the question when to use what is: It depends.
page4-elem5,Let me share my top three reasons why to use operators.
page4-elem5,Top three Reasons to use Operators:
page4-elem5,1.
page4-elem5,Automation of Day 2 Operations
page4-elem5,* Autopilot and Self-Healing
page4-elem5,* Predictive analytics
page4-elem5,2.
page4-elem5,Reusability of Software
page4-elem5,* Hubs and Marketplaces
page4-elem5,* Internal and external multi-tenancy Applications
page4-elem5,3.
page4-elem5,Leverage of the Kubernetes Community
page4-elem5,* Ecosystem Tools and Offerings
page4-elem5,* Industry Standard
page4-elem5,1.
page4-elem5,Automation of Day 2 Operations
page4-elem5,The main purpose of operators is to automate operations.
page4-elem5,While this sounds obvious often there is a perception that operators are only another way to deploy software.
page4-elem5,The key value of operators is to make operations of software as seamless as possible.
page4-elem5,I often think of operators as the next best way after Software as as Service (SaaS) to easily manage software running in our own clusters.
page4-elem5,Kubernetes operators are capable to automate the expensive and error likely human operations.
page4-elem5,Features like autopilot and self-healing are great scenarios.
page4-elem5,For example databases can be archived automatically software can automatically be scaled up and down and missing resources can be recreated when they have been deleted by mistake or in catastrophic events.
page4-elem5,It’s even possible to predict failures before they happen and to take appropriate actions.
page4-elem5,These automations work for all types of workloads (stateful and stateless) and all types of Kubernetes resources (compute storage network).
page4-elem5,All business specific automation functionality can be bundled in one operator component rather than using a diverse set of tools.
page4-elem5,2.
page4-elem5,Reusability of Software
page4-elem5,Another benefit of operators is the reusability of software.
page4-elem5,With operators software and libraries can be bundled so that it can be used in different contexts.
page4-elem5,This approach is different from CI/CD pipelines where software is deployed more frequently specifically for one application.
page4-elem5,Software providers can expose their operators in various catalogs hubs and marketplaces like OperatorHub.io Red Hat Marketplace the integrated OperatorHub in OpenShift etc. This allows them to reach new markets and to promote their software.
page4-elem5,While catalogs like OperatorHub.io are primarily used to increase awareness marketplaces like Red Hat Marketplace provide commercialization options.
page4-elem5,Additionally operators can be utilized to share software within companies and to share software with specific clients.
page4-elem5,In this scenario operators can be published in internal catalogs or they can be directly deployed to clusters.
page4-elem5,3.
page4-elem5,Leverage of the Kubernetes Community
page4-elem5,Kubernetes is the de-facto standard how to run software in the cloud.
page4-elem5,There is a huge ecosystem supporting Kubernetes providing samples tools and commercial offerings and more.
page4-elem5,Operators leverage this community since they are a natural and Kubernetes-native way to extend Kubernetes.
page4-elem5,Operators are not only used to manage third-party resources but they are also used internally.
page4-elem5,Because operators base on the Kubernetes model existing development tools CLIs monitoring tools etc. can be used.
page4-elem5,Kubernetes provides proven and established industry standards how to define deploy and operate software.
page4-elem5,For example for production workloads a declarative approach is employed (e.g. GitOps ready) custom resources can be defined and standard CLIs and data formats (yaml json) are used.
page4-elem5,It is not necessary to learn and utilize proprietary functionality.
page4-elem5,What’s next?
page4-elem5,To learn more about operator patterns and best practices check out the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go] repo.
page4-elem5,The repo comes with two typical operators for stateless and stateful workloads samples how to implement autopilot functionality and documentation.
page4-elem5,The post Why you should build Kubernetes Operators [http://heidloff.net/article/why-you-should-build-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem6,Accessing Kubernetes from Go Applications.
page4-elem6,When developing auto-pilot capabilities in Kubernetes operators often CronJobs and Jobs are used to automate operations.
page4-elem6,This article describes how to implement such jobs with Golang.
page4-elem6,The complete source code from this article is available in the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go/tree/main/operator-database-backup] repo.
page4-elem6,My previous article Automatically Archiving Data with Kubernetes Operators [http://heidloff.net/article/automatically-archiving-data-kubernetes-operators/] describes an auto pilot sample scenario to back up data on a scheduled basis.
page4-elem6,The code [https://github.com/IBM/operator-sample-go/blob/0b46e5ee18b892293ce2ff2eb565ea9500de298b/operator-database-backup/backup/backup.go] of the backup job is pretty straight forward.
page4-elem6,I’ve implemented a Go image with the following functionality.
page4-elem6,* Get the database backup resource from Kubernetes
page4-elem6,* Validate input environment variables
page4-elem6,* Read data from the database system
page4-elem6,* Write data to object storage
page4-elem6,* Write status as conditions in database backup resource
page4-elem6,Dockerfile
page4-elem6,To package up the Go application I’ve defined the following Dockerfile [https://github.com/IBM/operator-sample-go/blob/64dac7d036ce81b9ceba3e1dd2dd1f1c83cd2968/operator-database-backup/Dockerfile].
page4-elem6,Some notes:
page4-elem6,* Uses two stages one for build and one for runtime
page4-elem6,* The Go dependencies are downloaded first to cache them in an image layer
page4-elem6,* With the parameter ‘GOOS=linux’ the application is built for Linux
page4-elem6,* Uses Red Hat’s UBI image for example in order to also run on OpenShift
page4-elem6,* The compiled ‘app’ file is a program that ends after it’s done (not a web server)
page4-elem6,FROM golang:1.18.0 AS builder
page4-elem6,WORKDIR /app
page4-elem6,COPY go.mod ./
page4-elem6,COPY go.sum ./
page4-elem6,RUN go mod download
page4-elem6,COPY main.go ./
page4-elem6,COPY backup ./backup/
page4-elem6,RUN CGO_ENABLED=0 GOOS=linux go build -a -o app .
page4-elem6,FROM registry.access.redhat.com/ubi8/ubi-micro:8.5-833
page4-elem6,WORKDIR /
page4-elem6,COPY --from=builder /app /
page4-elem6,CMD ["./app"]
page4-elem6,Access to Kubernetes
page4-elem6,Jobs that execute work on behalf of operators usually have to access Kubernetes built-in and custom resources.
page4-elem6,For example jobs need to store the output of the jobs in the ‘status.conditions’ field of custom resources.
page4-elem6,Operators built with the Operator SDK provide convenience functionality to get an instance of the Kubernetes client to access resources in clusters.
page4-elem6,Go applications that are not operators can use the same library but the initialization is slightly different.
page4-elem6,Let’s take a look at the code [https://github.com/IBM/operator-sample-go/blob/64dac7d036ce81b9ceba3e1dd2dd1f1c83cd2968/operator-database-backup/backup/backup_resource.go].
page4-elem6,import (
page4-elem6,databaseoperatorv1alpha1 "github.com/ibm/operator-sample-go/operator-database/api/v1alpha1"
page4-elem6,"k8s.io/apimachinery/pkg/runtime"
page4-elem6,"k8s.io/apimachinery/pkg/runtime/schema"
page4-elem6,"k8s.io/client-go/rest"
page4-elem6,"k8s.io/client-go/tools/clientcmd"
page4-elem6,"sigs.k8s.io/controller-runtime/pkg/client"
page4-elem6,"sigs.k8s.io/controller-runtime/pkg/scheme"
page4-elem6,func getBackupResource() error {
page4-elem6,config err := rest.InClusterConfig()
page4-elem6,if err != nil {
page4-elem6,kubeconfig := filepath.Join(
page4-elem6,os.Getenv("HOME") ".kube" "config"
page4-elem6,fmt.Println("Using kubeconfig file: " kubeconfig)
page4-elem6,config err = clientcmd.BuildConfigFromFlags("" kubeconfig)
page4-elem6,if err != nil {
page4-elem6,return err
page4-elem6,var GroupVersion = schema.GroupVersion{Group: "database.sample.third.party" Version: "v1alpha1"}
page4-elem6,var SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}
page4-elem6,var databaseOperatorScheme *runtime.Scheme
page4-elem6,databaseOperatorScheme err = SchemeBuilder.Build()
page4-elem6,...
page4-elem6,err = databaseoperatorv1alpha1.AddToScheme(databaseOperatorScheme)
page4-elem6,...
page4-elem6,kubernetesClient err = client.New(config client.Options{Scheme: databaseOperatorScheme})
page4-elem6,...
page4-elem6,databaseBackupResource = &databaseoperatorv1alpha1.DatabaseBackup{}
page4-elem6,err = kubernetesClient.Get(applicationContext types.NamespacedName{Name: backupResourceName Namespace: namespace} databaseBackupResource)
page4-elem6,...
page4-elem6,return nil
page4-elem6,To get an instance of the controller-runtime client a rest.Config object is needed.
page4-elem6,When running in clusters this config can be read via the API rest.InClusterConfig().
page4-elem6,When running locally the config can be read from the local file $Home/.kube/config.
page4-elem6,If you want to access resource definitions defined by a controller (other image and other Go package) you can import [http://heidloff.net/article/importing-go-modules-kubernetes-operators/] them.
page4-elem6,In the example above the custom resource definition ‘DatabaseBackup’ from the ‘operator-database’ project is used to access database backup resources.
page4-elem6,To learn more about operator patterns and best practices check out the repo operator-sample-go [https://github.com/IBM/operator-sample-go].
page4-elem6,The post Accessing Kubernetes from Go Applications [http://heidloff.net/article/accessing-kubernetes-from-go-applications/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem7,Automatically Archiving Data with Kubernetes Operators.
page4-elem7,Kubernetes operators allow the automation of day 2 operational tasks.
page4-elem7,A good example is the automatic archiving of data.
page4-elem7,This article describes how data from a simple database can be archived automatically in S3 buckets using CronJobs Jobs and custom resources.
page4-elem7,The complete source code from this article is available in the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go/tree/43ec9d7e16f97b11ce4aa5b64d2e9a9ce0a9fde9/database-service] repo.
page4-elem7,The repo comes with a simple implementation of a database system which can be deployed on Kubernetes.
page4-elem7,Data is persisted in JSON files as outlined in the previous article Building Databases on Kubernetes with Quarkus [http://Building Databases on Kubernetes with Quarkus].
page4-elem7,To allow operators to backup data automatically you need a custom resource definition first which defines when to do backups and where to store the data.
page4-elem7,We’ve created a custom resource ‘DatabaseBackup [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/operator-database/config/samples/database.sample_v1alpha1_databasebackup.yaml]‘ with this information.
page4-elem7,We use buckets from IBM Cloud Object Storage.
page4-elem7,apiVersion: database.sample.third.party/v1alpha1
page4-elem7,kind: DatabaseBackup
page4-elem7,metadata:
page4-elem7,name: databasebackup-manual
page4-elem7,namespace: database
page4-elem7,spec:
page4-elem7,repos:
page4-elem7,- name: ibmcos-repo
page4-elem7,type: ibmcos
page4-elem7,secretName: ibmcos-repo
page4-elem7,serviceEndpoint: "https://s3.fra.eu.cloud-object-storage.appdomain.cloud"
page4-elem7,authEndpoint: "https://iam.cloud.ibm.com/identity/token"
page4-elem7,bucketNamePrefix: "database-backup-"
page4-elem7,manualTrigger:
page4-elem7,enabled: true
page4-elem7,time: "2022-12-15T02:59:43.1Z"
page4-elem7,repo: ibmcos-repo
page4-elem7,scheduledTrigger:
page4-elem7,enabled: false
page4-elem7,schedule: "0 * * * *"
page4-elem7,repo: ibmcos-repo
page4-elem7,In order to run the backups on a scheduled basis we use Kubernetes CronJobs.
page4-elem7,This makes also sense since the backup tasks can take quite some time for large datasets.
page4-elem7,The CronJob [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/operator-database-backup/kubernetes/cronjob.yaml] could be applied manually but a cleaner design is to let operators do this.
page4-elem7,Secrets like the HMAC secret should also be moved to Kubernetes secrets (or security tools).
page4-elem7,apiVersion: batch/v1
page4-elem7,kind: CronJob
page4-elem7,metadata:
page4-elem7,name: database-backup
page4-elem7,namespace: database
page4-elem7,spec:
page4-elem7,schedule: "0 * * * *"
page4-elem7,jobTemplate:
page4-elem7,spec:
page4-elem7,template:
page4-elem7,spec:
page4-elem7,containers:
page4-elem7,- name: database-backup
page4-elem7,image: docker.io/nheidloff/operator-database-backup:v1.0.7
page4-elem7,imagePullPolicy: IfNotPresent
page4-elem7,env:
page4-elem7,- name: BACKUP_RESOURCE_NAME
page4-elem7,value: "databasebackup-manual"
page4-elem7,- name: NAMESPACE
page4-elem7,value: "database"
page4-elem7,- name: CLOUD_OBJECT_STORAGE_HMAC_ACCESS_KEY_ID
page4-elem7,value: "xxx"
page4-elem7,- name: CLOUD_OBJECT_STORAGE_HMAC_SECRET_ACCESS_KEY
page4-elem7,value: "xxx"
page4-elem7,- name: CLOUD_OBJECT_STORAGE_REGION
page4-elem7,value: "eu-geo"
page4-elem7,- name: CLOUD_OBJECT_STORAGE_SERVICE_ENDPOINT
page4-elem7,value: "s3.eu.cloud-object-storage.appdomain.cloud"
page4-elem7,restartPolicy: OnFailure
page4-elem7,For testing purposes the following commands can be invoked to trigger a job manually to run immediately.
page4-elem7,In status.conditions of the database backup resource feedback is provided whether the backup has been successful.
page4-elem7,$ kubectl apply -f ../operator-database/config/samples/database.sample_v1alpha1_databasebackup.yaml
page4-elem7,$ kubectl apply -f kubernetes/role.yaml
page4-elem7,$ kubectl apply -f kubernetes/cronjob.yaml
page4-elem7,$ kubectl create job --from=cronjob/database-backup manuallytriggered -n database
page4-elem7,$ kubectl get databasebackups databasebackup-manual -n database -oyaml
page4-elem7,...
page4-elem7,status:
page4-elem7,conditions:
page4-elem7,- lastTransitionTime: "2022-04-07T05:16:30Z"
page4-elem7,message: Database has been archived
page4-elem7,reason: BackupSucceeded
page4-elem7,status: "True"
page4-elem7,type: Succeeded
page4-elem7,$ kubectl logs -n database $(kubectl get pods -n database | awk '/manuallytriggered/ {print $1;exit}')
page4-elem7,These screenshots show the deployed CronJob Job and Pod.
page4-elem7,Data is archived in IBM Cloud Object Storage.
page4-elem7,The code [https://github.com/IBM/operator-sample-go/blob/0b46e5ee18b892293ce2ff2eb565ea9500de298b/operator-database-backup/backup/backup.go] of the backup job is pretty straight forward.
page4-elem7,I’ve implemented a Go image with the following functionality.
page4-elem7,* Get the database backup resource from Kubernetes
page4-elem7,* Validate input environment variables
page4-elem7,* Read data from the database system
page4-elem7,* Write data to object storage
page4-elem7,* Write status as conditions in database backup resource
page4-elem7,To learn more about operator patterns and best practices check out the repo operator-sample-go [https://github.com/IBM/operator-sample-go].
page4-elem7,The post Automatically Archiving Data with Kubernetes Operators [http://heidloff.net/article/automatically-archiving-data-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem8,Building Databases on Kubernetes with Quarkus.
page4-elem8,While there are plenty of examples how to write stateless applications on Kubernetes there are relative few simple samples explaining how to write stateful applications.
page4-elem8,This article describes how to write a simple database system with Quarkus.
page4-elem8,The complete code of this article can be found in the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go/tree/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service] repo.
page4-elem8,My previous article How to build your own Database on Kubernetes [http://heidloff.net/article/how-to-build-your-own-database-on-kubernetes/] explains the concepts how stateful workloads can be run on Kubernetes.
page4-elem8,Before reading on make sure you understand StatefulSets.
page4-elem8,To recap here are the main components.
page4-elem8,Let’s look at the StatefulSet definition [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/kubernetes/statefulset.yaml] first:
page4-elem8,apiVersion: apps/v1
page4-elem8,kind: StatefulSet
page4-elem8,metadata:
page4-elem8,name: database-cluster
page4-elem8,namespace: database
page4-elem8,labels:
page4-elem8,app: database-cluster
page4-elem8,spec:
page4-elem8,serviceName: database-service
page4-elem8,replicas: 3
page4-elem8,selector:
page4-elem8,matchLabels:
page4-elem8,app: database-cluster
page4-elem8,template:
page4-elem8,metadata:
page4-elem8,labels:
page4-elem8,app: database-cluster
page4-elem8,spec:
page4-elem8,securityContext:
page4-elem8,fsGroup: 2000
page4-elem8,terminationGracePeriodSeconds: 10
page4-elem8,containers:
page4-elem8,- name: database-container
page4-elem8,image: nheidloff/database-service:v1.0.22
page4-elem8,imagePullPolicy: IfNotPresent
page4-elem8,ports:
page4-elem8,- containerPort: 8089
page4-elem8,name: api
page4-elem8,volumeMounts:
page4-elem8,- name: data-volume
page4-elem8,mountPath: /data
page4-elem8,env:
page4-elem8,- name: DATA_DIRECTORY
page4-elem8,value: /data/
page4-elem8,- name: POD_NAME
page4-elem8,valueFrom:
page4-elem8,fieldRef:
page4-elem8,apiVersion: v1
page4-elem8,fieldPath: metadata.name
page4-elem8,- name: NAMESPACE
page4-elem8,valueFrom:
page4-elem8,fieldRef:
page4-elem8,apiVersion: v1
page4-elem8,fieldPath: metadata.namespace
page4-elem8,volumeClaimTemplates:
page4-elem8,- metadata:
page4-elem8,name: data-volume
page4-elem8,spec:
page4-elem8,accessModes: [ "ReadWriteOnce" ]
page4-elem8,storageClassName: ibmc-vpc-block-5iops-tier
page4-elem8,resources:
page4-elem8,requests:
page4-elem8,storage: 1Mi
page4-elem8,Notes about the stateful set:
page4-elem8,* There are three replicas: One lead and two followers.
page4-elem8,* A storage class is used to provision volumes automatically.
page4-elem8,* Each pod/container has its own volume.
page4-elem8,* The volume is mounted into the container.
page4-elem8,* To allow containers to read metadata like their pod names environment variables are used.
page4-elem8,* The security context is set to “fsGroup: 2000” which allows file access from the Quarkus image.
page4-elem8,To access the pods a service is defined [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/kubernetes/service.yaml].
page4-elem8,For example the leader can be invoked via “http://database-cluster-0.database-service.database:8089/persons”.
page4-elem8,apiVersion: v1
page4-elem8,kind: Service
page4-elem8,metadata:
page4-elem8,labels:
page4-elem8,app: database-service
page4-elem8,name: database-service
page4-elem8,namespace: database
page4-elem8,spec:
page4-elem8,clusterIP: None
page4-elem8,ports:
page4-elem8,- port: 8089
page4-elem8,selector:
page4-elem8,app: database-cluster
page4-elem8,The database service uses a single JSON file [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/data.json] for storage.
page4-elem8,For the leader the file is created when the leader is initialized.
page4-elem8,Followers synchronize [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/src/main/java/heidloff/net/database/DataSynchronization.java#L18] the data from the leader when they are initialized.
page4-elem8,public static Response synchronizeDataFromLeader(LeaderUtils leaderUtils PersonResource personResource) {
page4-elem8,System.out.println("LeaderUtils.synchronizeDataFromLeader()");
page4-elem8,String leaderAddress = "http://database-cluster-0.database-service.database:8089/persons";
page4-elem8,int httpStatus = 200;
page4-elem8,if (leaderUtils.isLeader() == true) {
page4-elem8,httpStatus = 501; // Not Implemented
page4-elem8,} else {
page4-elem8,Set<Person> persons = null;
page4-elem8,try {
page4-elem8,// Note: This follower should update from the previous follower (or leader)
page4-elem8,// For simplification purposes updates are only read from the leader
page4-elem8,URL apiUrl = new URL(leaderAddress);
page4-elem8,System.out.println("Leader found.
page4-elem8,URL: " + leaderAddress);
page4-elem8,RemoteDatabaseService customRestClient = RestClientBuilder.newBuilder().baseUrl(apiUrl).
page4-elem8,register(ExceptionMapper.class).build(RemoteDatabaseService.class);
page4-elem8,persons = customRestClient.getAll();
page4-elem8,} catch (Exception e) {
page4-elem8,System.out.println("/persons could not be invoked");
page4-elem8,httpStatus = 503; // Service Unavailable
page4-elem8,if (persons != null) {
page4-elem8,try {
page4-elem8,personResource.updateAllPersons(persons);
page4-elem8,} catch (RuntimeException e) {
page4-elem8,System.out.println("Data could not be written");
page4-elem8,httpStatus = 503; // Service Unavailable
page4-elem8,return Response.status(httpStatus).build();
page4-elem8,Write operations are only allowed on the leader.
page4-elem8,When they are executed on the leader the followers need to be notified to update their state (see code [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/src/main/java/heidloff/net/database/DataSynchronization.java#L52]).
page4-elem8,public static void notifyFollowers() {
page4-elem8,KubernetesClient client = new DefaultKubernetesClient();
page4-elem8,String serviceName = "database-service";
page4-elem8,String namespace = System.getenv("NAMESPACE");
page4-elem8,PodList podList = client.pods().inNamespace(namespace).list();
page4-elem8,podList.getItems().forEach(pod -> {
page4-elem8,if (pod.getMetadata().getName().endsWith("-0") == false) {
page4-elem8,String followerAddress =  pod.getMetadata().getName() + "."
page4-elem8,+ serviceName + "."
page4-elem8,+ namespace + ":8089";
page4-elem8,System.out.println("Follower found: " + pod.getMetadata().getName() + " - " + followerAddress);
page4-elem8,try {
page4-elem8,URL apiUrl = new URL("http://" + followerAddress + "/api/onleaderupdated");
page4-elem8,RemoteDatabaseService customRestClient = RestClientBuilder.newBuilder().
page4-elem8,register(ExceptionMapper.class).baseUrl(apiUrl).build(RemoteDatabaseService.class);
page4-elem8,customRestClient.onLeaderUpdated();
page4-elem8,} catch (Exception e) {
page4-elem8,System.out.println("/onleaderupdated could not be invoked");
page4-elem8,});
page4-elem8,The next question is how the leader is determined.
page4-elem8,In this sample [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/src/main/java/heidloff/net/database/LeaderUtils.java#L88] a simple mechanism is used which is to check whether the container’s pod name ends with “-0”.
page4-elem8,public void electLeader() {
page4-elem8,String podName = System.getenv("POD_NAME");
page4-elem8,if ((podName != null) && (podName.endsWith("-0"))) {
page4-elem8,setLeader(true);
page4-elem8,The state of all pods is stored on the volumes too (podstate.json [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/podstate.json]) so that the new pods can continue with the state previous pod instances left off.
page4-elem8,To simulate a real database system the database application has SQL-like APIs [https://github.com/IBM/operator-sample-go/blob/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service/src/main/java/heidloff/net/database/API.java] to execute statements and queries.
page4-elem8,To learn more check out the complete source code [https://github.com/IBM/operator-sample-go/tree/8ce338d65d2cc9f8db437e3aa635f94a45156922/database-service].
page4-elem8,The post Building Databases on Kubernetes with Quarkus [http://heidloff.net/quarkus/building-databases-kubernetes-quarkus/] appeared first on Niklas Heidloff [http://heidloff.net].
page4-elem9,How to build your own Database on Kubernetes.
page4-elem9,Kubernetes supports running stateless and stateful workloads.
page4-elem9,This article describes the core concepts how to build and run a simple database on Kubernetes using an open source sample.
page4-elem9,The snippets below are part of the ibm/operator-sample-go [https://github.com/IBM/operator-sample-go/tree/main/database-service] repo which describes patterns and best practices how to build operators.
page4-elem9,A key benefit of operators is the automation of day 2 tasks like the management of stateful applications.
page4-elem9,For example operators can act as an auto-pilot to automatically run backups for databases.
page4-elem9,In order to demonstrate this I’ve built a simple database sample.
page4-elem9,The key concept to build stateful applications are StatefulSets.
page4-elem9,Check out the videos below especially the first one that describe this Kubernetes capability.
page4-elem9,* Kubernetes StatefulSet simply explained | Deployment vs StatefulSet [https://youtu.be/pPQKAR1pA9U]
page4-elem9,* A Kubernetes Operator for etcd [https://youtu.be/nyUe-3zmHRc]
page4-elem9,Here is a summary of the first video:
page4-elem9,* Pods have identities for example database-cluster-0
page4-elem9,* Pods are created and deleted after each other
page4-elem9,* Each pod has it’s own PVC and PV or StorageClass
page4-elem9,* Data will survive when all pods die
page4-elem9,* Pod state is stored on volumes to allow pod recreations
page4-elem9,* One leader and multiple followers
page4-elem9,* Only the leader can write
page4-elem9,* Followers need to synchronize data from leader
page4-elem9,* Pods have fixed individual DNS names
page4-elem9,This diagram shows the involved components in my sample database.
page4-elem9,Here are screenshots of the deployed database application.
page4-elem9,The database is very simple.
page4-elem9,It stores all data in one JSON file.
page4-elem9,The file [https://github.com/IBM/operator-sample-go/blob/433655ed56fe4408b83b8ec033ba1176a3b7b72b/database-service/data.json] contains a list of persons with first and last names.
page4-elem9,"firstName": "Niklas"
page4-elem9,"lastName": "Heidloff"
page4-elem9,"id": "e0a08c5b-62d5-4b20-a024-e1c270d901c2"
page4-elem9,}
page4-elem9,"firstName": "Adam"
page4-elem9,"lastName": "Deleeuw"
page4-elem9,"id": "93115462-543c-4149-9b75-2b1a84bd326c"
page4-elem9,}
page4-elem9,"firstName": "Thomas"
page4-elem9,"lastName": "Suedbroecker"
page4-elem9,"id": "dd35e011-093b-4c63-9d54-040a5dc3d28f"
page4-elem9,}
page4-elem9,"firstName": "Alain"
page4-elem9,"lastName": "Airom"
page4-elem9,"id": "918b0da0-afda-4cbf-a370-0347a6ede98e"
page4-elem9,Let’s take a look at the behaviour [https://github.com/IBM/operator-sample-go/blob/433655ed56fe4408b83b8ec033ba1176a3b7b72b/database-service/README.md#testing-apis-on-kubernetes] of the database.
page4-elem9,After the stateful set has been deployed there will be three pods.
page4-elem9,database-cluster-0 is the leader database-cluster-1 and database-cluster-2 are the followers.
page4-elem9,It takes some time for the pods to come up since they are started after each other and since they need to bind and potentially create volumes.
page4-elem9,$ kubectl exec -n database database-cluster-1 -- curl -s http://localhost:8089/persons
page4-elem9,$ kubectl exec -n database database-cluster-1 -- curl -s http://localhost:8089/api/leader
page4-elem9,$ kubectl logs -n database database-cluster-1
page4-elem9,$ kubectl exec -n database database-cluster-1 -- curl -s -X 'POST' 'http://localhost:8089/persons' -H 'accept: application/json' -H 'Content-Type: application/json' -d '{"firstName": "Johanna""lastName": "Koester""id": "e956b5d0-fa0c-40e8-9da9-333c214dcaf7"}'
page4-elem9,$ kubectl exec -n database database-cluster-1 -- curl -s http://localhost:8089/persons
page4-elem9,The /persons endpoint for ‘1’ returns all four initial persons.
page4-elem9,The follower ‘1’ received the data from the leader when it started.
page4-elem9,The attempt to create a new person fails since only the leader can write.
page4-elem9,The same write operation works if executed on the leader ‘0’.
page4-elem9,After this the data will be synchronized to all followers again.
page4-elem9,$ kubectl exec -n database database-cluster-0 -- curl -s http://localhost:8089/persons
page4-elem9,$ kubectl exec -n database database-cluster-0 -- curl -s http://localhost:8089/api/leader
page4-elem9,$ kubectl logs -n database database-cluster-0
page4-elem9,$ kubectl exec -n database database-cluster-0 -- curl -s -X 'POST' 'http://localhost:8089/persons' -H 'accept: application/json' -H 'Content-Type: application/json' -d '{"firstName": "Johanna""lastName": "Koester""id": "e956b5d0-fa0c-40e8-9da9-333c214dcaf7"}'
page4-elem9,$ kubectl exec -n database database-cluster-0 -- curl -s http://localhost:8089/persons
page4-elem9,$ kubectl exec -n database database-cluster-1 -- curl -s http://localhost:8089/persons
page4-elem9,When the leader ‘0’ goes down the data remains on the volume.
page4-elem9,The stateless set will automatically start a new pod with the same name ‘0’ which marks the pod as leader.
page4-elem9,The new pod ‘0’ will bind the existing volume.
page4-elem9,$ kubectl delete pod database-cluster-0 -n database
page4-elem9,$ kubectl exec -n database database-cluster-0 -- curl -s http://localhost:8089/persons
page4-elem9,When adding more pods the new pods read the data from other pods when they are started.
page4-elem9,$ kubectl scale statefulsets database-cluster --replicas=3 -n database
page4-elem9,$ kubectl exec -n database database-cluster-2 -- curl -s http://localhost:8089/persons
page4-elem9,I’ll blog in more detail how I’ve implemented this.
page4-elem9,For now check out the repo [https://github.com/IBM/operator-sample-go].
page4-elem9,The post How to build your own Database on Kubernetes [http://heidloff.net/article/how-to-build-your-own-database-on-kubernetes/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem0,Tekton without Tekton in DevSecOps Pipelines.
page5-elem0,IBM provides a DevSecOps reference implementation which is especially useful for regulated industries to adhere to policies.
page5-elem0,This article describes how we have implemented the CI and CD pipelines for our SaaS reference architecture.
page5-elem0,This article is part of a mini series.
page5-elem0,Read the previous articles to understand the benefits of the DevSecOps reference implementation and how to use the CI/CD pipelines from a consumer perspective.
page5-elem0,In those articles I explain the DevSecOps reference implementation via a concrete sample scenario which is a SaaS reference architecture [https://github.com/IBM/multi-tenancy] that shows how our clients and partners can build software as a service.
page5-elem0,* DevSecOps for SaaS Reference Architecture on OpenShift [http://heidloff.net/article/devsecops-saas-reference-architecture-openshift/]
page5-elem0,* Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]
page5-elem0,* Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/]
page5-elem0,* Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/]
page5-elem0,* This article: Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/]
page5-elem0,The DevSecOps reference implementation uses internally Tekton [https://tekton.dev/] which is a Kubernetes-native CI/CD technology with several benefits like the big community lots of reusable tasks multi-cloud support and more.
page5-elem0,One challenge I’ve experienced with Tekton is that it is sometimes not as easy and convenient as I had hoped.
page5-elem0,A good example is how parameters are passed around.
page5-elem0,To support clean encapsulations and allow reuse of assets Tekton assets provide interfaces which describe exactly the input and output of assets like tasks.
page5-elem0,While this concept makes a lot of sense it can add complexity (some people might say unnecessary work) when developing pipelines.
page5-elem0,For example to pass a property to a task it might involve multiple stages: from initial definition to pipeline to pipeline run to task and then to task run.
page5-elem0,In my case this often caused issues since I forgot steps and the debugging was time consuming.
page5-elem0,This is why I use the article title ‘Tekton without Tekton’.
page5-elem0,The DevSecOps reference implementation comes with its own programming model to make it easier to build pipelines.
page5-elem0,For common tasks like vulnerability checks secret detections and building images out of the box functionality is provided.
page5-elem0,In other words no code has to be written.
page5-elem0,Instead tasks can be reused and customized either declaratively or programmatically.
page5-elem0,Rather than writing Tekton tasks scripts can be invoked.
page5-elem0,The .pipeline-config.yaml [https://github.com/IBM/multi-tenancy/blob/2692acce6588f12011ce4b52e7dccb425b219530/.pipeline-config.yaml] file defines which scripts to invoke in which stages of the CI/CD pipelines.
page5-elem0,For each stage different base images can be chosen.
page5-elem0,For example the ‘deploy’ stage cannot be handled generically which is why custom specific scripts need to be provided.
page5-elem0,# Documentation: https://pages.github.ibm.com/one-pipeline/docs/custom-scripts.html
page5-elem0,version: '1'
page5-elem0,setup:
page5-elem0,image: icr.io/continuous-delivery/pipeline/pipeline-base-image:2.12@sha256:ff4053b0bca784d6d105fee1d008cfb20db206011453071e86b69ca3fde706a4
page5-elem0,script: |
page5-elem0,#!/usr/bin/env bash
page5-elem0,source cd-scripts/setup.sh
page5-elem0,deploy:
page5-elem0,image: icr.io/continuous-delivery/pipeline/pipeline-base-image:2.12@sha256:ff4053b0bca784d6d105fee1d008cfb20db206011453071e86b69ca3fde706a4
page5-elem0,script: |
page5-elem0,#!/usr/bin/env bash
page5-elem0,if [[ "$PIPELINE_DEBUG" == 1 ]]; then
page5-elem0,trap env EXIT
page5-elem0,env
page5-elem0,set -x
page5-elem0,fi
page5-elem0,source cd-scripts/deploy_setup.sh
page5-elem0,source cd-scripts/deploy.sh
page5-elem0,I especially like the tools that provide various convenience functionality.
page5-elem0,For example this is how you can use global variables (see documentation [https://cloud.ibm.com/docs/devsecops?topic=devsecops-devsecops-pipelinectl]).
page5-elem0,Define variable [https://github.com/IBM/multi-tenancy/blob/2692acce6588f12011ce4b52e7dccb425b219530/cd-scripts/deploy_setup.sh#L154]:
page5-elem0,set_env PLATFORM_NAME "${PLATFORM_NAME}"
page5-elem0,Read variable: [https://github.com/IBM/multi-tenancy/blob/2692acce6588f12011ce4b52e7dccb425b219530/cd-scripts/deploy.sh#L5]
page5-elem0,PLATFORM_NAME="$(get_env PLATFORM_NAME)"
page5-elem0,Check out the IBM Toolchains documentation [https://cloud.ibm.com/docs/devsecops?topic=devsecops-tutorial-cd-devsecops] and the SaaS reference architecture [https://github.com/IBM/multi-tenancy] to find out more.
page5-elem0,The post Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem1,Continuous Delivery with DevSecOps Reference Architecture.
page5-elem1,IBM provides a DevSecOps reference implementation which is especially useful for regulated industries to adhere to policies.
page5-elem1,This article describes the CD pipeline to deploy software using a GitOps approach.
page5-elem1,Here is the definition of DevSecOps [https://cloud.ibm.com/docs/devsecops?topic=devsecops-devsecops_intro] from IBM:
page5-elem1,> DevSecOps is an evolution of Agile and DevOps integrating secure development best practices as early as possible in the software delivery lifecycle (also known as “shift left”).
page5-elem1,This approach prevents security problems from reaching production systems and failing corporate audits.
page5-elem1,DevSecOps requires automating security and compliance controls as part of continuous integration and continuous delivery processes.
page5-elem1,Evidence of these controls is also collected to demonstrate to auditors that every change in history meets the necessary controls.
page5-elem1,This article is part of a mini series:
page5-elem1,* DevSecOps for SaaS Reference Architecture on OpenShift [http://heidloff.net/article/devsecops-saas-reference-architecture-openshift/]
page5-elem1,* Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]
page5-elem1,* Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/]
page5-elem1,* This article: Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/]
page5-elem1,* Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/]
page5-elem1,In my previous blog [http://heidloff.net/article/change-evidence-issue-management-devsecops/] I explained the CI pipeline.
page5-elem1,The CI pipeline template that is part of IBM’s DevSecOps reference implementation builds and pushes images and runs various security and code tests.
page5-elem1,Only if all checks pass the application can be deployed to production via the CD pipeline.
page5-elem1,This assures that new versions can be deployed at any time based on business (not technical) decisions.
page5-elem1,The CD (continuous delivery) pipeline generates all of the evidence and change request summary content.
page5-elem1,The pipeline deploys the build artifacts to a specific environment and collects creates and uploads all existing log files evidence and artifacts to the evidence locker.
page5-elem1,Here is an overview of the functionality [https://cloud.ibm.com/docs/devsecops?topic=devsecops-cd-devsecops-cd-pipeline] provided by the CD pipeline:
page5-elem1,* Determine deployment delta
page5-elem1,* Calculate deployment BOM
page5-elem1,* Collect evidence summary
page5-elem1,* Prepare and create change request
page5-elem1,* Check change request approval
page5-elem1,* Perform deployment
page5-elem1,* Run acceptance test
page5-elem1,Let’s take a look at a concrete sample.
page5-elem1,My team has developed a SaaS reference architecture [https://github.com/IBM/multi-tenancy] that shows how our clients and partners can build software as a service.
page5-elem1,While the compute components are identical for multiple platforms like Kubernetes OpenShift and Serverless the way these components are deployed is specific to the platforms.
page5-elem1,Here is how the CD pipeline is used for Kubernetes and OpenShift deployments.
page5-elem1,In order to deploy a new application version for a specific tenant a pull request has to be created and merged.
page5-elem1,The pull request asks to merge the latest version from the main branch of the inventory to the tenant specific branches in the inventory.
page5-elem1,After the latest version has been merged into the branch of a specific tenant the deployment functionality of the DevSecOps reference implementation uses GitOps to deploy the application to the production environment of the tenant.
page5-elem1,This is done by comparing the actual ‘as is’ state in the cluster with the ‘to be’ state in the tenant branch.
page5-elem1,Here are the key steps performed in the CD pipelines.
page5-elem1,For the complete flow read the documentation.
page5-elem1,* Promotion pipeline [https://github.com/IBM/multi-tenancy-documentation/blob/main/documentation/kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pull-request.md]: The first CD pipeline is a very simple ‘pipeline’ which only creates a pull request.
page5-elem1,* CD pipeline [https://github.com/IBM/multi-tenancy-documentation/blob/main/documentation/kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/cd-pipeline.md]: The second CD pipeline is the actual CD pipeline.
page5-elem1,Create the pull request to deploy the latest version for a specific tenant.
page5-elem1,After defining all data the pull request can be merged.
page5-elem1,The actual CD pipeline (the second one) can be started in either of the following ways:
page5-elem1,* Preferred: Trigger the CD pipeline manually.
page5-elem1,* Optional: Automatically after every merge action in the inventory repository
page5-elem1,Global and tenant specific configuration is read.
page5-elem1,Either Kubernetes or OpenShift can be used; in a shared cluster or isolated clusters for tentants.
page5-elem1,The delta is calculated since only changes are deployed.
page5-elem1,Additionally security checks are performed again.
page5-elem1,After the actual deployment has been performed data is collected.
page5-elem1,Check out the IBM Toolchains documentation [https://cloud.ibm.com/docs/devsecops?topic=devsecops-tutorial-cd-devsecops] and the SaaS reference architecture [https://github.com/IBM/multi-tenancy] to find out more.
page5-elem1,The post Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem2,Change Evidence and Issue Management with DevSecOps.
page5-elem2,IBM provides a DevSecOps reference implementation which is especially useful for regulated industries to adhere to policies.
page5-elem2,This article describes the CI pipeline which provides in addition to the usual CI functionality change evidence and issue management capabilities.
page5-elem2,This article is part of a mini series:
page5-elem2,* DevSecOps for SaaS Reference Architecture on OpenShift [http://heidloff.net/article/devsecops-saas-reference-architecture-openshift/]
page5-elem2,* Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]
page5-elem2,* This article: Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/]
page5-elem2,* Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/]
page5-elem2,* Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/]
page5-elem2,The CI pipeline template that is part of IBM’s DevSecOps reference implementation builds and pushes images and runs various security and code tests.
page5-elem2,Only if all checks pass the application can be deployed to production via the CD pipelines.
page5-elem2,This assures that new versions can be deployed at any time based on business (not technical) decisions.
page5-elem2,Functionality of the (second) CI pipeline:
page5-elem2,* Build and push images
page5-elem2,* Run various security checks (secret detection image vulnerabilities compliance)
page5-elem2,* Run various code tests (unit tests acceptance tests)
page5-elem2,* Deploy services to integration/testing Kubernetes namespaces or OpenShift projects
page5-elem2,* Manage changes evidence and issues
page5-elem2,I’ve documented [https://github.com/IBM/multi-tenancy-documentation/blob/main/documentation/kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pipeline.md] all steps of the pipeline on GitHub.
page5-elem2,Let’s take a look at the most important steps.
page5-elem2,The CI pipeline is triggered automatically after the pull request has been merged (see previous article [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]).
page5-elem2,The CI pipeline reads both global and tenant specific configuration from a Git repo.
page5-elem2,In the CI pipeline the tenant configuration is not from an actual tenant but a dummy/test tenant used to run the CI tests.
page5-elem2,The image is built and pushed.
page5-elem2,Next various security vulnerability and compliance checks are run.
page5-elem2,Then the container is deployed to an integration/testing Kubernetes namespace or OpenShift project to run more tests.
page5-elem2,The status can be monitored in IBM DevOps Insights.
page5-elem2,The latest successful version is stored in the inventory repo.
page5-elem2,Evidence is collected in the evidence repo.
page5-elem2,If the pipeline run has been successful no issues are created in the compliance issues repo.
page5-elem2,After a successful run of the CI pipeline the CD pipeline can be run.
page5-elem2,I’ll blog about this soon.
page5-elem2,Check out the IBM Toolchains documentation [https://cloud.ibm.com/docs/devsecops?topic=devsecops-tutorial-cd-devsecops] and the SaaS reference architecture [https://github.com/IBM/multi-tenancy] to find out more.
page5-elem2,The post Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem3,Shift-Left Continuous Integration with DevSecOps Pipelines.
page5-elem3,IBM provides a DevSecOps reference implementation that I’ve used to build our SaaS reference architecture.
page5-elem3,This article describes the CI pipeline especially the part to enforce branch protection.
page5-elem3,This article is part of a mini series.
page5-elem3,* DevSecOps for SaaS Reference Architecture on OpenShift [http://heidloff.net/article/devsecops-saas-reference-architecture-openshift/]
page5-elem3,* This article: Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/]
page5-elem3,* Change Evidence and Issue Management with DevSecOps [http://heidloff.net/article/change-evidence-issue-management-devsecops/]
page5-elem3,* Continuous Delivery with DevSecOps Reference Architecture [http://heidloff.net/article/continuous-delivery-ibm-devsecops-reference-architecture/]
page5-elem3,* Tekton without Tekton in DevSecOps Pipelines [http://heidloff.net/article/tekton-without-tekton-devsecops-pipelines/]
page5-elem3,A core concept of DevSecOps is ‘shift left testing and security’.
page5-elem3,The basic idea is simple.
page5-elem3,Do as much security and functional testing as early as possible in the software development lifecycle.
page5-elem3,The earlier issues are detected the easier and cheaper is it to fix the issues.
page5-elem3,These are the main tasks of the first of the two CI pipelines that are part of the IBM DevSecOps reference implementation.
page5-elem3,* Branch protection is ensured so that developers cannot push to the main branch directly
page5-elem3,* CIS checks are run
page5-elem3,* Secrets in code and config files are detected
page5-elem3,* Unit tests are run
page5-elem3,* Vulnerability scans are performed
page5-elem3,The documentation [http://Continuous Integration (CI) toolchain introduction https://cloud.ibm.com/docs/devsecops?topic=devsecops-tutorial-cd-devsecops#devsecops-ci-toolchain-intro] describes these steps in more details.
page5-elem3,Let’s look how this can be done when developers check in code.
page5-elem3,You can find the complete flow [https://github.com/IBM/multi-tenancy-documentation/blob/main/documentation/kubernetes-via-ibm-kubernetes-service-and-ibm-openshift/ci-pull-request.md] in the GitHub repo.
page5-elem3,Changes from developers are not pushed into the main branch directly.
page5-elem3,Instead feature branches are used which is a good pattern anyway.
page5-elem3,After the pull request has been created the first CI pipeline starts automatically to perform the various security and functional tests.
page5-elem3,I like the integration in GitHub.
page5-elem3,As a developer I can see most of the results directly in the GitHub experience.
page5-elem3,In the background the pipeline on the IBM Cloud is run.
page5-elem3,Even if the various security checks have been successful the pull request still cannot be merged.
page5-elem3,A second developer (in this case my colleague Adam) needs to approve first.
page5-elem3,From my personal experience I can say that these reviews are another very good pattern.
page5-elem3,Check out the IBM Toolchains documentation [https://cloud.ibm.com/docs/devsecops?topic=devsecops-tutorial-cd-devsecops] and the SaaS reference architecture [https://github.com/IBM/multi-tenancy] to find out more.
page5-elem3,The post Shift-Left Continuous Integration with DevSecOps Pipelines [http://heidloff.net/article/shift-left-continuous-integration-devsecops-pipelines/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem4,The Kubernetes Operator Metamodel.
page5-elem4,Operators provide huge value by automating day 2 operations for software running on Kubernetes.
page5-elem4,However for operator developers there is a steep learning curve.
page5-elem4,This article describes the key objects and concepts you need to understand before building operators.
page5-elem4,There are several sites tutorials and articles that describe operators.
page5-elem4,When I started to work on operators it would have helped me to understand the metamodel key concepts and high level architecture first.
page5-elem4,Unfortunately I didn’t find such an overview.
page5-elem4,The best overview I found is the Kubebuilder Architecture Concept Diagram [https://book.kubebuilder.io/architecture.html].
page5-elem4,I’ve extended and changed this diagram to add key components that were missing and to help me explaining our clients how operators work.
page5-elem4,I’ve also simplified the diagram to focus on key capabilities only.
page5-elem4,Most of the concepts in the diagram are generic for all types of operators no matter how they have been implemented.
page5-elem4,Some parts are specific to operators built with Golang [https://go.dev/] Operator SDK [https://sdk.operatorframework.io/] including Kubebuilder [https://github.com/kubernetes-sigs/kubebuilder] and the Operator Lifecycle Manager Framework [https://operatorframework.io/].
page5-elem4,Below are some more details about the different objects.
page5-elem4,I have also added links to samples.
page5-elem4,The samples are part of a bigger sample called operator-sample-go [https://github.com/IBM/operator-sample-go] which is available on GitHub that describes various operator patterns and best practises.
page5-elem4,Operator
page5-elem4,The term ‘Operator’ or ‘Kubernetes Operator’ describes the mechanism to automate deployments and day 2 operations for software running on Kubernetes which implements the operator pattern [https://kubernetes.io/docs/concepts/extend-kubernetes/operator/].
page5-elem4,This pattern is used by Kubernetes internally as well as externally for custom resources.
page5-elem4,Operators contain custom resource definitions and business logic to manage these resources.
page5-elem4,The self contained operators are deployed as containers on Kubernetes.
page5-elem4,Usually there is one running operator instance per cluster.
page5-elem4,For production deployments the Operator Lifecycle Manager (OLM) provides functionality to deploy and operate the operators for example to handle multiple versions.
page5-elem4,Operators are packaged in CSVs (cluster service versions).
page5-elem4,Samples:
page5-elem4,* Operator Dockerfile [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/Dockerfile]
page5-elem4,* Operator CSV [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/bundle/manifests/operator-application.clusterserviceversion.yaml]
page5-elem4,* Operator initialization in main.go [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/main.go]
page5-elem4,API
page5-elem4,The term API is often used as synonym to custom resource definition.
page5-elem4,Custom resource definitions have schemas and potentially multiple versions.
page5-elem4,This allows managing resources declaratively in Kubernetes production environments.
page5-elem4,Custom resource definitions are identified [https://book.kubebuilder.io/cronjob-tutorial/gvks.html] by their group version and resource name.
page5-elem4,One operator can contain multiple resource definitions.
page5-elem4,Samples:
page5-elem4,* Sample resource [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/config/samples/application.sample_v1beta1_application.yaml]
page5-elem4,* Schema as Go struct [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/api/v1beta1/application_types.go]
page5-elem4,* Schema as yaml [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/config/crd/bases/application.sample.ibm.com_applications.yaml]
page5-elem4,* Versions [https://github.com/IBM/operator-sample-go/tree/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/api]
page5-elem4,Manager
page5-elem4,The manager contains the business logic of the operator which knows how to deploy and manage custom resources.
page5-elem4,Additionally it comes with generic built in functionality to handle HA leader election export metrics handle webhook certs and broadcasts events.
page5-elem4,It also provides a client to access Kubernetes and a cache to improve efficiency.
page5-elem4,Sample:
page5-elem4,* Manager creation in main.go [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/main.go#L57]
page5-elem4,Controller
page5-elem4,The main responsibility of controllers is to synchronize the ‘to be’ states as defined in custom resources with the ‘as is’ states in Kubernetes clusters.
page5-elem4,This includes creations of new resources updates to existing resources or deletions.
page5-elem4,This logic is implemented in the controllers’ reconcile function.
page5-elem4,The reconciler doesn’t use an imperative model to manage resources because of the nature of distributed Kubernetes systems and because of the long time it can take to change resources without blocking anything.
page5-elem4,Instead the reconciler is invoked over and over again until it signals that it’s done.
page5-elem4,This is why reconcilers need to be idempotent.
page5-elem4,One controller manages one custom resource definition including all versions of it.
page5-elem4,The controller uses caches and Kubernetes clients and gets events via filters.
page5-elem4,Samples:
page5-elem4,* Flow in Reconcile function [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/controllers/application/controller.go]
page5-elem4,* Synchronization of resources [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/controllers/application/service.go#L43-L49]
page5-elem4,* Creations and updates of resources [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/controllers/application/secret.go]
page5-elem4,* Definition of resources to watch [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/controllers/application/controller.go#L126]
page5-elem4,Webhooks
page5-elem4,With webhooks values of resources can be changed and conversions between different versions can be done.
page5-elem4,Samples:
page5-elem4,* Initialization [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/api/v1beta1/application_webhook.go#L28]
page5-elem4,* Validation [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/api/v1beta1/application_webhook.go#L38-L83]
page5-elem4,* Conversion [https://github.com/IBM/operator-sample-go/blob/884e14053522645fcfedde38e4ae8a03378902f8/operator-application/api/v1alpha1/application_conversion.go]
page5-elem4,To learn more about operator patterns and best practices check out the repo operator-sample-go [https://github.com/IBM/operator-sample-go].
page5-elem4,The post The Kubernetes Operator Metamodel [http://heidloff.net/article/the-kubernetes-operator-metamodel/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem5,Converting Custom Resource Versions in Operators.
page5-elem5,Custom Kubernetes resources typically have multiple versions.
page5-elem5,Operators need to be able to convert between all different versions in all directions.
page5-elem5,This article describes how to implement this using a simple example.
page5-elem5,As applications evolve custom resource definitions need to be extended.
page5-elem5,As for every API these changes need to be upwards compatible.
page5-elem5,Additionally the information from the newer versions needs also to be stored in older versions.
page5-elem5,This is why conversions need to be done in BOTH directions without loosing information.
page5-elem5,This allows Kubernetes to provide the following functionality.
page5-elem5,See the documentation Versions in CustomResourceDefinitions [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/] for details.
page5-elem5,* Custom resource is requested in a different version than stored version.
page5-elem5,* Watch is created in one version but the changed object is stored in another version.
page5-elem5,* Custom resource PUT request is in a different version than storage version.
page5-elem5,The best documentation I’ve found about conversion comes from Kubebuilder:
page5-elem5,* Hubs spokes and other wheel metaphors [https://book.kubebuilder.io/multiversion-tutorial/conversion-concepts.html]
page5-elem5,* Kubebuilder Doc – Implementing Conversion [https://book.kubebuilder.io/multiversion-tutorial/conversion.html]
page5-elem5,Let’s look at a concrete example.
page5-elem5,I’m working on a GitHub repo that describes various operator patterns and best practises [https://github.com/IBM/operator-sample-go].
page5-elem5,There is a custom resource ‘Application’ which has two version: The intial v1alpha1 version and the latest version v1beta1.
page5-elem5,This is a resource using the alpha version [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/config/samples/application.sample_v1alpha1_application.yaml]:
page5-elem5,apiVersion: application.sample.ibm.com/v1alpha1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,name: application
page5-elem5,namespace: application-alpha
page5-elem5,spec:
page5-elem5,version: "1.0.0"
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,The beta version [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/config/samples/application.sample_v1beta1_application.yaml] has one additional property ‘title’.
page5-elem5,apiVersion: application.sample.ibm.com/v1beta1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,name: application
page5-elem5,namespace: application-beta
page5-elem5,spec:
page5-elem5,version: "1.0.0"
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,title: Movies
page5-elem5,Once deployed the application resource can be read via the following kubectl commands.
page5-elem5,By default the latest version is returned.
page5-elem5,$ kubectl get applications/application -n application-beta -oyaml
page5-elem5,or
page5-elem5,$ kubectl get applications.v1beta1.application.sample.ibm.com/application -n application-beta -oyaml
page5-elem5,apiVersion: application.sample.ibm.com/v1beta1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,annotations:
page5-elem5,kubectl.kubernetes.io/last-applied-configuration: ...
page5-elem5,...
page5-elem5,spec:
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,title: Movies
page5-elem5,version: 1.0.0
page5-elem5,You can also request a specific version in this case the alpha version from the application-alpha resource.
page5-elem5,In the sample the ‘title’ is missing since it wasn’t part of the resource when it was created.
page5-elem5,$ kubectl get applications.v1alpha1.application.sample.ibm.com/application -n application-alpha -oyaml
page5-elem5,apiVersion: application.sample.ibm.com/v1alpha1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,annotations:
page5-elem5,kubectl.kubernetes.io/last-applied-configuration: ...
page5-elem5,...
page5-elem5,spec:
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,version: 1.0.0
page5-elem5,Furthermore you can request the beta version of the application-alpha resource.
page5-elem5,In this case there is a title which has the value ‘Undefined’ since it was not set initially.
page5-elem5,$ kubectl get applications.v1beta1.application.sample.ibm.com/application -n application-alpha -oyaml | grep -A6 -e "spec:" -e "apiVersion: application.sample.ibm.com/"
page5-elem5,apiVersion: application.sample.ibm.com/v1beta1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,annotations:
page5-elem5,kubectl.kubernetes.io/last-applied-configuration: ...
page5-elem5,...
page5-elem5,spec:
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,title: Undefined
page5-elem5,version: 1.0.0
page5-elem5,You can even request the application-beta resource in the alpha version.
page5-elem5,In this case the title can not be stored in the ‘spec’ part.
page5-elem5,The trick is to use annotations.
page5-elem5,Annotations are part of every resource in the metadata section.
page5-elem5,They are basically a ‘generic schema’ which name/values pairs.
page5-elem5,$ kubectl get applications.v1alpha1.application.sample.ibm.com/application -n application-beta -oyaml | grep -A6 -e "spec:" -e "apiVersion: application.sample.ibm.com/"
page5-elem5,apiVersion: application.sample.ibm.com/v1alpha1
page5-elem5,kind: Application
page5-elem5,metadata:
page5-elem5,annotations:
page5-elem5,applications.application.sample.ibm.com/title: Movies
page5-elem5,kubectl.kubernetes.io/last-applied-configuration: ...
page5-elem5,...
page5-elem5,spec:
page5-elem5,amountPods: 1
page5-elem5,databaseName: database
page5-elem5,databaseNamespace: database
page5-elem5,version: 1.0.0
page5-elem5,Next let me describe how to implement this scenario.
page5-elem5,First you need to define which of the versions should be used to store the resources in etcd via ‘+kubebuilder:storageversion’ (code [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/api/v1beta1/application_types.go#L31-L41]).
page5-elem5,//+kubebuilder:object:root=true
page5-elem5,//+kubebuilder:subresource:status
page5-elem5,//+kubebuilder:storageversion
page5-elem5,type Application struct {
page5-elem5,metav1.TypeMeta   `json:"inline"`
page5-elem5,metav1.ObjectMeta `json:"metadataomitempty"`
page5-elem5,Spec   ApplicationSpec   `json:"specomitempty"`
page5-elem5,Status ApplicationStatus `json:"statusomitempty"`
page5-elem5,Next you need to define which of the versions is your hub.
page5-elem5,All other ones are spokes.
page5-elem5,See the Kubebuilder documentation.
page5-elem5,I’ve defined the latest as hub which only contains the empty Hub() function (code [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/api/v1beta1/application_conversion.go]).
page5-elem5,package v1beta1
page5-elem5,func (*Application) Hub() {}
page5-elem5,Next the spokes need to implement ConvertTo() and ConvertFrom().
page5-elem5,Here is the ConvertFrom() function [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/api/v1alpha1/application_conversion.go#L42-L60] that converts from the latest to the initial version.
page5-elem5,// convert from the hub version (src= v1beta1) to this version (dst = v1alpha1)
page5-elem5,func (dst *Application) ConvertFrom(srcRaw conversion.Hub) error {
page5-elem5,src := srcRaw.
page5-elem5,(*v1beta1.Application)
page5-elem5,dst.ObjectMeta = src.ObjectMeta
page5-elem5,dst.Status.Conditions = src.Status.Conditions
page5-elem5,dst.Spec.AmountPods = src.Spec.AmountPods
page5-elem5,dst.Spec.DatabaseName = src.Spec.DatabaseName
page5-elem5,dst.Spec.DatabaseNamespace = src.Spec.DatabaseNamespace
page5-elem5,dst.Spec.SchemaUrl = src.Spec.SchemaUrl
page5-elem5,dst.Spec.Version = src.Spec.Version
page5-elem5,if dst.ObjectMeta.Annotations == nil {
page5-elem5,dst.ObjectMeta.Annotations = make(map[string]string)
page5-elem5,dst.ObjectMeta.Annotations[variables.ANNOTATION_TITLE] = string(src.Spec.Title)
page5-elem5,return nil
page5-elem5,And here is the ConvertTo() function [https://github.com/IBM/operator-sample-go/blob/d4b54480a059a8d46443a03f02a5af0e2f3d15a2/operator-application/api/v1alpha1/application_conversion.go#L12-L40] that converts from the initial to the latest version.
page5-elem5,// convert this version (src = v1alpha1) to the hub version (dst = v1beta1)
page5-elem5,func (src *Application) ConvertTo(dstRaw conversion.Hub) error {
page5-elem5,dst := dstRaw.
page5-elem5,(*v1beta1.Application)
page5-elem5,dst.Spec.AmountPods = src.Spec.AmountPods
page5-elem5,dst.Spec.DatabaseName = src.Spec.DatabaseName
page5-elem5,dst.Spec.DatabaseNamespace = src.Spec.DatabaseNamespace
page5-elem5,dst.Spec.SchemaUrl = src.Spec.SchemaUrl
page5-elem5,dst.Spec.Version = src.Spec.Version
page5-elem5,if src.ObjectMeta.Annotations == nil {
page5-elem5,dst.Spec.Title = variables.DEFAULT_ANNOTATION_TITLE
page5-elem5,} else {
page5-elem5,title annotationFound := src.ObjectMeta.Annotations[variables.ANNOTATION_TITLE]
page5-elem5,if annotationFound {
page5-elem5,dst.Spec.Title = title
page5-elem5,} else {
page5-elem5,dst.Spec.Title = variables.DEFAULT_ANNOTATION_TITLE
page5-elem5,dst.ObjectMeta = src.ObjectMeta
page5-elem5,dst.Status.Conditions = src.Status.Conditions
page5-elem5,return nil
page5-elem5,The implementation of the conversion webhooks is rather straight forward.
page5-elem5,The setup of the webhooks is a little bit more tricky.
page5-elem5,Check out my earlier blog Configuring Webhooks for Kubernetes Operators [http://heidloff.net/article/configuring-webhooks-kubernetes-operators/].
page5-elem5,Try the sample operator [https://github.com/IBM/operator-sample-go] which demonstrates the capabilities outlined above as well as many other operator patterns.
page5-elem5,The post Converting Custom Resource Versions in Operators [http://heidloff.net/article/converting-custom-resource-versions-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem6,Initialization and Validation Webhooks in Operators.
page5-elem6,When developing custom Kubernetes resources static defaults and simple validations for resource properties can be defined in OpenAPI/JSON schemas.
page5-elem6,For more flexible scenarios webhooks can be used to initialize and validate resources with Go code.
page5-elem6,In the easiest case defaults and validations can be defined via Kubebuilder annotations directly in the Go code [https://github.com/IBM/operator-sample-go/blob/a449303076310bc99e3595c1904e6aeb6ee03b87/operator-application/api/v1beta1/application_types.go] of the custom resource definition for example:
page5-elem6,type ApplicationSpec struct {
page5-elem6,//+kubebuilder:default:="1.0.0"
page5-elem6,Version string `json:"versionomitempty"`
page5-elem6,//+kubebuilder:validation:Minimum=0
page5-elem6,//+kubebuilder:default:=1
page5-elem6,AmountPods int32 `json:"amountPods"`
page5-elem6,// +kubebuilder:default:="database"
page5-elem6,DatabaseName string `json:"databaseNameomitempty"`
page5-elem6,// +kubebuilder:default:="databaseNamespace"
page5-elem6,DatabaseNamespace string `json:"databaseNamespaceomitempty"`
page5-elem6,// +kubebuilder:default:="https://raw.githubusercontent.com/IBM/multi-tenancy/main/my.sql"
page5-elem6,SchemaUrl string `json:"schemaUrlomitempty"`
page5-elem6,Title string `json:"title`
page5-elem6,Read the Kubebuilder documentation [https://book.kubebuilder.io/reference/markers/crd-validation.html] for more details.
page5-elem6,Additionally webhooks can be implemented as part of Kubernetes operators which are executed before custom resources are created updated and deleted.
page5-elem6,The implementation of these webhooks is straight forward.
page5-elem6,The setup of the webhooks is a little bit more tricky.
page5-elem6,Check out my earlier blog Configuring Webhooks for Kubernetes Operators [http://heidloff.net/article/configuring-webhooks-kubernetes-operators/].
page5-elem6,In order to develop initialization and validation webhooks you have to implement the methods ‘Default()’ ‘ValidateCreate()’ ‘ValidateUpdate()’ and ‘ValidateDelete()’.
page5-elem6,Let’s take a look at a sample.
page5-elem6,The sample [https://github.com/IBM/operator-sample-go] is part of a GitHub repo that demonstrates various best practises for building operators.
page5-elem6,The Default() function sets the default of the title property read from a Go variable (code [https://github.com/IBM/operator-sample-go/blob/a449303076310bc99e3595c1904e6aeb6ee03b87/operator-application/api/v1beta1/application_webhook.go#L28-L33]).
page5-elem6,func (reconciler *Application) Default() {
page5-elem6,if reconciler.Spec.Title == "" {
page5-elem6,reconciler.Spec.Title = variables.DEFAULT_ANNOTATION_TITLE
page5-elem6,Here are some snippets [https://github.com/IBM/operator-sample-go/blob/a449303076310bc99e3595c1904e6aeb6ee03b87/operator-application/api/v1beta1/application_webhook.go#L38-L83] how to validate two properties.
page5-elem6,func (reconciler *Application) ValidateCreate() error {
page5-elem6,return reconciler.validate()
page5-elem6,func (reconciler *Application) ValidateUpdate(old runtime.Object) error {
page5-elem6,return reconciler.validate()
page5-elem6,func (reconciler *Application) validate() error {
page5-elem6,var allErrors field.ErrorList
page5-elem6,if err := reconciler.validateSchemaUrl(); err != nil {
page5-elem6,allErrors = append(allErrors err)
page5-elem6,if err := reconciler.validateName(); err != nil {
page5-elem6,allErrors = append(allErrors err)
page5-elem6,if len(allErrors) == 0 {
page5-elem6,return nil
page5-elem6,return apierrors.NewInvalid(
page5-elem6,schema.GroupKind{Group: GroupVersion.Group Kind: reconciler.Kind}
page5-elem6,reconciler.Name allErrors)
page5-elem6,func (reconciler *Application) validateSchemaUrl() *field.Error {
page5-elem6,if !strings.HasPrefix(reconciler.Spec.SchemaUrl "http") {
page5-elem6,return field.Invalid(field.NewPath("spec").Child("schemaUrl") reconciler.Name "must start with 'http'")
page5-elem6,return nil
page5-elem6,func (reconciler *Application) validateName() *field.Error {
page5-elem6,// Note: Names of Kubernetes objects can only have a length is 63 characters
page5-elem6,// Note: Since deployment name = application name + ‘-deployment-microservice' the name cannot have more than 35 characters
page5-elem6,if len(reconciler.ObjectMeta.Name) > validationutils.DNS1035LabelMaxLength-24 {
page5-elem6,return field.Invalid(field.NewPath("metadata").Child("name") reconciler.Name "must be no more than 35 characters")
page5-elem6,return nil
page5-elem6,To learn more read the Kubebuilder documentation [https://book.kubebuilder.io/cronjob-tutorial/webhook-implementation.html] and try the sample [https://github.com/IBM/operator-sample-go] operator which demonstrates defaulting/validation webhooks as well as many other operator patterns.
page5-elem6,The post Initialization and Validation Webhooks in Operators [http://heidloff.net/article/developing-initialization-validation-webhooks-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem7,Configuring Webhooks for Kubernetes Operators.
page5-elem7,Kubernetes operators can initialize validate and convert custom resources via webhooks.
page5-elem7,Coding the webhooks is straight forward setting them up is a lot harder.
page5-elem7,This article summarizes the important setup steps.
page5-elem7,There are three types of webhooks used by operators:
page5-elem7,1.
page5-elem7,Initialization: To set defaults when creating new resources.
page5-elem7,This webhook is a Kubernetes admission webhook.
page5-elem7,2.
page5-elem7,Validation: To validate resources when created updated or deleted.
page5-elem7,This webhook is a Kubernetes admission webhook.
page5-elem7,3.
page5-elem7,Conversion: To convert between different resource definition versions in all directions.
page5-elem7,This webhook is a Kubernetes CRD conversion webhook.
page5-elem7,As mentioned the setup of the webhooks is not trivial.
page5-elem7,There are different pieces of documentation in various articles and blogs.
page5-elem7,My colleague Vincent Hou has written a great mini series and helped me to get our sample [https://github.com/IBM/operator-sample-go] working.
page5-elem7,* What is a webhook?
page5-elem7,* Deploying the cert manager [https://book.kubebuilder.io/cronjob-tutorial/cert-manager.html]
page5-elem7,* Deploying Admission Webhooks [https://book.kubebuilder.io/cronjob-tutorial/running-webhook.html]
page5-elem7,* How to create conversion webhook for my operator with operator-sdk [https://vincenthou.medium.com/how-to-create-conversion-webhook-for-my-operator-with-operator-sdk-36f5ee0170de]
page5-elem7,* How to use mutating webhook for the operator with operator-sdk [https://vincenthou.medium.com/how-to-use-mutating-webhook-for-the-operator-with-operator-sdk-f940bd98e10b]
page5-elem7,* How to create validating webhook with operator-sdk [https://vincenthou.medium.com/how-to-create-validating-webhook-with-operator-sdk-73f9c6332609]
page5-elem7,* Shipping an operator that includes Webhooks [https://olm.operatorframework.io/docs/advanced-tasks/adding-admission-and-conversion-webhooks/]
page5-elem7,* Webhook Configuration [https://book.kubebuilder.io/reference/markers/webhook.html]
page5-elem7,* Running and deploying the controller [https://book.kubebuilder.io/cronjob-tutorial/running.html]
page5-elem7,Before you get started it’s important to understand that webhooks require another component that needs to be installed in Kubernetes.
page5-elem7,Webhooks are invoked by the Kubernetes API server and require authentication and authorization.
page5-elem7,That’s why components like cert-manager [https://cert-manager.io/docs/] are required to inject the credentials.
page5-elem7,And that’s one of the reasons why running webhooks locally is very difficult (plus you need a proxy to call the local webhooks from Kubernetes).
page5-elem7,Let’s take a look how to set up a new operator with initialization and validation webhooks.
page5-elem7,The steps are a summary from Vincent’s article above.
page5-elem7,Create the project api and webhook:
page5-elem7,$ operator-sdk init --domain ibm.com --repo github.com/houshengbo/operator-sample-go/operator-application
page5-elem7,$ operator-sdk create api --group application.sample --version v1beta1 --kind Application --resource --controller
page5-elem7,$ operator-sdk create webhook --group application.sample --version v1beta1 --kind Application --defaulting --programmatic-validation --force
page5-elem7,In api/v1beta1/application_webhook.go change from admissionReviewVersions=v1 to admissionReviewVersions=v1beta1.
page5-elem7,Then change Default():
page5-elem7,func (r *Application) Default() {
page5-elem7,applicationlog.Info(“default” “name” r.Name)
page5-elem7,r.Spec.Foo = “default”
page5-elem7,Get the dependencies and create manifests:
page5-elem7,$ go mod vendor
page5-elem7,$ make generate
page5-elem7,$ make manifests
page5-elem7,In config/crd/kustomization.yaml uncomment the following lines:
page5-elem7,#- patches/webhook_in_memcacheds.yaml
page5-elem7,#- patches/cainjection_in_memcacheds.yaml
page5-elem7,In config/default/kustomization.yaml uncomment the following lines:
page5-elem7,#- ../webhook
page5-elem7,#- ../certmanager
page5-elem7,#- manager_webhook_patch.yaml
page5-elem7,#- webhookcainjection_patch.yaml
page5-elem7,In the same file uncomment all the lines below ‘vars’:
page5-elem7,#- name: CERTIFICATE_NAMESPACE # namespace of the certificate CR
page5-elem7,#  objref:
page5-elem7,#    kind: Certificate
page5-elem7,#    group: cert-manager.io
page5-elem7,#    version: v1
page5-elem7,#    name: serving-cert # this name should match the one in certificate.yaml
page5-elem7,#  fieldref:
page5-elem7,#    fieldpath: metadata.namespace
page5-elem7,#- name: CERTIFICATE_NAME
page5-elem7,#  objref:
page5-elem7,#    kind: Certificate
page5-elem7,#    group: cert-manager.io
page5-elem7,#    version: v1
page5-elem7,#    name: serving-cert # this name should match the one in certificate.yaml
page5-elem7,#- name: SERVICE_NAMESPACE # namespace of the service
page5-elem7,#  objref:
page5-elem7,#    kind: Service
page5-elem7,#    version: v1
page5-elem7,#    name: webhook-service
page5-elem7,#  fieldref:
page5-elem7,#    fieldpath: metadata.namespace
page5-elem7,#- name: SERVICE_NAME
page5-elem7,#  objref:
page5-elem7,#    kind: Service
page5-elem7,#    version: v1
page5-elem7,#    name: webhook-service
page5-elem7,Change config/samples/application.sample_v1beta1_application.yaml into this:
page5-elem7,apiVersion: v1
page5-elem7,kind: Namespace
page5-elem7,metadata:
page5-elem7,name: application-sample
page5-elem7,---
page5-elem7,apiVersion: application.sample.ibm.com/v1beta1
page5-elem7,kind: Application
page5-elem7,metadata:
page5-elem7,name: application-sample
page5-elem7,namespace: application-sample
page5-elem7,Deploy the operator including the webhook to Kubernetes and run it:
page5-elem7,export REGISTRY=‘docker.io’
page5-elem7,export ORG=‘nheidloff’
page5-elem7,export IMAGE=‘application-controller:v1’
page5-elem7,make docker-build docker-push IMG=“$REGISTRY/$ORG/$IMAGE”
page5-elem7,make deploy IMG=“$REGISTRY/$ORG/$IMAGE”
page5-elem7,kubectl logs -f deploy/operator-application-controller-manager -n operator-application-system -c manager
page5-elem7,Create the custom resource and check whether foo is “default”:
page5-elem7,$ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml
page5-elem7,$ kubectl get Application -n application-sample -oyaml
page5-elem7,Check out our repo [https://github.com/IBM/operator-sample-go] that contains samples for all webhooks.
page5-elem7,Keep an eye on my blog.
page5-elem7,I’ll write more about other operator patterns soon.
page5-elem7,The post Configuring Webhooks for Kubernetes Operators [http://heidloff.net/article/configuring-webhooks-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem8,Defining Dependencies in Kubernetes Operators.
page5-elem8,Operators can automate the deployment and operations of custom Kubernetes resources.
page5-elem8,These resources might dependent on other third party resources.
page5-elem8,This article describes how to define these dependencies.
page5-elem8,I’m working on a sample [https://github.com/nheidloff/operator-sample-go] that describes different patterns and best practices to build operators with Golang.
page5-elem8,The repo demonstrates how a custom resource ‘Application’ uses internally a third party ‘Database’ resource which is managed by another controller.
page5-elem8,This is a simplified version of the typical scenario to use a managed database in the cloud.
page5-elem8,Read my previous blog [http://heidloff.net/article/accessing-third-party-custom-resources-go-operators/] that explains how to access third party resources in controllers’ Go code.
page5-elem8,Additionally you need to ensure that the dependent operator (in my sample the database operator) exists when an operator (in my sample the application operator) is deployed.
page5-elem8,This can be done in the cluster service version (CSV).
page5-elem8,The CSV is the operator bundle/package which contains the definition of a specific operator version.
page5-elem8,Here is the CSV [https://github.com/nheidloff/operator-sample-go/blob/1280fe242726a329642a6a3950d1a8b9990e14d0/operator-application/bundle/manifests/operator-application.clusterserviceversion.yaml#L26-L38] of the application operator.
page5-elem8,The dependency is defined in the ‘required’ section of the spec.
page5-elem8,apiVersion: operators.coreos.com/v1alpha1
page5-elem8,kind: ClusterServiceVersion
page5-elem8,spec:
page5-elem8,apiservicedefinitions: {}
page5-elem8,customresourcedefinitions:
page5-elem8,owned:
page5-elem8,- displayName: Application
page5-elem8,kind: Application
page5-elem8,name: applications.application.sample.ibm.com
page5-elem8,version: v1alpha1
page5-elem8,required:
page5-elem8,- displayName: Database
page5-elem8,kind: Database
page5-elem8,name: databases.database.sample.third.party
page5-elem8,version: v1alpha1
page5-elem8,When you try to deploy the application operator but the database operator doesn’t exist you get an error.
page5-elem8,$ operator-sdk run bundle "$REGISTRY/$ORG/$BUNDLEIMAGE" -n operators
page5-elem8,INFO[0040] Successfully created registry pod: docker-io-nheidloff-application-controller-bundle-v15
page5-elem8,INFO[0041] Created CatalogSource: operator-application-catalog
page5-elem8,INFO[0041] Created Subscription: operator-application-v0-0-1-sub
page5-elem8,FATA[0120] Failed to run bundle: install plan is not available for the subscription operator-application-v0-0-1-sub: timed out waiting for the condition
page5-elem8,The logs in the catalog pod describe the error.
page5-elem8,$ kubectl get pods -n olm
page5-elem8,$ kubectl logs catalog-operator-b4dfcff47-55plr -n olm
page5-elem8,Event(v1.ObjectReference{Kind:"Namespace" Namespace:"" Name:"operators" ...
page5-elem8,type: 'Warning' reason: 'ResolutionFailed' constraints not satisfiable: bundle operator-application.v0.0.1 requires an operator providing an API with group: database.sample.third.party version: v1alpha1 kind: Database
page5-elem8,The following resources describe more details.
page5-elem8,* Operator Dependency and Requirement Resolution [https://operator-framework.github.io/olm-book/docs/operator-dependencies-and-requirements.html]
page5-elem8,* Creating operator manifests [https://olm.operatorframework.io/docs/tasks/creating-operator-manifests/]
page5-elem8,* Setup and Deployment via Operator Lifecycle Manager [https://github.com/nheidloff/operator-sample-go/blob/1280fe242726a329642a6a3950d1a8b9990e14d0/operator-application/SetupDeploymentViaOLM.md]
page5-elem8,Check out the repo [https://github.com/nheidloff/operator-sample-go] and keep an eye on my blog.
page5-elem8,I’ll write more about other operator patterns soon.
page5-elem8,The post Defining Dependencies in Kubernetes Operators [http://heidloff.net/article/defining-dependencies-kubernetes-operators/] appeared first on Niklas Heidloff [http://heidloff.net].
page5-elem9,Deploying Operators with the Operator Lifecycle Manager.
page5-elem9,Kubernetes operators automate the deployment and operations of Kubernetes based software.
page5-elem9,This article describes the Operator Lifecycle Manager which provides a declarative way to install manage and upgrade operators on a cluster.
page5-elem9,I’m working on an operator sample [https://github.com/nheidloff/operator-sample-go] implemented in Go that shows typical operator patterns.
page5-elem9,There are instructions how to run the operator:
page5-elem9,1.
page5-elem9,Run and debug the operator locally [https://github.com/nheidloff/operator-sample-go/blob/main/operator-application/SetupLocal.md]
page5-elem9,2.
page5-elem9,Deploy the operator manually to Kubernetes [https://github.com/nheidloff/operator-sample-go/blob/main/operator-application/SetupManualDeployment.md]
page5-elem9,3.
page5-elem9,Deploy the operator via Operator Lifecycle Manager [https://github.com/nheidloff/operator-sample-go/blob/main/operator-application/README.md#setup-and-deployment-via-operator-lifecycle-manager] (focus of this article) [https://github.com/nheidloff/operator-sample-go/blob/main/operator-application/SetupDeploymentViaOLM.md]
page5-elem9,There is a really good video Intro to the Operator Lifecycle Manager [https://www.youtube.com/watch?v=5PorcMTYZTo] describing OLM.
page5-elem9,Watch it first before reading on.
page5-elem9,The Operator SDK [https://sdk.operatorframework.io/] and the Operator Framework [https://operatorframework.io/] make it pretty simple to build and deploy operators.
page5-elem9,Without repeating everything from the video here are the necessary commands and highlights that you need to know.
page5-elem9,Note that you can also deploy operators via the OLM without using the operator-sdk CLI by using kubectl and yaml files instead.
page5-elem9,See the bottom of this article.
page5-elem9,First the OLM needs to be installed.
page5-elem9,$ operator-sdk olm install latest
page5-elem9,$ kubectl get all -n olm
page5-elem9,Next the bundle is created the bundle image is built and pushed and then the operator is run.
page5-elem9,$ export REGISTRY='docker.io'
page5-elem9,$ export ORG='nheidloff'
page5-elem9,$ export IMAGE='application-controller:v11'
page5-elem9,$ make bundle IMG="$REGISTRY/$ORG/$IMAGE"
page5-elem9,$ export BUNDLEIMAGE="application-controller-bundle:v11"
page5-elem9,$ make bundle-build BUNDLE_IMG="$REGISTRY/$ORG/$BUNDLEIMAGE"
page5-elem9,$ docker push "$REGISTRY/$ORG/$BUNDLEIMAGE"
page5-elem9,$ operator-sdk run bundle "$REGISTRY/$ORG/$BUNDLEIMAGE" -n operators
page5-elem9,The key artifact that is created is the cluster service version [https://github.com/nheidloff/operator-sample-go/blob/ca204e86e23fe166168af0eb61eac281e1f8de85/operator-application/bundle/manifests/operator-application.clusterserviceversion.yaml] (CSV) which contains all metadata describing the operator or more precisely one version of the operator.
page5-elem9,apiVersion: operators.coreos.com/v1alpha1
page5-elem9,kind: ClusterServiceVersion
page5-elem9,...
page5-elem9,spec:
page5-elem9,apiservicedefinitions: {}
page5-elem9,customresourcedefinitions:
page5-elem9,owned:
page5-elem9,- displayName: Application
page5-elem9,kind: Application
page5-elem9,name: applications.application.sample.ibm.com
page5-elem9,version: v1alpha1
page5-elem9,...
page5-elem9,clusterPermissions:
page5-elem9,- rules:
page5-elem9,- apiGroups:
page5-elem9,- application.sample.ibm.com
page5-elem9,resources:
page5-elem9,- applications
page5-elem9,verbs:
page5-elem9,- create
page5-elem9,...
page5-elem9,deployments:
page5-elem9,- name: operator-application-controller-manager
page5-elem9,spec:
page5-elem9,replicas: 1
page5-elem9,...
page5-elem9,image: docker.io/nheidloff/application-controller:v10
page5-elem9,...
page5-elem9,installModes:
page5-elem9,- supported: true
page5-elem9,type: AllNamespaces
page5-elem9,version: 0.0.1
page5-elem9,Additionally annotations.yaml [https://github.com/nheidloff/operator-sample-go/blob/ca204e86e23fe166168af0eb61eac281e1f8de85/operator-application/bundle/metadata/annotations.yaml] is created with defaults that can be overwritten.
page5-elem9,annotations:
page5-elem9,# Core bundle annotations.
page5-elem9,operators.operatorframework.io.bundle.mediatype.v1: registry+v1
page5-elem9,operators.operatorframework.io.bundle.manifests.v1: manifests/
page5-elem9,operators.operatorframework.io.bundle.metadata.v1: metadata/
page5-elem9,operators.operatorframework.io.bundle.package.v1: operator-application
page5-elem9,operators.operatorframework.io.bundle.channels.v1: alpha
page5-elem9,operators.operatorframework.io.metrics.builder: operator-sdk-v1.18.0
page5-elem9,operators.operatorframework.io.metrics.mediatype.v1: metrics+v1
page5-elem9,operators.operatorframework.io.metrics.project_layout: go.kubebuilder.io/v3
page5-elem9,Let’s take a look which Kubernetes resources have been created as result of ‘operator-sdk run bundle’.
page5-elem9,The CatalogSource contains a link to the bundle image.
page5-elem9,A catalog is a repository of metadata that the OLM uses to discover and install operators and their dependencies.
page5-elem9,$ kubectl get catalogsource -n operators
page5-elem9,NAME                           DISPLAY                TYPE   PUBLISHER      AGE
page5-elem9,operator-application-catalog   operator-application   grpc   operator-sdk   3d1h
page5-elem9,$ kubectl get catalogsource  operator-application-catalog -n operators -oyaml
page5-elem9,apiVersion: operators.coreos.com/v1alpha1
page5-elem9,kind: CatalogSource
page5-elem9,metadata:
page5-elem9,annotations:
page5-elem9,operators.operatorframework.io/index-image: quay.io/operator-framework/opm:latest
page5-elem9,operators.operatorframework.io/injected-bundles: '[{"imageTag":"docker.io/nheidloff/application-controller-bundle:v11""mode":"semver"}]'
page5-elem9,operators.operatorframework.io/registry-pod-name: docker-io-nheidloff-application-controller-bundle-v11
page5-elem9,...
page5-elem9,Additionally the CSV resource is created which contains the information above plus some state information:
page5-elem9,$ kubectl get csv -n operators
page5-elem9,NAME                          DISPLAY                VERSION   REPLACES   PHASE
page5-elem9,operator-application.v0.0.1   operator-application   0.0.1                Succeeded
page5-elem9,$ kubectl get csv operator-application.v0.0.1 -n operators -oyaml
page5-elem9,The subscription resource is the glue between the catalog and the CSV:
page5-elem9,kubectl get subscriptions -n operators
page5-elem9,NAME                              PACKAGE                SOURCE                         CHANNEL
page5-elem9,operator-application-v0-0-1-sub   operator-application   operator-application-catalog   alpha
page5-elem9,$kubectl get subscriptions operator-application-v0-0-1-sub -n operators -oyaml
page5-elem9,apiVersion: operators.coreos.com/v1alpha1
page5-elem9,kind: Subscription
page5-elem9,metadata:
page5-elem9,creationTimestamp: "2022-03-21T15:57:40Z"
page5-elem9,generation: 1
page5-elem9,labels:
page5-elem9,operators.coreos.com/operator-application.operators: ""
page5-elem9,name: operator-application-v0-0-1-sub
page5-elem9,namespace: operators
page5-elem9,spec:
page5-elem9,channel: alpha
page5-elem9,installPlanApproval: Manual
page5-elem9,name: operator-application
page5-elem9,source: operator-application-catalog
page5-elem9,sourceNamespace: operators
page5-elem9,startingCSV: operator-application.v0.0.1
page5-elem9,This is the created install plan:
page5-elem9,$ kubectl get installplans -n operators
page5-elem9,$ kubectl get installplans install-xxxxx -n operators -oyaml
page5-elem9,apiVersion: operators.coreos.com/v1alpha1
page5-elem9,kind: InstallPlan
page5-elem9,metadata:
page5-elem9,...
page5-elem9,name: install-2gxl7
page5-elem9,namespace: operators
page5-elem9,ownerReferences:
page5-elem9,- apiVersion: operators.coreos.com/v1alpha1
page5-elem9,kind: Subscription
page5-elem9,name: operator-database-v0-0-1-sub
page5-elem9,...spec:
page5-elem9,approval: Manual
page5-elem9,approved: true
page5-elem9,clusterServiceVersionNames:
page5-elem9,- operator-database.v0.0.1
page5-elem9,- operator-application.v0.0.1
page5-elem9,generation: 1
page5-elem9,Last but not least the operator resource is created.
page5-elem9,$ kubectl config set-context --current --namespace=test1
page5-elem9,$ kubectl get operators -n operators
page5-elem9,NAME                             AGE
page5-elem9,operator-application.operators   3d2h
page5-elem9,$ kubectl get operators operator-application.operators -n operators -oyaml
page5-elem9,apiVersion: operators.coreos.com/v1
page5-elem9,kind: Operator
page5-elem9,metadata:
page5-elem9,...
page5-elem9,manager: olm
page5-elem9,operation: Update
page5-elem9,subresource: status
page5-elem9,time: '2022-03-18T12:48:10Z'
page5-elem9,name: operator-application.operators
page5-elem9,...
page5-elem9,status:
page5-elem9,components:
page5-elem9,labelSelector:
page5-elem9,matchExpressions:
page5-elem9,- key: operators.coreos.com/operator-application.operators
page5-elem9,operator: Exists
page5-elem9,...
page5-elem9,- apiVersion: operators.coreos.com/v1alpha1
page5-elem9,conditions:
page5-elem9,- lastTransitionTime: '2022-03-18T12:48:58Z'
page5-elem9,lastUpdateTime: '2022-03-18T12:48:58Z'
page5-elem9,message: install strategy completed with no errors
page5-elem9,reason: InstallSucceeded
page5-elem9,status: 'True'
page5-elem9,type: Succeeded
page5-elem9,kind: ClusterServiceVersion
page5-elem9,name: operator-application.v0.0.1
page5-elem9,namespace: operators
page5-elem9,...
page5-elem9,Deployment with kubectl
page5-elem9,You can also deploy operators via OLM using kubectl.
page5-elem9,$ kubectl apply -f olm/catalogsource.yaml
page5-elem9,$ kubectl apply -f olm/subscription.yaml
page5-elem9,$ kubectl get installplans -n operators
page5-elem9,$ kubectl -n operators patch installplan install-xxxxx -p '{"spec":{"approved":true}}' --type merge
page5-elem9,This creates the same resources as above.
page5-elem9,$ kubectl get all -n operators
page5-elem9,$ kubectl get catalogsource operator-application-catalog -n operators -oyaml
page5-elem9,$ kubectl get subscriptions operator-application-v0-0-1-sub -n operators -oyaml
page5-elem9,$ kubectl get csv operator-application.v0.0.1 -n operators -oyaml
page5-elem9,$ kubectl get installplans -n operators
page5-elem9,$ kubectl get installplans install-xxxxx -n operators -oyaml
page5-elem9,$ kubectl get operators operator-application.operators -n operators -oyaml
page5-elem9,The real value of the OLM is the management of different versions via a subscription model.
page5-elem9,I’d like to blog about this soon as well as other operator based topics.
page5-elem9,Check out the repo [https://github.com/nheidloff/operator-sample-go] and keep an eye on my blog.
page5-elem9,The post Deploying Operators with the Operator Lifecycle Manager [http://heidloff.net/article/deploying-operators-operator-lifecycle-manager-olm/] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem0,Training TensorFlow Object Detection Models.
page6-elem0,TensorFlow Object Detection is a powerful technology to recognize different objects in images including their positions.
page6-elem0,The trained Object Detection models can be run on mobile and edge devices to execute predictions really fast.
page6-elem0,I’ve used this technology to build a demo where Anki Overdrive cars and obstacles are detected via an iOS app.
page6-elem0,When […]
page6-elem0,The post Training TensorFlow Object Detection Models [http://heidloff.net/article/tensorflow-object-detection-deep-learning] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem1,Deploying Machine Learning Models in the Cloud.
page6-elem1,For software development there are many methodologies patterns and techniques to build deploy and run applications.
page6-elem1,DevOps is the state of the art methodology which describes a software engineering culture with a holistic view of software development and operation.
page6-elem1,For data science there is a lot of information how machine and deep learning models can […]
page6-elem1,The post Deploying Machine Learning Models in the Cloud [http://heidloff.net/article/deployment-machine-learning-models-cloud] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem2,Training AI Models on Kubernetes.
page6-elem2,Early this year IBM announced Deep Learning as a Service within Watson Studio.
page6-elem2,The core of this service is available as open source and can be run on Kubernetes clusters.
page6-elem2,This allows developers and data scientists to train models with confidential data on-premises for example on the Kubernetes-based IBM Cloud Private.
page6-elem2,The open source version […]
page6-elem2,The post Training AI Models on Kubernetes [http://heidloff.net/article/training-ai-models-kubernetes-open-source] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem3,Reusing Open Source Models in AI Applications.
page6-elem3,Building machine and deep learning models from scratch is often not trivial not for developers and sometimes not even for data scientists.
page6-elem3,Fortunately over the last years several models have been developed and shared that can be reused and sometimes extended.
page6-elem3,This allows developers adding AI to applications without having to be data scientists.
page6-elem3,This […]
page6-elem3,The post Reusing Open Source Models in AI Applications [http://heidloff.net/article/open-source-ai-models-zoo-exchange-onnx] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem4,Building Models with AutoML in IBM Watson Studio.
page6-elem4,Many developers including myself want to use AI in their applications.
page6-elem4,Building machine learning models however often requires a lot of expertise and time.
page6-elem4,This article describes a technique called AutoML which can be used by developers to build models without having to be data scientists.
page6-elem4,While developers only have to provide the data and […]
page6-elem4,The post Building Models with AutoML in IBM Watson Studio [http://heidloff.net/article/automl-ibm-watson-studio] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem5,Deploying TensorFlow Models on Edge Devices.
page6-elem5,While it has been possible to deploy TensorFlow models to mobile and embedded devices via TensorFlow for Mobile for some time Google released an experimental version of TensorFlow Lite as an evolution of TensorFlow for Mobile at the end of last year.
page6-elem5,This new functionality allows building exciting AI scenarios on edge devices and the […]
page6-elem5,The post Deploying TensorFlow Models on Edge Devices [http://heidloff.net/article/tensorflow-lite-ibm-watson-ios] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem6,Deploying Models from IBM’s Model Exchange to Kubernetes.
page6-elem6,In March IBM launched the Model Asset Exchange which is a place for developers to find and use open source deep learning models.
page6-elem6,In this article I describe how developers can use these models in applications and how the applications can be deployed to Kubernetes.
page6-elem6,The Model Asset Exchange contains currently 11 models for visual […]
page6-elem6,The post Deploying Models from IBM’s Model Exchange to Kubernetes [http://heidloff.net/article/model-asset-exchange-dl-kubernetes-tensorflow] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem7,Hyperparameter Optimization with IBM Watson Studio.
page6-elem7,In March IBM announced Deep Learning as a Service (DLaaS) which is part of IBM Watson Studio.
page6-elem7,Below I describe how to use this service to train models and how to optimize hyperparameters to easily find the best quality model.
page6-elem7,I’m not a data scientist but have been told that finding the right hyperparameters is […]
page6-elem7,The post Hyperparameter Optimization with IBM Watson Studio [http://heidloff.net/article/hyperparameter-optimization-ibm-watson-studio] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem8,Training TensorFlow.js Models with IBM Watson.
page6-elem8,Recently Google introduced TensorFlow.js which is a JavaScript library for training and deploying machine learning models in browsers and on Node.js.
page6-elem8,I like especially the ability to run predictions in browsers.
page6-elem8,Since running this code locally saves the remote calls to servers the performance is amazing!
page6-elem8,TensorFlow.js even allows the training of models in browsers […]
page6-elem8,The post Training TensorFlow.js Models with IBM Watson [http://heidloff.net/article/tensorflowjs-ibm-watson-web-browsers-dl] appeared first on Niklas Heidloff [http://heidloff.net].
page6-elem9,Building VR Applications with Unity and IBM Watson.
page6-elem9,I’ve continued to play with Unity and the IBM Watson SDK which allows using cognitive services like speech recognition in Unity projects.
page6-elem9,With this technology you can not only build games but also other exciting scenarios.
page6-elem9,I’ve changed my Augmented Reality sample slightly to run it as a Virtual Reality app that can be experienced […]
page6-elem9,The post Building VR Applications with Unity and IBM Watson [http://heidloff.net/article/vr-virtual-reality-unity-ibm-watson] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem0,Building AR Applications with Unity and IBM Watson.
page7-elem0,Over the last days I’ve enjoyed playing with Unity and the IBM Watson SDK which allows using cognitive services like speech recognition in Unity projects.
page7-elem0,With this technology you can not only build games but also other exciting scenarios.
page7-elem0,I’ve extended an Augmented Reality application from my colleague Amara Keller which allows iOS users to […]
page7-elem0,The post Building AR Applications with Unity and IBM Watson [http://heidloff.net/article/ar-applications-unity-ibm-watson] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem1,Pictures from We Are Developers in Vienna.
page7-elem1,Last week my colleagues and I attended We Are Developers in Vienna a developer conference with 8000 attendees.
page7-elem1,We presented and demonstrated the IBM Cloud and the IBM Code developer site.
page7-elem1,Below are some pictures.
page7-elem1,Some of my colleagues: Andrea Overthun Fernando Cejas Miriam Oglesby Niklas Heidloff Jonas Jacobi Joe Sepi Crane-Messina Christine Mayer Jonas […]
page7-elem1,The post Pictures from We Are Developers in Vienna [http://heidloff.net/article/pictures-wearedevs2018] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem2,Pictures from JAX 2018.
page7-elem2,Last week my colleagues and I attended JAX which is a developer conference with 2000 attendees with a focus on Java and enterprise technologies.
page7-elem2,My colleagues and I presented and demonstrated the IBM Cloud and the IBM Code developer site.
page7-elem2,Below are some pictures.
page7-elem2,The team: Denis Ley Miriam Oglesby Fernando Cejas Niklas Heidloff and […]
page7-elem2,The post Pictures from JAX 2018 [http://heidloff.net/article/pictures-jax-2018] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem3,Developing Polyglot Applications with OpenWhisk.
page7-elem3,As serverless platforms mature more and more sophisticated cloud-native applications are built with serverless technologies.
page7-elem3,I’ve created a sample application which uses multiple functions which have been developed with JavaScript TypeScript Java and Kotlin.
page7-elem3,Get the code from GitHub.
page7-elem3,Functions in a serverless application can be developed by different teams and might be reused in different applications.
page7-elem3,The functions are rather easy and stateless the interesting question is how to build applications which handle the invocations of functions and the flows of data between the functions.
page7-elem3,The way OpenWhisk [https://openwhisk.apache.org/] solves this is by providing an extension called Composer [https://github.com/ibm-functions/composer] which is also available as open source.
page7-elem3,In my sample application I’ve used several programming languages because the bigger applications get the more likely it is that you want to use different languages.
page7-elem3,For example you might want to reuse existing code or libraries in serverless functions or you want to leverage certain skills in your team.
page7-elem3,Additionally certain languages might allow you to implement functions more efficiently.
page7-elem3,OpenWhisk supports several programming languages out of the box.
page7-elem3,In the sample I’ve often used Docker which might be important for your requirements as well.
page7-elem3,For example with Docker you can develop and test the same image locally which is deployed onto the cloud.
page7-elem3,This minimizes the risk of running into issues due to different environments.
page7-elem3,Docker also allows you to use programming languages your cloud function provider doesn’t support natively or newer version of languages and runtimes than the ones supported by your cloud function provider.
page7-elem3,For a demo check out the video starting at 2:18 min [https://youtu.be/N0T8jkfkuEg?t=2m18s].
page7-elem3,This is the screenshot of the sample application in the IBM Cloud Shell.
page7-elem3,polyglot-serverless [http://heidloff.net/wp-content/uploads/2018/04/polyglot-serverless.png] [http://heidloff.net/wp-content/uploads/2018/04/polyglot-serverless.png]
page7-elem3,Want to run this sample yourself?
page7-elem3,Try it out on the IBM Cloud [https://ibm.biz/nheidloff].
page7-elem3,The post Developing Polyglot Applications with OpenWhisk [http://heidloff.net/article/polyglot-openwhisk-serverless] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem4,Serverless and Kubernetes Demos.
page7-elem4,Over the next weeks I’ll repeat my session “When to use Serverless?
page7-elem4,When to use Kubernetes?” several times.
page7-elem4,As backup and for people who won’t attend the sessions I’ve recorded the demos.
page7-elem4,Read the summary of the session and get the slides.
page7-elem4,Demo 1 – 0:33 min Developing and debugging OpenWhisk Node.js functions in Visual […]
page7-elem4,The post Serverless and Kubernetes Demos [http://heidloff.net/article/serverless-kubernetes-demos] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem5,Developing Polyglot Serverless Applications.
page7-elem5,As serverless platforms mature more and more sophisticated cloud-native applications are built with serverless technologies.
page7-elem5,These applications are assembled with potentially many functions that are loosely-coupled and can be developed by different teams.
page7-elem5,Teams might choose different languages for the following reasons: Reuse existing code or libraries Leverage available skills in the team Implement more […]
page7-elem5,The post Developing Polyglot Serverless Applications [http://heidloff.net/article/polyglot-serverless-applications] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem6,Developing protected Serverless Web Applications.
page7-elem6,Serverless platforms are often used to build APIs for web and mobile apps.
page7-elem6,I’ve open sourced a pattern that shows how to implement protected APIs with IBM Cloud Functions and how to invoke them from Angular web applications.
page7-elem6,Get the code from GitHub.
page7-elem6,IBM Cloud Functions comes with an API Gateway.
page7-elem6,Developers can grant access […]
page7-elem6,The post Developing protected Serverless Web Applications [http://heidloff.net/article/serverless-web-applications-oauth] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem7,Pictures from JavaLand 2018.
page7-elem7,Earlier this month I attended JavaLand in Brühl Germany.
page7-elem7,JavaLand took place for the fifth time and has grown to 1900 developers from 20 countries.
page7-elem7,It’s always a lot of fun at JavaLand; this was the third time I attended.
page7-elem7,What was great this year was that we had even more developers visiting our booth […]
page7-elem7,The post Pictures from JavaLand 2018 [http://heidloff.net/article/pictures-javaland-2018] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem8,IBM Think 2018 Highlights for Developers.
page7-elem8,Last week IBM hosted the Think 2018 conference in Las Vegas which is IBM’s flagship conference that combines previous conferences like InterConnect and World of Watson.
page7-elem8,Below is a list of announcements and news that I as a developer like most.
page7-elem8,For more information about IBM Think topics check out these sites: IBM THINK Blog […]
page7-elem8,The post IBM Think 2018 Highlights for Developers [http://heidloff.net/article/think2018-highlights-developers] appeared first on Niklas Heidloff [http://heidloff.net].
page7-elem9,Developing OpenWhisk Functions with Kotlin in IntelliJ.
page7-elem9,I’ve implemented some code that shows how Apache OpenWhisk functions can be developed with Kotlin.
page7-elem9,The Java code is built via Gradle and put into a Docker image which can be deployed to OpenWhisk cloud providers like the IBM Cloud.
page7-elem9,Get the code from GitHub.
page7-elem9,There are different approaches to write OpenWhisk functions with Kotlin.
page7-elem9,The post Developing OpenWhisk Functions with Kotlin in IntelliJ [http://heidloff.net/article/openwhisk-kotlin-intellij-docker] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem0,Developing OpenWhisk Functions with Spring Boot.
page8-elem0,I’ve open sourced some code that shows how Apache OpenWhisk functions can be developed with Spring Boot and how the functions which run in Docker can be debugged in Eclipse IDEs.
page8-elem0,Get the code from GitHub.
page8-elem0,Apache OpenWhisk is an open source serverless platform.
page8-elem0,One of the great OpenWhisk features is the capability to build […]
page8-elem0,The post Developing OpenWhisk Functions with Spring Boot [http://heidloff.net/article/openwhisk-spring-boot-eclipse] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem1,Transforming JSON Data in Serverless Applications.
page8-elem1,Serverless platforms like Apache OpenWhisk are gaining more and more traction.
page8-elem1,Rather than building only single functions developers are starting to develop cloud-native applications with many serverless functions/microservices.
page8-elem1,The challenge when building serverless applications is how to manage the data flows between the functions especially if you want loosely coupled functions without dependencies between each […]
page8-elem1,The post Transforming JSON Data in Serverless Applications [http://heidloff.net/article/transforming-json-serverless] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem2,Data Flows in Serverless Cloud-Native Applications.
page8-elem2,Serverless platforms like Apache OpenWhisk are gaining more and more traction.
page8-elem2,Rather than building only single functions developers are starting to develop cloud-native applications with many serverless functions/microservices.
page8-elem2,The challenge when building serverless cloud-native applications is how to orchestrate the different functions and how to manage the data flows between the functions.
page8-elem2,Especially if you […]
page8-elem2,The post Data Flows in Serverless Cloud-Native Applications [http://heidloff.net/article/serverless-data-flows] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem3,Deploying Angular React and Vue Apps on Kubernetes.
page8-elem3,I’ve open sourced some sample code that shows how to deploy Angular React and Vue web applications to Kubernetes on the IBM Cloud.
page8-elem3,Get the code from GitHub.
page8-elem3,In order to serve the web applications nginx is used.
page8-elem3,Check out nginx.conf for a simple sample configuration.
page8-elem3,The file also shows how to access other domains […]
page8-elem3,The post Deploying Angular React and Vue Apps on Kubernetes [http://heidloff.net/article/angular-react-vue-kubernetes] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem4,Slides: When to use Serverless?
page8-elem4,When to use Kubernetes?.
page8-elem4,Many developers want to know whether they should use Serverless or Kubernetes to build cloud-native applications.
page8-elem4,Both computing options have pros and cons and it depends on your needs which option you should choose.
page8-elem4,I blogged about when to use Serverless and when to use Kubernetes.
page8-elem4,Use Serverless if you have the following needs: Variable […]
page8-elem4,The post Slides: When to use Serverless?
page8-elem4,When to use Kubernetes?
page8-elem5,Accessing Machine Learning Models via REST APIs.
page8-elem5,Watson Machine Learning can be used by data scientists to create models which can be managed and deployed on the IBM Cloud.
page8-elem5,Developers can access these models from applications for example to run predictions.
page8-elem5,I blogged about a sample scenario to predict whether people would have survived the Titanic accident based on their age ticket […]
page8-elem5,The post Accessing Machine Learning Models via REST APIs [http://heidloff.net/article/machine-learning-models-rest-apis] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem6,A/B Testing with Kubernetes and Istio.
page8-elem6,Last week I gave a presentation “When to use Serverless?
page8-elem6,When to use Kubernetes?”
page8-elem6,One of the weaknesses of Serverless platforms is that you currently cannot do things like A/B testing well since there is no notion of versions.
page8-elem6,A/B testing allows running multiple variants of functionality in parallel so that through analytics of user […]
page8-elem6,The post A/B Testing with Kubernetes and Istio [http://heidloff.net/article/ab-testing-kubernetes-istio] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem7,Developing Serverless Functions with TypeScript.
page8-elem7,One of the coolest capabilities of Apache OpenWhisk is the ability to develop functions with Docker.
page8-elem7,This allows you to develop functions in languages which are not supported out of the box by the platform.
page8-elem7,I’ve open sourced a sample that shows how to develop and debug functions with TypeScript.
page8-elem7,I’m a big fan of […]
page8-elem7,The post Developing Serverless Functions with TypeScript [http://heidloff.net/article/serverless-functions-typescript-openwhisk] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem8,Pictures from OOP in Munich.
page8-elem8,This week I attended OOP in Munich Germany which is a conference with 2300 attendees.
page8-elem8,My colleagues and I presented and demonstrated the IBM Cloud.
page8-elem8,Below are some pictures.
page8-elem8,The IBM team: Ansgar’s and my session: When to use Serverless?
page8-elem8,When to use Kubernetes?
page8-elem8,Demo at our booth: Visual Recognition for Anki Cozmo with TensorFlow […]
page8-elem8,The post Pictures from OOP in Munich [http://heidloff.net/article/pictures-oop-munich-ibm] appeared first on Niklas Heidloff [http://heidloff.net].
page8-elem9,Presentation how to create Alexa Skills with IBM Watson.
page8-elem9,Last week there was a webinar Create Alexa skills with Watson Conversation where I was supposed to present.
page8-elem9,However since I couldn’t make it my colleague Mark Sturdevant jumped in on a short notice and did a great job.
page8-elem9,You can watch the recording read the pattern documentation and read the slides.
page8-elem9,Here are some […]
page8-elem9,The post Presentation how to create Alexa Skills with IBM Watson [http://heidloff.net/article/presentation-alexa-skills-watson] appeared first on Niklas Heidloff [http://heidloff.net].